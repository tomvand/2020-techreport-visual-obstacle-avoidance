Automatically generated by Mendeley Desktop 1.19.5
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@InProceedings{Dijk2019,
author = {Dijk, Tom van and Croon, Guido de},
title = {How Do Neural Networks See Depth in Single Images?},
booktitle = {The IEEE International Conference on Computer Vision (ICCV)},
month = {October},
year = {2019}
url = {http://openaccess.thecvf.com/content_ICCV_2019/html/van_Dijk_How_Do_Neural_Networks_See_Depth_in_Single_Images_ICCV_2019_paper.html}
} 
@book{Brockers2016,
address = {Cham},
author = {Brockers, Roland and Fragoso, Anthony and Rothrock, Brandon and Lee, Connor and Matthies, Larry},
booktitle = {International Symposium on Experimental Robotics},
doi = {10.1007/978-3-319-50115-4},
editor = {Kuli{\'{c}}, Dana and Nakamura, Yoshihiko and Khatib, Oussama and Venture, Gentiane},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Brockers et al. - 2016 - Vision-Based Obstacle Avoidance for Micro Air Vehicles Using an Egocylindrical Depth Map.pdf:pdf},
isbn = {978-3-319-50114-7},
pages = {505--514},
publisher = {Springer International Publishing},
series = {Springer Proceedings in Advanced Robotics},
title = {{2016 International Symposium on Experimental Robotics}},
url = {http://link.springer.com/10.1007/978-3-319-50115-4},
volume = {1},
year = {2017}
}
@inproceedings{Ross2013,
author = {Ross, St{\'{e}}phane and Melik-Barkhudarov, Narek and Shankar, Kumar Shaurya and Wendel, Andreas and Dey, Debadeepta and Bagnell, J. Andrew and Hebert, Martial},
booktitle = {International Conference on Robotics and Automation (ICRA)},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Ross et al. - 2013 - Learning Monocular Reactive UAV Control in Cluttered Natural Environments.pdf:pdf},
isbn = {9781467356435},
pages = {1765--1772},
publisher = {IEEE},
title = {{Learning Monocular Reactive UAV Control in Cluttered Natural Environments}},
year = {2013}
}
@article{Sivaraman2013,
abstract = {This document provides a review of the past decade's literature in on-road vision-based vehicle detection. Over the past decade, vision-based surround perception has matured significantly from its infancy. We detail advances in vehicle detection, discussing representative works from the monocular and stereo-vision domains.We provide discussion on the state-of-the-art, and provide perspective on future research directions in the field.},
author = {Sivaraman, Sayanan and Trivedi, Mohan M.},
doi = {10.1109/IVS.2013.6629487},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Sivaraman, Trivedi - 2013 - A review of recent developments in vision-based vehicle detection.pdf:pdf},
isbn = {978-1-4673-2755-8},
issn = {1931-0587},
journal = {Proc. IEEE Intelligent Vehicles Symposium},
keywords = {Active Safety,Driver Assistance,Feature extraction,Hidden Markov models,Machine Learning,Optical imaging,Real-time Vision,Three-dimensional displays,Tracking,Vehicle detection,Vehicles,active safety,driver assistance,driver information systems,machine learning,monocular-vision domains,object detection,real-time vision,stereo image processing,stereo-vision domains,vehicles,vision-based vehicle detection},
number = {Iv},
pages = {310--315},
title = {{A review of recent developments in vision-based vehicle detection}},
url = {http://ieeexplore.ieee.org/ielx7/6601112/6629437/06629487.pdf?tp={\&}arnumber=6629487{\&}isnumber=6629437{\%}255Cnhttp://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6629487},
year = {2013}
}
@article{Ummenhofer2016,
abstract = {In this paper we formulate structure from motion as a learning problem. We train a convolutional network end-to-end to compute depth and camera motion from successive, unconstrained image pairs. The architecture is composed of multiple stacked encoder-decoder networks, the core part being an iterative network that is able to improve its own predictions. The network estimates not only depth and motion, but additionally surface normals, optical flow between the images and confidence of the matching. A crucial component of the approach is a training loss based on spatial relative differences. Compared to traditional two-frame structure from motion methods, results are more accurate and more robust. In contrast to the popular depth-from-single-image networks, DeMoN learns the concept of matching and, thus, better generalizes to structures not seen during training.},
archivePrefix = {arXiv},
arxivId = {1612.02401},
author = {Ummenhofer, Benjamin and Zhou, Huizhong and Uhrig, Jonas and Mayer, Nikolaus and Ilg, Eddy and Dosovitskiy, Alexey and Brox, Thomas},
doi = {10.1109/CVPR.2017.596},
eprint = {1612.02401},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Ummenhofer et al. - 2016 - DeMoN Depth and Motion Network for Learning Monocular Stereo.pdf:pdf},
journal = {arXiv preprint arXiv:1612.02401},
title = {{DeMoN: Depth and Motion Network for Learning Monocular Stereo}},
url = {http://arxiv.org/abs/1612.02401},
year = {2016}
}
@inproceedings{Ross2013,
author = {Ross, St{\'{e}}phane and Melik-Barkhudarov, Narek and Shankar, Kumar Shaurya and Wendel, Andreas and Dey, Debadeepta and Bagnell, J. Andrew and Hebert, Martial},
booktitle = {International Conference on Robotics and Automation (ICRA)},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Ross et al. - 2013 - Learning Monocular Reactive UAV Control in Cluttered Natural Environments.pdf:pdf},
isbn = {9781467356435},
pages = {1765--1772},
publisher = {IEEE},
title = {{Learning Monocular Reactive UAV Control in Cluttered Natural Environments}},
year = {2013}
}
@article{Fortun2015,
annote = {Image Understanding for Real-world Distributed Video Networks},
author = {Fortun, Denis and Bouthemy, Patrick and Kervrann, Charles},
doi = {https://doi.org/10.1016/j.cviu.2015.02.008},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Fortun, Bouthemy, Kervrann - 2015 - Optical flow modeling and computation A survey.pdf:pdf},
issn = {1077-3142},
journal = {Computer Vision and Image Understanding},
keywords = {Feature matching,Motion estimation,Occlusions,Optical flow,Optimization,Parametric models,Regularization},
number = {Supplement C},
pages = {1--21},
title = {{Optical flow modeling and computation: A survey}},
url = {http://www.sciencedirect.com/science/article/pii/S1077314215000429},
volume = {134},
year = {2015}
}
@inproceedings{Byrne2006,
author = {Byrne, Jeffrey and Cosgrove, Martin and Mehra, Raman},
booktitle = {Robotics and Automation, 2006. ICRA 2006. Proceedings 2006 IEEE International Conference on},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Byrne, Cosgrove, Mehra - 2006 - Stereo Based Obstacle Detection for an Unmanned Air Vehicle.pdf:pdf},
pages = {2830--2835},
title = {{Stereo Based Obstacle Detection for an Unmanned Air Vehicle}},
year = {2006}
}
@article{Chen2013,
abstract = {A dynamic path-planning algorithm is proposed for routing unmanned air vehicles (UAVs) in order to track ground targets under path constraints, wind effects, and obstacle avoidance requirements. We first present the tangent vector field guidance (TVFG) and the Lyapunov vector field guidance (LVFG) algorithms. We demonstrate that the TVFG outperforms the LVFG as long as a tangent line is available between the UAV's turning circle and an objective circle, which is a desired orbit pattern over a target. Based on a hybrid version of the TVFG and LVFG, we then derive a theoretically shortest path algorithm with UAV operational constraints given a target position and the current UAV dynamic state. This algorithm has the efficiency of the TVFG when UAV is outside the standoff circle and the ability to follow the path via the LVFG when inside the standoff circle. In addition we adopt point-mass approximation of the target state probability density function (pdf) for target motion prediction by exploiting road network information and target dynamics as well as obstacle avoidance strategies. Overall, the proposed technical approach is practical and competitive, supported by solid theoretical analysis on several aspects of the algorithm performance. With extensive simulations we show that the tangent-plus-Lyapunov vector field guidance (T+LVFG) algorithm provides effective and robust tracking performance in various scenarios, including a target moving according to waypoints or a random kinematics model in an environment that may include obstacles and/or winds.},
author = {Chen, Hongda and Chang, Kuochu and Agate, Craig S.},
doi = {10.1109/TAES.2013.6494384},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Chen, Chang, Agate - 2013 - UAV path planning with tangent-plus-lyapunov vector field guidance and obstacle avoidance.pdf:pdf},
isbn = {0018-9251},
issn = {00189251},
journal = {IEEE Transactions on Aerospace and Electronic Systems},
number = {2},
pages = {840--856},
title = {{UAV path planning with tangent-plus-lyapunov vector field guidance and obstacle avoidance}},
volume = {49},
year = {2013}
}
@incollection{Kosov2009,
address = {Berlin, Heidelberg},
author = {Kosov, Sergey and Seidel, Hans-peter},
booktitle = {Advances in Visual Computing. ISVC 2009. Lecture Notes in Computer Science, vol 5875},
doi = {10.1007/978-3-642-10331-5_74},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Kosov, Seidel - 2009 - Accurate Real-Time Disparity Estimation with Variational Methods.pdf:pdf},
pages = {796--807},
publisher = {Springer},
title = {{Accurate Real-Time Disparity Estimation with Variational Methods}},
year = {2009}
}
@inproceedings{Forster,
author = {Forster, Christian and Carlone, Luca and Dellaert, Frank and Scaramuzza, Davide},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Forster et al. - 2015 - IMU Preintegration on Manifold for Efficient Visual-Inertial Maximum-a-Posteriori Estimation.pdf:pdf},
publisher = {Georgia Institute of Technology},
title = {{IMU Preintegration on Manifold for Efficient Visual-Inertial Maximum-a-Posteriori Estimation}},
year = {2015}
}
@article{Yousif2015,
abstract = {This paper is intended to pave the way for new researchers in the field of robotics and autonomous systems, particularly thosewho are interested in robot localization and mapping. We discuss the fundamentals of robot navigation requirements and provide a reviewof the state of the art tech- niques that form the bases of established solutions formobile robots localization and mapping. The topicswediscuss range from basic localization techniques such as wheel odometry and dead reckoning, to the more advance Visual Odometry (VO) and Simultaneous Localization and Mapping (SLAM) techniques. We discuss VO in both monocular and stereo vision systems using feature matching/tracking and optical flow techniques.We discuss and compare the basics of most common SLAM methods such as the Extended Kalman Fil- ter SLAM (EKF-SLAM), Particle Filter and the most recent RGB-D SLAM. We also provide techniques that form the building blocks to those methods such as feature extraction (i.e. SIFT, SURF, FAST), feature matching, outlier removal and data association techniques.},
author = {Yousif, Khalid and Bab-Hadiashar, Alireza and Hoseinnezhad, Reza},
doi = {10.1007/s40903-015-0032-7},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Yousif, Bab-Hadiashar, Hoseinnezhad - 2015 - An Overview to Visual Odometry and Visual SLAM Applications to Mobile Robotics.pdf:pdf},
isbn = {2199-854X},
issn = {2363-6912},
journal = {Intelligent Industrial Systems},
keywords = {Autonomous,Mapping,Navigation,RGB-D,Visual Odometry,Visual SLAM,autonomous,mapping,navigation,rgb-d,visual odometry,visual slam},
number = {4},
pages = {289--311},
publisher = {Springer Singapore},
title = {{An Overview to Visual Odometry and Visual SLAM: Applications to Mobile Robotics}},
url = {http://link.springer.com/10.1007/s40903-015-0032-7},
volume = {1},
year = {2015}
}
@article{Schmid2013a,
abstract = {We introduce our new quadrotor platform for re- alizing autonomous navigation in unknown indoor/outdoor en- vironments. Autonomous waypoint navigation, obstacle avoid- ance and flight control is implemented on-board. The system does not require a special environment, artificial markers or an external reference system. We developed a monolithic, mechanically damped perception unit which is equipped with a stereo camera pair, an Inertial Measurement Unit (IMU), two processor-and an FPGA board. Stereo images are processed on the FPGA by the Semi-Global Matching algorithm. Keyframe- based stereo odometry is fused with IMU data compensating for time delays that are induced by the vision pipeline. The system state estimate is used for control and on-board 3D mapping. An operator can set waypoints in the map, while the quadrotor autonomously plans its path avoiding obstacles. We show experiments with the quadrotor flying from inside a building to the outside and vice versa, traversing a window and a door respectively. A video of the experiments is part of this work. To the best of our knowledge, this is the first autonomously flying system with complete on-board processing that performs waypoint navigation with obstacle avoidance in geometrically unconstrained, complex indoor/outdoor environments.},
author = {Schmid, Korbinian and Tomic, Teodor and Ruess, Felix and Hirschmuller, Heiko and Suppa, Michael},
doi = {10.1109/IROS.2013.6696922},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Schmid et al. - 2013 - Stereo vision based indooroutdoor navigation for flying robots.pdf:pdf},
isbn = {9781467363587},
issn = {21530858},
journal = {IEEE International Conference on Intelligent Robots and Systems},
pages = {3955--3962},
title = {{Stereo vision based indoor/outdoor navigation for flying robots}},
year = {2013}
}
@article{Minguez2016,
author = {Minguez, Javier and Lamiraux, Florant and Laumond, Jean-Paul},
doi = {10.1007/978-3-319-32552-1_47},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Minguez, Lamiraux, Laumond - 2016 - Motion Planning and Obstacle Avoidance.pdf:pdf},
journal = {Springer Handbook of Robotics},
pages = {1177--1202},
title = {{Motion Planning and Obstacle Avoidance}},
url = {http://link.springer.com/10.1007/978-3-319-32552-1{\_}47},
year = {2016}
}
@inbook{Zach2007,
abstract = {Variational methods are among the most successful approaches to calculate the optical flow between two image frames. A particularly appealing formulation is based on total variation (TV) regularization and the robust L 1 norm in the data fidelity term. This formulation can preserve discontinuities in the flow field and offers an increased robustness against illumination changes, occlusions and noise. In this work we present a novel approach to solve the TV-L 1 formulation. Our method results in a very efficient numerical scheme, which is based on a dual formulation of the TV energy and employs an efficient point-wise thresholding step. Additionally, our approach can be accelerated by modern graphics processing units. We demonstrate the real-time performance (30 fps) of our approach for video inputs at a resolution of 320{\{}$\backslash$texttimes{\}}240 pixels.},
address = {Berlin, Heidelberg},
author = {Zach, C and Pock, T and Bischof, H},
booktitle = {Pattern Recognition: 29th DAGM Symposium, Heidelberg, Germany, September 12-14, 2007. Proceedings},
doi = {10.1007/978-3-540-74936-3_22},
editor = {Hamprecht, Fred A and Schn{\"{o}}rr, Christoph and J{\"{a}}hne, Bernd},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Zach, Pock, Bischof - 2007 - A Duality Based Approach for Realtime TV-L 1 Optical Flow.pdf:pdf},
isbn = {978-3-540-74936-3},
pages = {214--223},
publisher = {Springer Berlin Heidelberg},
title = {{A Duality Based Approach for Realtime TV-L 1 Optical Flow}},
url = {https://doi.org/10.1007/978-3-540-74936-3{\_}22},
year = {2007}
}
@phdthesis{Rannacher2009,
author = {Rannacher, Jens},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Rannacher - 2009 - Realtime 3D Motion Estimation on Graphics Hardware.pdf:pdf},
school = {Heidelberg University},
title = {{Realtime 3D Motion Estimation on Graphics Hardware}},
type = {Undergraduate Thesis},
year = {2009}
}
@book{Goerzen2010,
abstract = {A fundamental aspect of autonomous vehicle guidance is planning trajectories. Historically, two fields have contributed to trajectory or motion planning methods: robotics and dynamics and control. The former typically have a stronger focus on computational issues and real-time robot control, while the latter emphasize the dynamic behavior and more specific aspects of trajectory performance. Guidance for Unmanned Aerial Vehicles (UAVs), including fixed- and rotary-wing aircraft, involves significant differences from most traditionally defined mobile and manipulator robots. Qualities characteristic to UAVs include non-trivial dynamics, three-dimensional environments, disturbed operating conditions, and high levels of uncertainty in state knowledge. Otherwise, UAV guidance shares qualities with typical robotic motion planning problems, including partial knowledge of the environment and tasks that can range from basic goal interception, which can be precisely specified, to more general tasks like surveillance and reconnaissance, which are harder to specify. These basic planning problems involve continual interaction with the environment. The purpose of this paper is to provide an overview of existing motion planning algorithms while adding perspectives and practical examples from UAV guidance approaches.},
author = {Goerzen, C. and Kong, Z. and Mettler, B.},
booktitle = {Journal of Intelligent and Robotic Systems: Theory and Applications},
doi = {10.1007/s10846-009-9383-1},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Goerzen, Kong, Mettler - 2010 - A survey of motion planning algorithms from the perspective of autonomous UAV guidance.pdf:pdf},
isbn = {0921-0296},
issn = {09210296},
keywords = {Algorithm,Autonomous,Complexity,Guidance,Heuristics,Motion planning,Optimization,Trajectory,UAV},
number = {1-4},
pages = {65--100},
title = {{A survey of motion planning algorithms from the perspective of autonomous UAV guidance}},
volume = {57},
year = {2010}
}
@inproceedings{Newcombe2010,
abstract = {We present a method which enables rapid and dense reconstruction of scenes browsed by a single live camera. We take point-based real-time structure from motion (SFM) as our starting point, generating accurate 3D camera pose estimates and a sparse point cloud. Our main novel contribution is to use an approximate but smooth base mesh generated from the SFM to predict the view at a bundle of poses around automatically selected reference frames spanning the scene, and then warp the base mesh into highly accurate depth maps based on view-predictive optical flow and a constrained scene flow update. The quality of the resulting depth maps means that a convincing global scene model can be obtained simply by placing them side by side and removing overlapping regions. We show that a cluttered indoor environment can be reconstructed from a live hand-held camera in a few seconds, with all processing performed by current desktop hardware. Real-time monocular dense reconstruction opens up many application areas, and we demonstrate both real-time novel view synthesis and advanced augmented reality where augmentations interact physically with the 3D scene and are correctly clipped by occlusions.},
author = {Newcombe, R A and Davison, A J},
booktitle = {2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2010.5539794},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Newcombe, Davison - 2010 - Live dense reconstruction with a single moving camera.pdf:pdf},
issn = {1063-6919},
keywords = {3D camera pose estimates,3D scene,Augmented reality,Cameras,Clouds,Image motion analysis,Image reconstruction,Layout,Machine vision,Robot vision systems,Simultaneous localization and mapping,Surface reconstruction,accurate depth maps,advanced augmented reality,cameras,computer graphics,constrained scene flow update,desktop hardware,global scene model,image reconstruction,live dense reconstruction,live hand-held camera,occlusion,point-based real-time structure from motion,real-time monocular dense reconstruction,scenes,single moving camera,sparse point cloud,view-predictive optical flow},
pages = {1498--1505},
title = {{Live dense reconstruction with a single moving camera}},
year = {2010}
}
@inproceedings{Huguet2007,
abstract = {This paper presents a method for scene flow estimation from a calibrated stereo image sequence. The scene flow contains the 3-D displacement field of scene points, so that the 2-D optical flow can be seen as a projection of the scene flow onto the images. We propose to recover the scene flow by coupling the optical flow estimation in both cameras with dense stereo matching between the images, thus reducing the number of unknowns per image point. The use of a variational framework allows us to properly handle discontinuities in the observed surfaces and in the 3-D displacement field. Moreover our approach handles occlusions both for the optical flow and the stereo. We obtain a partial differential equations system coupling both the optical flow and the stereo, which is numerically solved using an original multi- resolution algorithm. Whereas previous variational methods were estimating the 3-D reconstruction at time t and the scene flow separately, our method jointly estimates both in a single optimization. We present numerical results on synthetic data with ground truth information, and we also compare the accuracy of the scene flow projected in one camera with a state-of-the-art single-camera optical flow computation method. Results are also presented on a real stereo sequence with large motion and stereo discontinuities. Source code and sample data are available for the evaluation of the algorithm.},
author = {Huguet, F and Devernay, F},
booktitle = {2007 IEEE 11th International Conference on Computer Vision},
doi = {10.1109/ICCV.2007.4409000},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Huguet, Devernay - 2007 - A Variational Method for Scene Flow Estimation from Stereo Sequences.pdf:pdf},
issn = {1550-5499},
keywords = {2D optical flow,Cameras,Data flow computing,Image motion analysis,Image sequences,Layout,Optical computing,Optical coupling,Optimization methods,Partial differential equations,Three dimensional displays,calibrated stereo image sequence,dense stereo matching,image matching,image sequences,optical flow estimation,partial differential equations,partial differential equations system,scene flow estimation,state-of-the-art single-camera optical flow comput,stereo discontinuities,stereo image processing,variational method,variational techniques},
pages = {1--7},
title = {{A Variational Method for Scene Flow Estimation from Stereo Sequences}},
year = {2007}
}
@inproceedings{Liu2017,
author = {Liu, Peidong and Heng, Lionel and Sattler, Torsten and Geiger, Andreas and Pollefeys, Marc},
booktitle = {International Conference on Intelligent Robots and Systems (IROS)},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Liu et al. - 2017 - Direct Visual Odometry for a Fisheye-Stereo Camera.pdf:pdf},
title = {{Direct Visual Odometry for a Fisheye-Stereo Camera}},
year = {2017}
}
@article{Braillon2006,
author = {Braillon, Christophe and Pradalier, Cedric and Crowley, James L and Laugier, Christian and Gravir, Laboratoire and Rhone-alpes, Inria},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Braillon et al. - 2006 - Real-time moving obstacle detection using optical flow models.pdf:pdf},
journal = {Evaluation},
keywords = {monocular,moving obstacles,nieuwe referenties,obstacle detection,optical flow},
mendeley-tags = {monocular,moving obstacles,nieuwe referenties,obstacle detection,optical flow},
pages = {466--471},
title = {{Real-time moving obstacle detection using optical flow models}},
year = {2006}
}
@inproceedings{Albaker2009,
author = {Albaker, B. M. and Rahim, N. A.},
booktitle = {Technical Postgraduates (TECHPOS), 2009 International Conference for},
doi = {10.1109/TECHPOS.2009.5412074},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Albaker, Rahim - 2009 - A Survey of Collision Avoidance Approaches for Unmanned Aerial Vehicles.pdf:pdf},
title = {{A Survey of Collision Avoidance Approaches for Unmanned Aerial Vehicles}},
year = {2009}
}
@inproceedings{Pizzoli2014,
author = {Pizzoli, Matia and Forster, Christian and Scaramuzza, Davide},
booktitle = {Robotics and Automation (ICRA), 2014 IEEE International Conference on},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Pizzoli, Forster, Scaramuzza - 2014 - REMODE Probabilistic, Monocular Dense Reconstruction in Real Time.pdf:pdf},
isbn = {9781479936854},
pages = {2609--2616},
publisher = {IEEE},
title = {{REMODE: Probabilistic, Monocular Dense Reconstruction in Real Time}},
year = {2014}
}
@article{Carloni2013,
author = {Carloni, Raffaella and Lippiello, Vincenzo and D'Auria, Massimo and Fumagalli, Matteo and Mersha, Abeje Y. and Stramigioli, Stefano and Siciliano, Bruno},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Carloni et al. - 2013 - Robot Vision Obstacle-Avoidance Techniques for Unmanned Aerial Vehicles.pdf:pdf},
journal = {IEEE ROBOTICS {\&} AUTOMATION MAGAZINE},
number = {December},
pages = {22--31},
title = {{Robot Vision: Obstacle-Avoidance Techniques for Unmanned Aerial Vehicles}},
year = {2013}
}
@article{VanHecke2017,
author = {van Hecke, Kevin and de Croon, Guido C.H.E. and Hennes, Daniel and Setterfield, Timothy P. and Saenz-Otero, Alvar and Izzo, Dario},
doi = {10.1016/j.actaastro.2017.07.038},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/van Hecke et al. - 2017 - Self-supervised learning as an enabling technology for future space exploration robots ISS experiments on mono.pdf:pdf},
journal = {Acta Astronautica},
keywords = {persistent self-supervised learning},
number = {February},
pages = {1--9},
title = {{Acta Astronautica}},
volume = {140},
year = {2017}
}
@article{Barron1994,
author = {Barron, J L and Fleet, D J and Beauchemin, S S},
doi = {10.1007/BF01420984},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Barron, Fleet, Beauchemin - 1994 - Performance of optical flow techniques.pdf:pdf},
issn = {0920-5691},
journal = {International Journal of Computer Vision},
month = {feb},
number = {1},
pages = {43--77},
title = {{Performance of optical flow techniques}},
url = {http://link.springer.com/10.1007/BF01420984},
volume = {12},
year = {1994}
}
@article{Hirschmuller2008,
author = {Hirschm{\"{u}}ller, Heiko},
doi = {10.1109/TPAMI.2007.1166},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Hirschm{\"{u}}ller - 2008 - Stereo Processing by Semiglobal Matching and Mutual Information.pdf:pdf},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
number = {2},
pages = {328--341},
title = {{Stereo Processing by Semiglobal Matching and Mutual Information ¨}},
volume = {30},
year = {2008}
}
@article{Sun2014,
author = {Sun, Deqing and Roth, Stefan and Black, Michael J},
doi = {10.1007/s11263-013-0644-x},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Sun, Roth, Black - 2014 - A Quantitative Analysis of Current Practices in Optical Flow Estimation and the Principles Behind Them.pdf:pdf},
journal = {International Journal of Computer Vision},
keywords = {median filtering,motion boundary,non-local term,optical flow estimation,practices},
number = {2},
pages = {115--137},
title = {{A Quantitative Analysis of Current Practices in Optical Flow Estimation and the Principles Behind Them}},
volume = {106},
year = {2014}
}
@inproceedings{Newcombe2011,
author = {Newcombe, Richard A and Lovegrove, Steven J and Davison, Andrew J},
booktitle = {2011 International Conference on Computer Vision},
doi = {10.1109/ICCV.2011.6126513},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Newcombe, Lovegrove, Davison - 2011 - DTAM Dense tracking and mapping in real-time.pdf:pdf},
isbn = {978-1-4577-1102-2},
month = {nov},
pages = {2320--2327},
publisher = {IEEE},
title = {{DTAM: Dense tracking and mapping in real-time}},
url = {http://ieeexplore.ieee.org/document/6126513/},
year = {2011}
}
@article{Tippetts2016,
abstract = {A significant amount of research in the field of stereo vision has been published in the past decade. Considerable progress has been made in improving accuracy of results as well as achieving real-time performance in obtaining those results. This work provides a comprehensive review of stereo vision algorithms with specific emphasis on real-time performance to identify those suitable for resource-limited systems. An attempt has been made to compile and present accuracy and runtime performance data for all stereo vision algorithms developed in the past decade. Algorithms are grouped into three categories: (1) those that have published results of real-time or near real-time performance on standard processors, (2) those that have real-time performance on specialized hardware (i.e. GPU, FPGA, DSP, ASIC), and (3) those that have not been shown to obtain near real-time performance. This review is intended to aid those seeking algorithms suitable for real-time implementation on resource-limited systems, and to encourage further research and development of the same by providing a snapshot of the status quo.},
author = {Tippetts, Beau and Lee, Dah Jye and Lillywhite, Kirt and Archibald, James},
doi = {10.1007/s11554-012-0313-2},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Tippetts et al. - 2016 - Review of stereo vision algorithms and their suitability for resource-limited systems.pdf:pdf},
isbn = {1861-8200},
issn = {18618200},
journal = {Journal of Real-Time Image Processing},
keywords = {low complexity,stereo vision,survey},
mendeley-tags = {low complexity,stereo vision,survey},
number = {1},
pages = {5--25},
publisher = {Springer-Verlag},
title = {{Review of stereo vision algorithms and their suitability for resource-limited systems}},
url = {http://dx.doi.org/10.1007/s11554-012-0313-2},
volume = {11},
year = {2016}
}
@article{Marzat2017,
author = {Marzat, Julien and Bertrand, Sylvain and Eudes, Alexandre},
doi = {10.1016/j.ifacol.2017.08.1910},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Marzat, Bertrand, Eudes - 2017 - Reactive MPC for Autonomous MAV Navigation in Indoor Cluttered Environments Flight Experiments.pdf:pdf},
issn = {24058963},
keywords = {flight experiments,localization and mapping,micro-air vehicles,model predictive control,vision-based},
title = {{Reactive MPC for Autonomous MAV Navigation in Indoor Cluttered Environments : Flight Experiments}},
year = {2017}
}
@article{Marzat2015,
abstract = {To cite this version: Julien Marzat, Julien Moras, Aur{\'{e}}lien Plyer, Alexandre Eudes, Pascal Morin. Vision-based lo-calization, mapping and control for autonomous MAV: EuRoC challenge results. 15th ONERA-DLR Aerospace Symposium (ODAS 2015), May 2015, Toulouse, France. {\textless}hal-01178916{\textgreater}},
author = {Marzat, Julien and Moras, Julien and Eudes, Alexandre and Morin, Pascal and Marzat, Julien and Moras, Julien and Eudes, Alexandre and Morin, Pascal},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Marzat et al. - 2015 - Vision-based localization, mapping and control for autonomous MAV EuRoC challenge results.pdf:pdf},
journal = {15th ONERADLR Aerospace Symposium (ODAS 2015)},
title = {{Vision-based localization, mapping and control for autonomous MAV: EuRoC challenge results}},
url = {https://hal.archives-ouvertes.fr/hal-01178916},
year = {2015}
}
@article{Carloni2013,
author = {Carloni, Raffaella and Lippiello, Vincenzo and D'Auria, Massimo and Fumagalli, Matteo and Mersha, Abeje Y. and Stramigioli, Stefano and Siciliano, Bruno},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Carloni et al. - 2013 - Robot Vision Obstacle-Avoidance Techniques for Unmanned Aerial Vehicles.pdf:pdf},
journal = {IEEE ROBOTICS {\&} AUTOMATION MAGAZINE},
number = {December},
pages = {22--31},
title = {{Robot Vision: Obstacle-Avoidance Techniques for Unmanned Aerial Vehicles}},
year = {2013}
}
@article{Mancini2016,
abstract = {Obstacle Detection is a central problem for any robotic system, and critical for autonomous systems that travel at high speeds in unpredictable environment. This is often achieved through scene depth estimation, by various means. When fast motion is considered, the detection range must be longer enough to allow for safe avoidance and path planning. Current solutions often make assumption on the motion of the vehicle that limit their applicability, or work at very limited ranges due to intrinsic constraints. We propose a novel appearance-based Object Detection system that is able to detect obstacles at very long range and at a very high speed ({\~{}}300Hz), without making assumptions on the type of motion. We achieve these results using a Deep Neural Network approach trained on real and synthetic images and trading some depth accuracy for fast, robust and consistent operation. We show how photo-realistic synthetic images are able to solve the problem of training set dimension and variety typical of machine learning approaches, and how our system is robust to massive blurring of test images.},
archivePrefix = {arXiv},
arxivId = {1607.06349},
author = {Mancini, Michele and Costante, Gabriele and Valigi, Paolo and Ciarfuglia, Thomas A.},
doi = {10.1109/IROS.2016.7759632},
eprint = {1607.06349},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Mancini et al. - 2016 - Fast robust monocular depth estimation for Obstacle Detection with fully convolutional networks.pdf:pdf},
isbn = {9781509037629},
issn = {21530866},
journal = {IEEE International Conference on Intelligent Robots and Systems},
pages = {4296--4303},
title = {{Fast robust monocular depth estimation for Obstacle Detection with fully convolutional networks}},
volume = {2016-Novem},
year = {2016}
}
@inproceedings{Bernini2014,
author = {Bernini, Nicola and Bertozzi, Massimo and Castangia, Luca and Patander, Marco and Sabbatelli, Mario},
booktitle = {Intelligent Transportation Systems (ITSC), 2014 IEEE 17th International Conference on},
doi = {10.1109/ITSC.2014.6957799},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Bernini et al. - 2014 - Real-Time Obstacle Detection using Stereo Vision for Autonomous Ground Vehicles A Survey.pdf:pdf},
isbn = {9781479960781},
number = {1},
pages = {873--878},
title = {{Real-Time Obstacle Detection using Stereo Vision for Autonomous Ground Vehicles: A Survey}},
year = {2014}
}
@article{Sanfourche2012,
author = {Sanfourche, M and Delaune, J and {Le Besnerais}, G and {De Plinval}, H and Israel, J and Cornic, P. and Treil, A and Watanabe, Y and Plyer, A},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Sanfourche et al. - 2012 - Perception for UAV Vision-Based Navigation and Environment Modeling.pdf:pdf},
journal = {AerospaceLab},
number = {4},
pages = {1--19},
title = {{Perception for UAV: Vision-Based Navigation and Environment Modeling.}},
year = {2012}
}
@article{Carloni2013,
author = {Carloni, Raffaella and Lippiello, Vincenzo and D'Auria, Massimo and Fumagalli, Matteo and Mersha, Abeje Y. and Stramigioli, Stefano and Siciliano, Bruno},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Carloni et al. - 2013 - Robot Vision Obstacle-Avoidance Techniques for Unmanned Aerial Vehicles.pdf:pdf},
journal = {IEEE ROBOTICS {\&} AUTOMATION MAGAZINE},
number = {December},
pages = {22--31},
title = {{Robot Vision: Obstacle-Avoidance Techniques for Unmanned Aerial Vehicles}},
year = {2013}
}
@article{Leutenegger2015,
author = {Leutenegger, Stefan and Lynen, Simon and Bosse, Michael and Siegwart, Roland and Furgale, Paul},
doi = {10.1177/0278364914554813},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Leutenegger et al. - 2015 - Keyframe-based visual–inertial odometry using nonlinear optimization.pdf:pdf},
journal = {The International Journal of Robotics Research},
keywords = {bundle adjustment,imu,inertial measurement unit,inertial odometry,keyframes,robotics,sensor fusion,simultaneous localization and mapping,slam,stereo camera,visual},
number = {3},
pages = {314--334},
title = {{Keyframe-based visual–inertial odometry using nonlinear optimization}},
volume = {34},
year = {2015}
}
@article{Baker2011,
abstract = {The quantitative evaluation of optical flow algorithms by Barron et al. led to significant advances in the performance of optical flow methods. The challenges for optical flow today go beyond the datasets and evaluation methods proposed in that paper and center on problems associated with nonrigid motion, real sensor noise, complex natural scenes, and motion discontinuities. Our goal is to establish a new set of benchmarks and evaluation methods for the next generation of optical flow algorithms. To that end, we contribute four types of data to test different aspects of optical flow algorithms: sequences with nonrigid motion where the ground-truth flow is determined by tracking hidden fluorescent texture; realistic synthetic sequences; high frame-rate video used to study interpolation error; and modified stereo sequences of static scenes. In addition to the average angular error used in Barron et al., we compute the absolute flow endpoint error, measures for frame interpolation error, improved statistics, and flow accuracy at motion boundaries and in textureless regions. We evaluate the performance of several well-known methods on this data to establish the current state of the art. Our database is freely available on the Web together with scripts for scoring and publication of the results at http://vision.middlebury.edu/flow/.},
archivePrefix = {arXiv},
arxivId = {1412.0767},
author = {Baker, Simon and Scharstein, Daniel and Lewis, J. P. and Roth, Stefan and Black, Michael J. and Szeliski, Richard},
doi = {10.1007/s11263-010-0390-2},
eprint = {1412.0767},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Baker et al. - 2011 - A database and evaluation methodology for optical flow.pdf:pdf},
isbn = {978-1-4244-1631-8},
issn = {09205691},
journal = {International Journal of Computer Vision},
keywords = {Algorithms,Benchmarks,Database,Evaluation,Metrics,Optical flow,Survey},
number = {1},
pages = {1--31},
pmid = {24356354},
title = {{A database and evaluation methodology for optical flow}},
volume = {92},
year = {2011}
}
@article{Weiss2011b,
abstract = {Recent development showed that Micro Aerial Vehicles (MAVs) are nowadays capable of autonomously take off at one point and land at another using only one single camera as exteroceptive sensor. During the flight and landing phase the MAV and user have, however, little knowledge about the whole terrain and potential obstacles. In this paper we show a new solution for a real-time dense 3D terrain reconstruction. This can be used for efficient unmanned MAV terrain exploration and yields a solid base for standard autonomous obstacle avoidance algorithms and path planners. Our approach is based on a textured 3D mesh on sparse 3D point features of the scene. We use the same feature points to localize and control the vehicle in the 3D space as we do for building the 3D terrain reconstruction mesh. This enables us to reconstruct the terrain without significant additional cost and thus in real-time. Experiments show that the MAV is easily guided through an unknown, GPS denied environment. Obstacles are recognized in the iteratively built 3D terrain reconstruction and are thus well avoided.},
author = {Weiss, Stephan and Achtelik, Markus and Kneip, Laurent and Scaramuzza, Davide and Siegwart, Roland},
doi = {10.1007/s10846-010-9491-y},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Weiss et al. - 2011 - Intuitive 3D maps for MAV terrain exploration and obstacle avoidance.pdf:pdf},
isbn = {0921-0296},
issn = {09210296},
journal = {Journal of Intelligent and Robotic Systems: Theory and Applications},
keywords = {3D map generation,MAV navigation,Mesh map,Obstacle avoidance,Terrain exploration},
number = {1-4},
pages = {473--493},
title = {{Intuitive 3D maps for MAV terrain exploration and obstacle avoidance}},
volume = {61},
year = {2011}
}
@inproceedings{Sanfourche2013,
author = {Sanfourche, Martial and Vittori, Vincent and {Le Besnerais}, Guy},
booktitle = {Intelligent Robots and Systems (IROS), 2013 IEEE/RSJ International Conference on},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Sanfourche, Vittori, Le Besnerais - 2013 - eVO a realtime embedded stereo odometry for MAV applications.pdf:pdf},
isbn = {9781467363587},
pages = {2107--2114},
publisher = {IEEE},
title = {{eVO: a realtime embedded stereo odometry for MAV applications}},
year = {2013}
}
@inproceedings{Shan2016,
author = {Shan, Mo and Bi, Yingcai and Qin, Hailong and Li, Jiaxin and Gao, Zhi and Lin, Feng and Chen, Ben M},
booktitle = {Industrial Electronics Society, IECON 2016-42nd Annual Conference of the IEEE},
doi = {10.1109/IECON.2016.7793198},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Shan et al. - 2016 - A Brief Survey of Visual Odometry for Micro Aerial Vehicles.pdf:pdf},
isbn = {9781509034741},
keywords = {abstract,and compre-,applications,for a range of,has experienced a,rapid growth,recently,this survey paper attempts,to provide a timely,visual odometry,vo,which makes it viable},
pages = {6049--6054},
publisher = {IEEE},
title = {{A Brief Survey of Visual Odometry for Micro Aerial Vehicles}},
year = {2016}
}
@inproceedings{Bloesch2015,
author = {Bloesch, Michael and Omari, Sammy and Hutter, Marco and Siegwart, Roland},
booktitle = {Intelligent Robots and Systems (IROS), 2015 IEEE/RSJ International Conference on},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Bloesch et al. - 2015 - Robust Visual Inertial Odometry Using a Direct EKF-Based Approach.pdf:pdf},
isbn = {9781479999941},
pages = {298--304},
publisher = {IEEE},
title = {{Robust Visual Inertial Odometry Using a Direct EKF-Based Approach}},
year = {2015}
}
@article{Marzat2015,
abstract = {To cite this version: Julien Marzat, Julien Moras, Aur{\'{e}}lien Plyer, Alexandre Eudes, Pascal Morin. Vision-based lo-calization, mapping and control for autonomous MAV: EuRoC challenge results. 15th ONERA-DLR Aerospace Symposium (ODAS 2015), May 2015, Toulouse, France. {\textless}hal-01178916{\textgreater}},
author = {Marzat, Julien and Moras, Julien and Eudes, Alexandre and Morin, Pascal and Marzat, Julien and Moras, Julien and Eudes, Alexandre and Morin, Pascal},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Marzat et al. - 2015 - Vision-based localization, mapping and control for autonomous MAV EuRoC challenge results.pdf:pdf},
journal = {15th ONERADLR Aerospace Symposium (ODAS 2015)},
title = {{Vision-based localization, mapping and control for autonomous MAV: EuRoC challenge results}},
url = {https://hal.archives-ouvertes.fr/hal-01178916},
year = {2015}
}
@inbook{Mordohai2012,
abstract = {This paper surveys the state of the art in evaluating the performance of scene flow estimation and points out the difficulties in generating benchmarks with ground truth which have not allowed the development of general, reliable solutions. Hopefully, the renewed interest in dynamic 3D content, which has led to increased research in this area, will also lead to more rigorous evaluation and more effective algorithms. We begin by classifying methods that estimate depth, motion or both from multi-view sequences according to their parameterization of shape and motion. Then, we present several criteria for their evaluation, discuss their strengths and weaknesses and conclude with recommendations.},
address = {Berlin, Heidelberg},
author = {Mordohai, Philippos},
booktitle = {Computer Vision -- ECCV 2012. Workshops and Demonstrations: Florence, Italy, October 7-13, 2012, Proceedings, Part II},
doi = {10.1007/978-3-642-33868-7_15},
editor = {Fusiello, Andrea and Murino, Vittorio and Cucchiara, Rita},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Mordohai - 2012 - On the Evaluation of Scene Flow Estimation.pdf:pdf},
isbn = {978-3-642-33868-7},
pages = {148--157},
publisher = {Springer Berlin Heidelberg},
title = {{On the Evaluation of Scene Flow Estimation}},
url = {https://doi.org/10.1007/978-3-642-33868-7{\_}15},
year = {2012}
}
@article{Mujumdar2011,
author = {Mujumdar, Anusha and Padhi, Radhakant},
doi = {10.2514/1.49985},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Mujumdar, Padhi - 2011 - Evolving Philosophies on Autonomous ObstacleCollision Avoidance of Unmanned Aerial Vehicles.pdf:pdf},
journal = {JOURNAL OF AEROSPACE COMPUTING, INFORMATION,AND COMMUNICATION},
number = {February},
title = {{Evolving Philosophies on Autonomous Obstacle/Collision Avoidance of Unmanned Aerial Vehicles}},
volume = {8},
year = {2011}
}
@article{Heng2011,
abstract = {We present a novel stereo-based obstacle avoidance system on a vision-guided micro air vehicle (MAV) that is capable of fully autonomous maneuvers in unknown and dynamic environments. All algorithms run exclusively on the vehicle's on-board computer, and at high frequencies that allow the MAV to react quickly to obstacles appearing in its flight trajectory. Our MAV platform is a quadrotor aircraft equipped with an inertial measurement unit and two stereo rigs. An obstacle mapping algorithm processes stereo images, producing a 3D map representation of the environment; at the same time, a dynamic anytime path planner plans a collision-free path to a goal point.},
author = {Heng, Lionel and Meier, Lorenz and Tanskanen, Petri and Fraundorfer, Friedrich and Pollefeys, Marc},
doi = {10.1109/ICRA.2011.5980095},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Heng et al. - 2011 - Autonomous obstacle avoidance and maneuvering on a vision-guided MAV using on-board processing.pdf:pdf},
isbn = {9781612843865},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
keywords = {cartesian map,deliberate obstacle avoidance,low complexity,obstacle avoidance},
mendeley-tags = {cartesian map,deliberate obstacle avoidance,low complexity,obstacle avoidance},
pages = {2472--2477},
title = {{Autonomous obstacle avoidance and maneuvering on a vision-guided MAV using on-board processing}},
year = {2011}
}
@inproceedings{Ross2013,
author = {Ross, St{\'{e}}phane and Melik-Barkhudarov, Narek and Shankar, Kumar Shaurya and Wendel, Andreas and Dey, Debadeepta and Bagnell, J. Andrew and Hebert, Martial},
booktitle = {International Conference on Robotics and Automation (ICRA)},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Ross et al. - 2013 - Learning Monocular Reactive UAV Control in Cluttered Natural Environments.pdf:pdf},
isbn = {9781467356435},
pages = {1765--1772},
publisher = {IEEE},
title = {{Learning Monocular Reactive UAV Control in Cluttered Natural Environments}},
year = {2013}
}
@phdthesis{Tijmons2012,
annote = {semi global block matching stereo. Verschillende avoidance technieken incl (voorloper van?) droplet.},
author = {Tijmons, S},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Tijmons - 2012 - Stereo Vision for Flapping Wing MAVs.pdf:pdf},
keywords = {TU Delft,low complexity,obstacle avoidance,reactive obstacle avoidance,safe region,scenario: room,stereo vision},
mendeley-tags = {TU Delft,low complexity,obstacle avoidance,reactive obstacle avoidance,safe region,scenario: room,stereo vision},
pages = {96},
title = {{Stereo Vision for Flapping Wing MAVs}},
year = {2012}
}
@article{Li2013,
author = {Li, Mingyang and Mourikis, Anastasios I},
doi = {10.1177/0278364913481251},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Li, Mourikis - 2013 - High-precision, consistent EKF-based visual-inertial odometry.pdf:pdf},
journal = {The International Journal of Robotics Research},
keywords = {extended kalman filter consistency,vision-aided inertial navigation,visual-inertial odometry,visual-inertial slam},
number = {6},
pages = {690--711},
title = {{High-precision, consistent EKF-based visual-inertial odometry}},
volume = {32},
year = {2013}
}
@article{VanHecke2016,
abstract = {Self-Supervised Learning (SSL) is a reliable learning mechanism in which a robot uses an original, trusted sensor cue for training to recognize an additional, complementary sensor cue. We study for the first time in SSL how a robot's learning behavior should be organized, so that the robot can keep performing its task in the case that the original cue becomes unavailable. We study this persistent form of SSL in the context of a flying robot that has to avoid obstacles based on distance estimates from the visual cue of stereo vision. Over time it will learn to also estimate distances based on monocular appearance cues. A strategy is introduced that has the robot switch from stereo vision based flight to monocular flight, with stereo vision purely used as 'training wheels' to avoid imminent collisions. This strategy is shown to be an effective approach to the 'feedback-induced data bias' problem as also experienced in learning from demonstration. Both simulations and real-world experiments with a stereo vision equipped AR drone 2.0 show the feasibility of this approach, with the robot successfully using monocular vision to avoid obstacles in a 5 x 5 room. The experiments show the potential of persistent SSL as a robust learning approach to enhance the capabilities of robots. Moreover, the abundant training data coming from the own sensors allows to gather large data sets necessary for deep learning approaches.},
archivePrefix = {arXiv},
arxivId = {1603.08047},
author = {van Hecke, Kevin and de Croon, Guido and van der Maaten, Laurens and Hennes, Daniel and Izzo, Dario},
eprint = {1603.08047},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/van Hecke et al. - 2016 - Persistent self-supervised learning principle from stereo to monocular vision for obstacle avoidance.pdf:pdf},
keywords = {TU Delft,monoc-,obstacle detection,persistent self-supervised learning,robotics,stereo vision,ular depth estimation},
mendeley-tags = {TU Delft,obstacle detection,stereo vision},
pages = {1--17},
title = {{Persistent self-supervised learning principle: from stereo to monocular vision for obstacle avoidance}},
url = {http://arxiv.org/abs/1603.08047},
year = {2016}
}
@article{Menze2015,
abstract = {This paper proposes a novel model and dataset for 3D scene flow estimation with an application to autonomous driving. Taking advantage of the fact that outdoor scenes often decompose into a small number of independently mov- ing objects, we represent each element in the scene by its rigid motion parameters and each superpixel by a 3D plane as well as an index to the corresponding object. This minimal representation increases robustness and leads to a discrete-continuous CRF where the data term decomposes into pairwise potentials between superpixels and objects. Moreover, our model intrinsically segments the scene into its constituting dynamic components. We demonstrate the performance of our model on existing benchmarks as well as a novel realistic dataset with scene flow ground truth. We obtain this dataset by annotating 400 dynamic scenes from the KITTI raw data collection using detailed 3D CAD models for all vehicles in motion. Our experiments also re- veal novel challenges which cannot be handled by existing methods. 1.},
archivePrefix = {arXiv},
arxivId = {1512.02134},
author = {Menze, Moritz and Geiger, Andreas},
doi = {10.1109/CVPR.2015.7298925},
eprint = {1512.02134},
file = {:home/tom/Documents/phd/mendeley{\_}pdf//Menze, Geiger - 2015 - Object Scene Flow for Autonomous Vehicles.pdf:pdf},
isbn = {9781467369640},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {3061--3070},
pmid = {15747803},
title = {{Object scene flow for autonomous vehicles}},
volume = {07-12-June},
year = {2015}
}
@inproceedings{Zhang2000,
abstract = {Scene flow is the 3D motion field of points in the world. Given N (N{\textgreater}1) image sequences gathered with a N-eye stereo camera or N calibrated cameras, we present a novel system which integrates 3D scene flow and structure recovery in order to complement each other's performance. We do not assume rigidity of the scene motion, thus allowing for non-rigid motion in the scene. In our work, images are segmented into small regions. We assume that each small region is undergoing similar motion, represented by a 3D affine model. Nonlinear motion model fitting based on both optical flow constraints and stereo constraints is then carried over each image region in order to simultaneously estimate 3D motion correspondences and structure. To ensure the robustness, several regularization constraints are also introduced. A recursive algorithm is designed to incorporate the local and regularization constraints. Experimental results on both synthetic and real data demonstrate the effectiveness of our integrated 3D motion and structure analysis scheme},
author = {Zhang, Y and Kambhamettu, C},
booktitle = {Proceedings IEEE Conference on Computer Vision and Pattern Recognition. CVPR 2000 (Cat. No.PR00662)},
doi = {10.1109/CVPR.2000.854939},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Zhang, Kambhamettu - 2000 - Integrated 3D scene flow and structure recovery from multiview image sequences.pdf:pdf},
issn = {1063-6919},
keywords = {3D affine model,3D motion correspondences,3D motion field,Algorithm design and analysis,Cameras,Image motion analysis,Image segmentation,Image sequences,Layout,Motion analysis,Motion estimation,N-eye stereo camera,Nonlinear optics,Robustness,calibrated cameras,cameras,image region,image restoration,image segmentation,image sequences,integrated 3D motion,integrated 3D scene flow,motion estimation,multiview image sequences,non-rigid motion,nonlinear motion model fitting,optical flow constraints,recursive algorithm,regularization constraints,scene motion,stereo constraints,structure analysis scheme,structure recovery},
pages = {674--681 vol.2},
title = {{Integrated 3D scene flow and structure recovery from multiview image sequences}},
volume = {2},
year = {2000}
}
@phdthesis{VanHecke2015,
author = {van Hecke, Kevin},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/van Hecke - 2015 - Persistent self-supervised learning principle study and demonstration on flying robots.pdf:pdf},
school = {Delft University of Technology},
title = {{Persistent self-supervised learning principle: study and demonstration on flying robots}},
year = {2015}
}
@inbook{Wedel2008,
abstract = {This paper presents a technique for estimating the three-dimensional velocity vector field that describes the motion of each visible scene point (scene flow). The technique presented uses two consecutive image pairs from a stereo sequence. The main contribution is to decouple the position and velocity estimation steps, and to estimate dense velocities using a variational approach. We enforce the scene flow to yield consistent displacement vectors in the left and right images. The decoupling strategy has two main advantages: Firstly, we are independent in choosing a disparity estimation technique, which can yield either sparse or dense correspondences, and secondly, we can achieve frame rates of 5 fps on standard consumer hardware. The approach provides dense velocity estimates with accurate results at distances up to 50 meters.},
address = {Berlin, Heidelberg},
author = {Wedel, Andreas and Rabe, Clemens and Vaudrey, Tobi and Brox, Thomas and Franke, Uwe and Cremers, Daniel},
booktitle = {Computer Vision -- ECCV 2008: 10th European Conference on Computer Vision, Marseille, France, October 12-18, 2008, Proceedings, Part I},
doi = {10.1007/978-3-540-88682-2_56},
editor = {Forsyth, David and Torr, Philip and Zisserman, Andrew},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Wedel et al. - 2008 - Efficient Dense Scene Flow from Sparse or Dense Stereo Data.pdf:pdf},
isbn = {978-3-540-88682-2},
pages = {739--751},
publisher = {Springer Berlin Heidelberg},
title = {{Efficient Dense Scene Flow from Sparse or Dense Stereo Data}},
url = {https://doi.org/10.1007/978-3-540-88682-2{\_}56},
year = {2008}
}
@article{Khatib2006,
abstract = {This paper presents a unique real-time obstacle avoidance approach for manipulators and mobile robots based on the artificial potential field concept. Collision avoidance, tradi-tionally considered a high level planning problem, can be effectively distributed between different levels of control, al-lowing real-time robot operations in a complex environment. This method has been extended to moving obstacles by using a time-varying artificial patential field. We have applied this obstacle avoidance scheme to robot arm mechanisms and have used a new approach to the general problem of real-time manipulator control. We reformulated the manipulator con-trol problem as direct control of manipulator motion in oper-ational space{\&}mdash;the space in which the task is originally described{\&}mdash;rather than as control of the task's corresponding joint space motion obtained only after geometric and kine-matic transformation. Outside the obstacles ' regions of influ-ence, we caused the end effector to move in a straight line with an upper speed limit. The artificial potential field ap-proach has been extended to collision avoidance for all ma-nipulator links. In addition, a joint space artificial potential field is used to satisfy the manipulator internal joint con-straints. This method has been implemented in the COSMOS system for a PUMA 560 robot. Real-time collision avoidance demonstrations on moving obstacles have been performed by using visual sensing.},
author = {Khatib, Oussama},
doi = {10.1177/027836498600500106},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Khatib - 1986 - Real-Time Obstacle Avoidance for Manipulators and Mobile Robots.pdf:pdf},
isbn = {142440259X},
issn = {0278-3649},
journal = {The International Journal of Robotics Research},
month = {mar},
number = {1},
pages = {90--98},
title = {{Real-Time Obstacle Avoidance for Manipulators and Mobile Robots}},
url = {http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.910.1287 http://journals.sagepub.com/doi/10.1177/027836498600500106},
volume = {5},
year = {1986}
}
@article{Scharstein2002,
abstract = {Stereo matching is one of the most active research areas in computer vision. While a large number of algorithms for stereo correspondence have been developed, relatively little work has been done on characterizing their performance. In this paper, we present a taxonomy of dense, two-frame stereo methods designed to assess the different components and design decisions made in individual stereo algorithms. Using this taxonomy, we compare existing stereo methods and present experiments evaluating the performance of many different variants. In order to establish a common software platform and a collection of data sets for easy evaluation, we have designed a stand-alone, flexible C++ implementation that enables the evaluation of individual components and that can be easily extended to include new algorithms. We have also produced several new multiframe stereo data sets with ground truth, and are making both the code and data sets available on the Web},
author = {Scharstein, Daniel and Szeliski, Richard},
doi = {10.1023/A:1014573219977},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Scharstein, Szeliski - 2002 - A Taxonomy and Evaluation of Dense Two-Frame Stereo Correspondence Algorithms(2).pdf:pdf},
isbn = {0769513271},
issn = {09205691},
journal = {International Journal of Computer Vision},
keywords = {evaluation of performance,stereo correspondence software,stereo matching survey},
number = {1/3},
pages = {7--42},
pmid = {350},
title = {{A Taxonomy and Evaluation of Dense Two-Frame Stereo Correspondence Algorithms}},
url = {http://link.springer.com/10.1023/A:1014573219977},
volume = {47},
year = {2002}
}
@article{Lee2011,
author = {Lee, Jeong-Oog and Lee, Keun-Hwan and Park, Sang-Heon and Im, Sung-Gyu and Park, Jungkeun},
doi = {10.1108/00022661111173270},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Lee et al. - 2011 - Obstacle avoidance for small UAVs using monocular vision.pdf:pdf},
isbn = {0002266111117},
journal = {Aircraft Engineering and Aerospace Technology: An International Journal},
keywords = {collisions,image processing,monocular vision,mops,obstacle avoidance,paper type research paper,sift algorithm,small uavs},
number = {6},
pages = {397--406},
title = {{Obstacle avoidance for small UAVs using monocular vision}},
volume = {83},
year = {2011}
}
@article{Burri2016,
abstract = {This paper presents visual-inertial datasets collected on-board a micro aerial vehicle. The datasets contain synchronized stereo images, IMU measurements and accurate ground truth. The first batch of datasets facilitates the design and evalua-tion of visual-inertial localization algorithms on real flight data. It was collected in an industrial environment and contains millimeter accurate position ground truth from a laser tracking system. The second batch of datasets is aimed at precise 3D environment reconstruction and was recorded in a room equipped with a motion capture system. The datasets contain 6D pose ground truth and a detailed 3D scan of the environment. Eleven datasets are provided in total, ranging from slow flights under good visual conditions to dynamic flights with motion blur and poor illumination, enabling researchers to thoroughly test and evaluate their algorithms. All datasets contain raw sensor measurements, spatio-temporally aligned sensor data and ground truth, extrinsic and intrinsic calibrations and datasets for custom calibrations.},
archivePrefix = {arXiv},
arxivId = {arXiv:1508.04886v1},
author = {Burri, Michael and Nikolic, Janosch and Gohl, Pascal and Schneider, Thomas and Rehder, Joern and Omari, Sammy and Achtelik, Markus W and Siegwart, Roland},
doi = {10.1177/0278364915620033},
eprint = {arXiv:1508.04886v1},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Burri et al. - 2016 - The EuRoC micro aerial vehicle datasets.pdf:pdf},
isbn = {0037549716666},
issn = {0278-3649},
journal = {The International Journal of Robotics Research},
keywords = {dataset,ground truth,mav,visual-inertial},
number = {10},
pages = {1157--1163},
title = {{The EuRoC micro aerial vehicle datasets}},
url = {http://journals.sagepub.com/doi/10.1177/0278364915620033},
volume = {35},
year = {2016}
}
@article{Zhong2017,
abstract = {Exiting deep-learning based dense stereo matching methods often rely on ground-truth disparity maps as the training signals, which are however not always available in many situations. In this paper, we design a simple convolutional neural network architecture that is able to learn to compute dense disparity maps directly from the stereo inputs. Training is performed in an end-to-end fashion without the need of ground-truth disparity maps. The idea is to use image warping error (instead of disparity-map residuals) as the loss function to drive the learning process, aiming to find a depth-map that minimizes the warping error. While this is a simple concept well-known in stereo matching, to make it work in a deep-learning framework, many non-trivial challenges must be overcome, and in this work we provide effective solutions. Our network is self-adaptive to different unseen imageries as well as to different camera settings. Experiments on KITTI and Middlebury stereo benchmark datasets show that our method outperforms many state-of-the-art stereo matching methods with a margin, and at the same time significantly faster.},
archivePrefix = {arXiv},
arxivId = {1709.00930},
author = {Zhong, Yiran and Dai, Yuchao and Li, Hongdong},
eprint = {1709.00930},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Zhong, Dai, Li - 2017 - Self-Supervised Learning for Stereo Matching with Self-Improving Ability.pdf:pdf},
month = {sep},
title = {{Self-Supervised Learning for Stereo Matching with Self-Improving Ability}},
url = {http://arxiv.org/abs/1709.00930},
year = {2017}
}
@article{Geiger2011a,
abstract = {Accurate 3d perception from video sequences is a core subject in computer vision and robotics, since it forms the basis of subsequent scene analysis. In practice however, online requirements often severely limit the utilizable camera resolution and hence also reconstruction accuracy. Furthermore, real-time systems often rely on heavy parallelism which can prevent applications in mobile devices or driver assistance systems, especially in cases where FPGAs cannot be employed. This paper proposes a novel approach to build 3d maps from high-resolution stereo sequences in real-time. Inspired by recent progress in stereo matching, we propose a sparse feature matcher in conjunction with an efficient and robust visual odometry algorithm. Our reconstruction pipeline combines both techniques with efficient stereo matching and a multi-view linking scheme for generating consistent 3d point clouds. In our experiments we show that the proposed odometry method achieves state-of-the-art accuracy. Including feature matching, the visual odometry part of our algorithm runs at 25 frames per second, while - at the same time - we obtain new depth maps at 3-4 fps, sufficient for online 3d reconstructions.},
author = {Geiger, Andreas and Ziegler, Julius and Stiller, Christoph},
doi = {10.1109/IVS.2011.5940405},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Geiger, Ziegler, Stiller - 2011 - StereoScan Dense 3d reconstruction in real-time.pdf:pdf},
isbn = {9781457708909},
issn = {1931-0587},
journal = {IEEE Intelligent Vehicles Symposium, Proceedings},
number = {Iv},
pages = {963--968},
pmid = {24344074},
title = {{StereoScan: Dense 3d reconstruction in real-time}},
year = {2011}
}
@article{Zia2016,
abstract = {SLAM has matured significantly over the past few years, and is beginning to appear in serious commercial products. While new SLAM systems are being proposed at every conference, evaluation is often restricted to qualitative visualizations or accuracy estimation against a ground truth. This is due to the lack of benchmarking methodologies which can holistically and quantitatively evaluate these systems. Further investigation at the level of individual kernels and parameter spaces of SLAM pipelines is non-existent, which is absolutely essential for systems research and integration. We extend the recently introduced SLAMBench framework to allow comparing two state-of-the-art SLAM pipelines, namely KinectFusion and LSD-SLAM, along the metrics of accuracy, energy consumption, and processing frame rate on two different hardware platforms, namely a desktop and an embedded device. We also analyze the pipelines at the level of individual kernels and explore their algorithmic and hardware design spaces for the first time, yielding valuable insights.},
archivePrefix = {arXiv},
arxivId = {1509.04648},
author = {Zia, M. Zeeshan and Nardi, Luigi and Jack, Andrew and Vespa, Emanuele and Bodin, Bruno and Kelly, Paul H.J. and Davison, Andrew J.},
doi = {10.1109/ICRA.2016.7487261},
eprint = {1509.04648},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Zia et al. - 2016 - Comparative design space exploration of dense and semi-dense SLAM.pdf:pdf},
isbn = {9781467380263},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
pages = {1292--1299},
title = {{Comparative design space exploration of dense and semi-dense SLAM}},
volume = {2016-June},
year = {2016}
}
@article{Schmid2013a,
abstract = {We introduce our new quadrotor platform for re- alizing autonomous navigation in unknown indoor/outdoor en- vironments. Autonomous waypoint navigation, obstacle avoid- ance and flight control is implemented on-board. The system does not require a special environment, artificial markers or an external reference system. We developed a monolithic, mechanically damped perception unit which is equipped with a stereo camera pair, an Inertial Measurement Unit (IMU), two processor-and an FPGA board. Stereo images are processed on the FPGA by the Semi-Global Matching algorithm. Keyframe- based stereo odometry is fused with IMU data compensating for time delays that are induced by the vision pipeline. The system state estimate is used for control and on-board 3D mapping. An operator can set waypoints in the map, while the quadrotor autonomously plans its path avoiding obstacles. We show experiments with the quadrotor flying from inside a building to the outside and vice versa, traversing a window and a door respectively. A video of the experiments is part of this work. To the best of our knowledge, this is the first autonomously flying system with complete on-board processing that performs waypoint navigation with obstacle avoidance in geometrically unconstrained, complex indoor/outdoor environments.},
author = {Schmid, Korbinian and Tomic, Teodor and Ruess, Felix and Hirschmuller, Heiko and Suppa, Michael},
doi = {10.1109/IROS.2013.6696922},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Schmid et al. - 2013 - Stereo vision based indooroutdoor navigation for flying robots.pdf:pdf},
isbn = {9781467363587},
issn = {21530858},
journal = {IEEE International Conference on Intelligent Robots and Systems},
pages = {3955--3962},
title = {{Stereo vision based indoor/outdoor navigation for flying robots}},
year = {2013}
}
@incollection{Dey2016a,
archivePrefix = {arXiv},
arxivId = {1411.6326v1},
author = {Dey, Debadeepta and Shankar, Kumar Shaurya and Zeng, Sam and Mehta, Rupesh and Agcayazi, M Talha and Eriksen, Christopher and Daftry, Shreyansh and Hebert, Martial and Bagnell, J Andrew},
booktitle = {Springer Tracts in Advanced Robotics},
editor = {Wettergreen, D. and Barfoot, T.},
eprint = {1411.6326v1},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Dey et al. - 2016 - Vision and Learning for Deliberative Monocular Cluttered Flight.pdf:pdf},
number = {Field and Service Robotics},
pages = {391--409},
publisher = {Springer},
title = {{Vision and Learning for Deliberative Monocular Cluttered Flight}},
volume = {113},
year = {2016}
}
@article{Hrabar2012,
abstract = {It is anticipatedthat theMars ScienceLaboratory rover, namedCuriosity,will traverse 10–20 kmon the surface of Mars during its primary mission. In preparation for this traverse, Earth-based tests were performed using Mars weight vehicles. These vehicles were driven over Mars analog bedrock, cohesive soil, and cohesionless sand at various slopes. Vehicle slip was characterized on each of these terrains versus slope for direct upslope driving. Results show that slopes up to 22 degrees are traversable on smooth bedrock and that slopes up to 28 degrees are traversable on some cohesive soils. In cohesionless sand, results show a sharp transition between moderate slip on 10 degree slopes and vehicle embedding at 17 degrees. For cohesionless sand, data are also presented showing the relationship between vehicle slip and wheel sinkage. Side by side testing of the Mars Exploration Rover test vehicle and the Mars Science Laboratory test vehicle show how increased wheel diameter leads to better slope climbing ability in sand for vehicles with nearly identical ground pressure. Lastly, preliminary data from Curiosity's initial driving on Mars are presented and compared to the Earth-based testing, showing good agreement for the driving done during the first 250 Martian days.},
archivePrefix = {arXiv},
arxivId = {10.1.1.91.5767},
author = {Hrabar, Stefan},
doi = {10.1002/rob.21404},
eprint = {10.1.1.91.5767},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Hrabar - 2012 - An evaluation of stereo and laser-based range sensing for rotorcraft unmanned aerial vehicle obstacle avoidance.pdf:pdf},
isbn = {9783902661623},
issn = {15564959},
journal = {Journal of Field Robotics},
keywords = {Backstepping,Non-linear control,Path-following},
month = {mar},
number = {2},
pages = {215--239},
pmid = {22164016},
title = {{An evaluation of stereo and laser-based range sensing for rotorcraft unmanned aerial vehicle obstacle avoidance}},
url = {http://doi.wiley.com/10.1002/rob.21404},
volume = {29},
year = {2012}
}
@inproceedings{DeWagter2005,
author = {de Wagter, C. and Mulder, J.A.},
booktitle = {AIAA Guidance, Navigation, and Control Conference and Exhibit},
doi = {10.2514/6.2005-5872},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/de Wagter, Mulder - 2005 - Towards Vision-Based UAV Situation Awareness.pdf:pdf},
number = {August},
pages = {1--16},
title = {{Towards Vision-Based UAV Situation Awareness}},
year = {2005}
}
@article{Tijmons2016,
abstract = {The development of autonomous lightweight MAVs, capable of navigating in unknown indoor environments, is one of the major challenges in robotics. The complexity of this challenge comes from constraints on weight and power consumption of onboard sensing and processing devices. In this paper we propose the "Droplet" strategy, an avoidance strategy that outperforms reactive avoidance strategies by allowing constant speed maneuvers while being computationally extremely efficient. The strategy deals with nonholonomic motion constraints of most fixed and flapping wing platforms, and with the limited field-of-view of stereo camera systems. It guarantees obstacle-free flight in the absence of sensor and motor noise. We first analyze the strategy in simulation, and then show its robustness in real-world conditions by implementing it on a 21-gram flapping wing MAV.},
archivePrefix = {arXiv},
arxivId = {1604.00833},
author = {Tijmons, Sjoerd and de Croon, Guido and Remes, Bart and {De Wagter}, Christophe and Mulder, Max},
eprint = {1604.00833},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Tijmons et al. - 2016 - Obstacle Avoidance Strategy using Onboard Stereo Vision on a Flapping Wing MAV.pdf:pdf},
keywords = {TU Delft,experiment,low complexity,nieuwe referenties,obstacle avoidance,reactive obstacle avoidance,safe region,scenario: room,simulation,stereo vision},
mendeley-tags = {TU Delft,experiment,low complexity,nieuwe referenties,obstacle avoidance,reactive obstacle avoidance,safe region,scenario: room,simulation,stereo vision},
pages = {1--13},
title = {{Obstacle Avoidance Strategy using Onboard Stereo Vision on a Flapping Wing MAV}},
url = {http://arxiv.org/abs/1604.00833},
year = {2016}
}
@article{Tippetts2011,
author = {Tippetts, Beau J and Lee, Dah-jye and Archibald, James K and Lillywhite, Kirt D},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Tippetts et al. - 2011 - Dense Disparity Real-Time Stereo Vision Algorithm for Resource-Limited Systems.pdf:pdf},
number = {10},
pages = {1547--1555},
title = {{Dense Disparity Real-Time Stereo Vision Algorithm for Resource-Limited Systems}},
volume = {21},
year = {2011}
}
@article{Scherer2008,
author = {Scherer, Sebastian and Singh, Sanjiv and Chamberlain, Lyle and Elgersma, Mike},
doi = {10.1177/0278364908090949},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Scherer et al. - 2008 - Flying Fast and Low Among Obstacles Methodology and Experiments.pdf:pdf},
isbn = {0278364908090},
journal = {The International Journal of Robotics Research},
keywords = {aerial robotics,learning},
number = {5},
pages = {549--574},
title = {{Flying Fast and Low Among Obstacles: Methodology and Experiments}},
volume = {27},
year = {2008}
}
@inproceedings{Marzat2009,
author = {Marzat, J. and Dumortier, Y. and Ducrot, Andre},
booktitle = {WSCG '2009: Full Papers Proceedings: The 17th International Conference in Central Europe on Computer Graphics, Visualization and Computer Vision in co-operation with EUROGRAPHICS},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Marzat, Dumortier, Ducrot - 2009 - Real-time dense and accurate parallel optical flow using CUDA.pdf:pdf},
isbn = {9788086943930},
keywords = {cuda,gpu,image processing,monocular vision,optical flow,parallel processing},
pages = {105--112},
title = {{Real-time dense and accurate parallel optical flow using CUDA}},
url = {http://julien.marzat.free.fr/2008{\_}Stage{\_}Ingenieur{\_}INRIA/WSCG09{\_}Marzat{\_}Dumortier{\_}Ducrot.pdf},
year = {2009}
}
@inproceedings{Barry2015a,
abstract = {We present a novel stereo vision algorithm that is capable of obstacle detection on a mobile-CPU processor at 120 frames per second. Our system performs a subset of standard block-matching stereo processing, searching only for obstacles at a single depth. By using an onboard IMU and state-estimator, we can recover the position of obstacles at all other depths, building and updating a full depth-map at framerate. Here, we describe both the algorithm and our implementation on a high-speed, small UAV, flying at over 20 MPH (9 m/s) close to obstacles. The system requires no external sensing or computation and is, to the best of our knowledge, the first high-framerate stereo detection system running onboard a small UAV.},
archivePrefix = {arXiv},
arxivId = {1407.7091},
author = {Barry, Andrew J. and Tedrake, Russ},
booktitle = {International Conference on Robotics and Automation (ICRA)},
doi = {10.1109/ICRA.2015.7139617},
eprint = {1407.7091},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Barry, Tedrake - 2015 - Pushbroom Stereo for High-Speed Navigation in Cluttered Environments.pdf:pdf},
isbn = {9781479969227},
keywords = {cartesian map,experiment,low complexity,nieuwe referenties,obstacle detection,stereo vision,voxel map},
mendeley-tags = {cartesian map,experiment,low complexity,nieuwe referenties,obstacle detection,stereo vision,voxel map},
pages = {3046--3052},
title = {{Pushbroom Stereo for High-Speed Navigation in Cluttered Environments}},
url = {http://arxiv.org/abs/1407.7091},
year = {2015}
}
@inproceedings{Liu2017,
author = {Liu, Peidong and Heng, Lionel and Sattler, Torsten and Geiger, Andreas and Pollefeys, Marc},
booktitle = {International Conference on Intelligent Robots and Systems (IROS)},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Liu et al. - 2017 - Direct Visual Odometry for a Fisheye-Stereo Camera.pdf:pdf},
title = {{Direct Visual Odometry for a Fisheye-Stereo Camera}},
year = {2017}
}
@article{Pantilie2010,
abstract = {Mobile robots as well as tomorrows intelligent vehicles acting in complex dynamic environments must be able to detect both static and moving obstacles. In intersections or crowded urban areas this task proves to be highly demanding. Stereo vision has been extensively used for this task, as it provides a large amount of data. Since it does not reveal any motion information, static and dynamic objects immediately next to each other, or closely positioned obstacles moving in different directions are often merged into a single obstacle. In this paper we address these problems through a powerful fusion between 3D position information delivered by the stereo sensor and 3D motion information, derived from optical flow, in a depth-adaptive occupancy grid. The proposed model is presented and then applied for determining obstacle localization, orientation and speed.},
author = {Pantilie, C.D. and Nedevschi, S.},
doi = {10.1109/ITSC.2010.5625174},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Pantilie, Nedevschi - 2010 - Real-time obstacle detection in complex scenarios using dense stereo vision and optical flow.pdf:pdf},
isbn = {978-1-4244-7657-2},
journal = {3th International IEEE Conference on Intelligent Transportation Systems (ITSC)},
keywords = {3D motion information,3D position information,collision avoidance,dense stereo vision,depth-adaptive occupancy grid,experiment,intelligent vehicle,mobile robots,moving obstacle,moving obstacles,nieuwe referenties,obstacle detection,obstacle localization,optical flow,polar map,real-time obstacle detection,real-time systems,robot vision,stereo image processing,stereo sensor,stereo vision},
mendeley-tags = {experiment,moving obstacles,nieuwe referenties,obstacle detection,optical flow,polar map,stereo vision},
pages = {439--444},
title = {{Real-time obstacle detection in complex scenarios using dense stereo vision and optical flow}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=5625174},
year = {2010}
}
@article{Li2013,
author = {Li, Mingyang and Mourikis, Anastasios I},
doi = {10.1177/0278364913481251},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Li, Mourikis - 2013 - High-precision, consistent EKF-based visual-inertial odometry.pdf:pdf},
journal = {The International Journal of Robotics Research},
keywords = {extended kalman filter consistency,vision-aided inertial navigation,visual-inertial odometry,visual-inertial slam},
number = {6},
pages = {690--711},
title = {{High-precision, consistent EKF-based visual-inertial odometry}},
volume = {32},
year = {2013}
}
@article{Faessler2016,
author = {Faessler, Matthias and Fontana, Flavio and Forster, Christian and Mueggler, Elias and Pizzoli, Matia and Scaramuzza, Davide},
doi = {10.1002/rob},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Faessler et al. - 2016 - Autonomous, Vision-based Flight and Live Dense 3D Mapping with a Quadrotor Micro Aerial Vehicle.pdf:pdf},
journal = {Journal of Field Robotics},
number = {4},
pages = {431--450},
title = {{Autonomous, Vision-based Flight and Live Dense 3D Mapping with a Quadrotor Micro Aerial Vehicle}},
volume = {33},
year = {2016}
}
@article{Alcantarilla2012a,
author = {Alcantarilla, Pablo F and Bergasa, Luis M},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Alcantarilla et al. - 2012 - On combining visual SLAM and dense scene flow to increase the robustness of localization and mapping in dyn.pdf:pdf},
isbn = {9781467314053},
pages = {1290--1297},
title = {{On Combining Visual SLAM and Dense Scene Flow to Increase the Robustness of Localization and Mapping in Dynamic Environments}},
year = {2012}
}
@article{Borenstein1989,
author = {Borenstein, J and Koren, Y},
doi = {10.1109/21.44033},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Borenstein, Koren - 1989 - Real-time obstacle avoidance for fast mobile robots.pdf:pdf},
issn = {0018-9472},
journal = {IEEE Transactions on Systems, Man, and Cybernetics},
month = {sep},
number = {5},
pages = {1179--1187},
title = {{Real-time obstacle avoidance for fast mobile robots}},
url = {http://ieeexplore.ieee.org/document/44033/},
volume = {19},
year = {1989}
}
@article{Tijmons2017,
archivePrefix = {arXiv},
arxivId = {arXiv:1604.00833v2},
author = {Tijmons, S. and de Croon, G.C.H.E. and Remes, B.D.W. and {De Wagter}, C. and Mulder, M.},
eprint = {arXiv:1604.00833v2},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Tijmons et al. - 2017 - Obstacle Avoidance Strategy using Onboard Stereo Vision on a Flapping Wing MAV.pdf:pdf},
journal = {IEEE Transactions on Robotics},
number = {4},
pages = {858--874},
title = {{Obstacle Avoidance Strategy using Onboard Stereo Vision on a Flapping Wing MAV}},
volume = {33},
year = {2017}
}
@article{Mur-Artal2017,
author = {Mur-Artal, R. and Tard{\'{o}}s, J.D.},
doi = {10.1109/TRO.2017.2705103},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Mur-Artal, Tard{\'{o}}s - 2017 - ORB-SLAM2 An Open-Source SLAM System for Monocular, Stereo, and RGB-D Cameras.pdf:pdf},
journal = {IEEE Transactions on Robotics},
number = {5},
pages = {1255--1262},
title = {{ORB-SLAM2: An Open-Source SLAM System for Monocular, Stereo, and RGB-D Cameras}},
volume = {33},
year = {2017}
}
@inproceedings{Geiger2011,
author = {Geiger, Andreas and Roser, Martin and Urtasun, Raquel},
booktitle = {Computer Vision – ACCV 2010},
doi = {10.1007/978-3-642-19315-6_3},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Geiger, Roser, Urtasun - 2011 - Efficient Large-Scale Stereo Matching.pdf:pdf},
isbn = {978-3-642-19314-9},
pages = {25--38},
title = {{Efficient Large-Scale Stereo Matching}},
year = {2011}
}
@article{DeCroon2017,
archivePrefix = {arXiv},
arxivId = {arXiv:1709.08126v1},
author = {de Croon, G.C.H.E.},
eprint = {arXiv:1709.08126v1},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/de Croon - 2017 - Self-supervised learning When is fusion of the primary and secondary sensor cue useful.pdf:pdf},
journal = {arXiv preprint},
number = {arXiv:1709.08126},
title = {{Self-supervised learning: When is fusion of the primary and secondary sensor cue useful?}},
year = {2017}
}
@article{Zhong2017,
abstract = {Exiting deep-learning based dense stereo matching methods often rely on ground-truth disparity maps as the training signals, which are however not always available in many situations. In this paper, we design a simple convolutional neural network architecture that is able to learn to compute dense disparity maps directly from the stereo inputs. Training is performed in an end-to-end fashion without the need of ground-truth disparity maps. The idea is to use image warping error (instead of disparity-map residuals) as the loss function to drive the learning process, aiming to find a depth-map that minimizes the warping error. While this is a simple concept well-known in stereo matching, to make it work in a deep-learning framework, many non-trivial challenges must be overcome, and in this work we provide effective solutions. Our network is self-adaptive to different unseen imageries as well as to different camera settings. Experiments on KITTI and Middlebury stereo benchmark datasets show that our method outperforms many state-of-the-art stereo matching methods with a margin, and at the same time significantly faster.},
archivePrefix = {arXiv},
arxivId = {1709.00930},
author = {Zhong, Yiran and Dai, Yuchao and Li, Hongdong},
eprint = {1709.00930},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Zhong, Dai, Li - 2017 - Self-Supervised Learning for Stereo Matching with Self-Improving Ability.pdf:pdf},
month = {sep},
title = {{Self-Supervised Learning for Stereo Matching with Self-Improving Ability}},
url = {http://arxiv.org/abs/1709.00930},
year = {2017}
}
@article{Kerl2015,
abstract = {Abstract We propose a dense continuous-time tracking and mapping method for RGB-D cameras. We parametrize the camera trajectory using continuous B-splines and optimize the trajectory through dense, direct image alignment. Our method also directly models rolling ...$\backslash$n},
author = {Kerl, Christian and Stuckler, Jorg and Cremers, Daniel},
doi = {10.1109/ICCV.2015.261},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Kerl, Stuckler, Cremers - 2015 - Dense continuous-time tracking and mapping with rolling shutter RGB-D cameras.pdf:pdf},
isbn = {9781467383912},
issn = {15505499},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
pages = {2264--2272},
pmid = {389660},
title = {{Dense continuous-time tracking and mapping with rolling shutter RGB-D cameras}},
volume = {2015 Inter},
year = {2015}
}
@article{Santoso2017,
abstract = {— In this paper, we comprehensively discuss the current progress of visual–inertial (VI) navigation systems and sensor fusion research with a particular focus on small unmanned aerial vehicles, known as microaerial vehicles (MAVs). Such fusion has become very topical due to the complementary characteristics of the two sensing modalities. We discuss the pros and cons of the most widely implemented VI systems against the navigational and maneuvering capabilities of MAVs. Considering the issue of optimum data fusion from multiple heterogeneous sensors, we examine the potential of the most widely used advanced state estimation techniques (both linear and nonlinear as well as Bayesian and non-Bayesian) against various MAV design considerations. Finally, we highlight several research opportunities and potential challenges associated with each technique. Note to Practitioners—Robotic aircraft have been widely implemented to improve safety, efficiency, and productivity (e.g., agriculture, law enforcement, building inspections, and so on). As a part of its autonomous navigation system, this review aims to address several aspects of VI navigation systems both from data fusion and technological perspectives. Index Terms— Microaerial vehicles (MAVs), sensor fusion, visual–inertial (VI) navigation systems.},
author = {Santoso, Fendy and Garratt, Matthew A. and Anavatti, Sreenatha G.},
doi = {10.1109/TASE.2016.2582752},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Santoso, Garratt, Anavatti - 2017 - Visual-inertial navigation systems for aerial robotics Sensor fusion and technology.pdf:pdf},
issn = {15455955},
journal = {IEEE Transactions on Automation Science and Engineering},
keywords = {Microaerial vehicles (MAVs),Sensor fusion,Visual-inertial (VI) navigation systems},
number = {1},
pages = {260--275},
title = {{Visual-inertial navigation systems for aerial robotics: Sensor fusion and technology}},
volume = {14},
year = {2017}
}
@phdthesis{TomasCardosoRezioMartins2017,
author = {{Tom{\'{a}}s Cardoso R{\'{e}}zio Martins}, Diogo},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Tom{\'{a}}s Cardoso R{\'{e}}zio Martins - 2017 - Fusion of stereo and monocular depth estimates in a self-supervised learning context.pdf:pdf},
school = {Delft University of Technology},
title = {{Fusion of stereo and monocular depth estimates in a self-supervised learning context}},
type = {MSc thesis},
year = {2017}
}
@article{Mori2013,
abstract = {Obstacle avoidance is desirable for lightweight micro aerial vehicles and is a challenging problem since the payload constraints only permit monocular cameras and obstacles cannot be directly observed. Depth can however be inferred based on various cues in the image. Prior work has examined optical flow, and perspective cues, however these methods cannot handle frontal obstacles well. In this paper we examine the problem of detecting obstacles right in front of the vehicle. We developed a method to detect relative size changes of image patches that is able to detect size changes in the absence of optical flow. The method uses SURF feature matches in combination with template matching to compare relative obstacle sizes with different image spacing. We present results from our algorithm in autonomous flight tests on a small quadrotor. We are able to detect obstacles with a frame- to-frame enlargement of 120{\%} with a high confidence and confirmed our algorithm in 20 successful flight experiments. In future work, we will improve the control algorithms to avoid more complicated obstacle configurations},
annote = {SIFT feature scale verandering, apparent size om obstakels te herkennen. Mooi overzicht van obstacle detection technieken, daarom survey tag.},
author = {Mori, Tomoyuki and Scherer, Sebastian},
doi = {10.1109/ICRA.2013.6630807},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Mori, Scherer - 2013 - First results in detecting and avoiding frontal obstacles from a monocular camera for micro unmanned aerial vehic.pdf:pdf},
isbn = {9781467356411},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
keywords = {expansion rate,experiment,highlight,low complexity,monocular,nieuwe referenties,obstacle avoidance,obstacle detection,reactive obstacle avoidance,scenario: forest,simulation,survey},
mendeley-tags = {expansion rate,experiment,highlight,low complexity,monocular,nieuwe referenties,obstacle avoidance,obstacle detection,reactive obstacle avoidance,scenario: forest,simulation,survey},
pages = {1750--1757},
title = {{First results in detecting and avoiding frontal obstacles from a monocular camera for micro unmanned aerial vehicles}},
year = {2013}
}
@phdthesis{Lamers2016,
author = {Lamers, K},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Lamers - 2016 - Self-Supervised Monocular Distance Learning on a Lightweight Micro Air Vehicle.pdf:pdf},
school = {Delft University of Technology},
title = {{Self-Supervised Monocular Distance Learning on a Lightweight Micro Air Vehicle}},
type = {MSc},
year = {2016}
}
@article{Coombs1998,
abstract = {The lure of using motion vision as a fundamental element in the$\backslash$nperception of space drives this effort to use flow features as the sole$\backslash$ncues for robot mobility. Real-time estimates of image flow and flow$\backslash$ndivergence provide the robot's sense of space. The robot steers down a$\backslash$nconceptual corridor, comparing left and right peripheral flows. Large$\backslash$ncentral flow divergence warns the robot of impending collisions at$\backslash$n{\&}ldquo;dead ends{\&}rdquo;. When this occurs, the robot turns around and$\backslash$nresumes wandering. Behavior is generated by directly using flow-based$\backslash$ninformation in the 2D image sequence. Active mechanical gaze$\backslash$nstabilization simplifies the visual interpretation problems by reducing$\backslash$ncamera rotation. By combining corridor following and dead-end$\backslash$ndeflection, the robot has wandered around the lab at 30 cm/s for as long$\backslash$nas 20 min without collision. The ability to support this behavior in$\backslash$nreal-time with current equipment promises expanded capabilities as$\backslash$ncomputational power increases in the future},
author = {Coombs, David and Herman, Martin and Hong, Tsai Hong and Nashman, Marilyn},
doi = {10.1109/70.660840},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Coombs et al. - 1998 - Real-time obstacle avoidance using central flow divergence, and peripheral flow.pdf:pdf},
isbn = {0-8186-7042-8},
issn = {1042296X},
journal = {IEEE Transactions on Robotics and Automation},
keywords = {Active vision,Digital control,Feedback systems,Image motion analysis,Mobile robot motion planning,Mobile robots,Object identification,Real time systems,Recursive estimation,Robot vision systems},
number = {1},
pages = {49--59},
title = {{Real-time obstacle avoidance using central flow divergence, and peripheral flow}},
volume = {14},
year = {1998}
}
@article{Borenstein1991,
abstract = {A real-time obstacle avoidance method for mobile robots which has$\backslash$nbeen developed and implemented is described. This method, named the$\backslash$nvector field histogram (VFH), permits the detection of unknown obstacles$\backslash$nand avoids collisions while simultaneously steering the mobile robot$\backslash$ntoward the target. The VFH method uses a two-dimensional Cartesian$\backslash$nhistogram grid as a world model. This world model is updated$\backslash$ncontinuously with range data sampled by onboard range sensors. The VFH$\backslash$nmethod subsequently uses a two-stage data-reduction process to compute$\backslash$nthe desired control commands for the vehicle. Experimental results from$\backslash$na mobile robot traversing densely cluttered obstacle courses in smooth$\backslash$nand continuous motion and at an average speed of 0.6-0.7 m/s are shown.$\backslash$nA comparison of the VFN method to earlier methods is given},
author = {Borenstein, Johann and Koren, Yoram},
doi = {10.1109/70.88137},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Borenstein, Koren - 1991 - The Vector Field Histogram - Fast Obstacle Avoidance for Mobile Robots.pdf:pdf},
isbn = {1042-296X VO - 7},
issn = {1042296X},
journal = {IEEE Transactions on Robotics and Automation},
keywords = {cartesian map,low complexity,obstacle avoidance,polar map,reactive obstacle avoidance},
mendeley-tags = {cartesian map,low complexity,obstacle avoidance,polar map,reactive obstacle avoidance},
number = {3},
pages = {278--288},
pmid = {88137},
title = {{The Vector Field Histogram - Fast Obstacle Avoidance for Mobile Robots}},
volume = {7},
year = {1991}
}
@article{Pham2015,
abstract = {Collision avoidance is a key factor in enabling the integration of unmanned aerial vehicle into real life use, whether it is in military or civil application. For a long time there have been a large number of works to address this problem; therefore a comparative summary of them would be desirable. This paper presents a survey on the major collision avoidance systems developed in up to date publications. Each collision avoidance system contains two main parts: sensing and detection, and collision avoidance. Based on their characteristics each part is divided into different categories; and those categories are explained, compared and discussed about advantages and disadvantages in this paper.},
archivePrefix = {arXiv},
arxivId = {1508.07723},
author = {Pham, Hung and Smolka, Scott A and Stoller, Scott D and Phan, Dung and Yang, Junxing},
eprint = {1508.07723},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Pham et al. - 2015 - A survey on unmanned aerial vehicle collision avoidance systems.pdf:pdf},
journal = {arXiv preprint},
month = {aug},
number = {arXiv:1508.07723},
title = {{A survey on unmanned aerial vehicle collision avoidance systems}},
url = {http://arxiv.org/abs/{\%}5Cnhttp://arxiv.org/abs/1508.07723 http://arxiv.org/abs/1508.07723},
year = {2015}
}
@article{Alvarez2016,
author = {Alvarez, H and Paz, L M and Sturm, J and Cremers, D},
doi = {10.1007/978-3-319-23778-7},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Alvarez et al. - 2016 - Collision Avoidance for Quadrotors with a Monocular Camera.pdf:pdf},
isbn = {9783319237787},
journal = {Springer Tracts in Advanced Robotics},
number = {Experimental Robotics},
pages = {195--209},
title = {{Collision Avoidance for Quadrotors with a Monocular Camera}},
volume = {109},
year = {2016}
}
@article{Forster2017,
author = {Forster, Christian and Zhang, Zichao and Gassner, Michael and Werlberger, Manuel and Scaramuzza, Davide},
doi = {10.1109/TRO.2016.2623335},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Forster et al. - 2017 - SVO Semidirect Visual Odometry for Monocular and Multicamera Systems.pdf:pdf},
journal = {IEEE Transactions on Robotics},
number = {2},
pages = {249--265},
title = {{SVO: Semidirect Visual Odometry for Monocular and Multicamera Systems}},
volume = {33},
year = {2017}
}
@article{Tijmons2017,
archivePrefix = {arXiv},
arxivId = {arXiv:1604.00833v2},
author = {Tijmons, S. and de Croon, G.C.H.E. and Remes, B.D.W. and {De Wagter}, C. and Mulder, M.},
eprint = {arXiv:1604.00833v2},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Tijmons et al. - 2017 - Obstacle Avoidance Strategy using Onboard Stereo Vision on a Flapping Wing MAV.pdf:pdf},
journal = {IEEE Transactions on Robotics},
number = {4},
pages = {858--874},
title = {{Obstacle Avoidance Strategy using Onboard Stereo Vision on a Flapping Wing MAV}},
volume = {33},
year = {2017}
}
@article{Heng2011,
abstract = {We present a novel stereo-based obstacle avoidance system on a vision-guided micro air vehicle (MAV) that is capable of fully autonomous maneuvers in unknown and dynamic environments. All algorithms run exclusively on the vehicle's on-board computer, and at high frequencies that allow the MAV to react quickly to obstacles appearing in its flight trajectory. Our MAV platform is a quadrotor aircraft equipped with an inertial measurement unit and two stereo rigs. An obstacle mapping algorithm processes stereo images, producing a 3D map representation of the environment; at the same time, a dynamic anytime path planner plans a collision-free path to a goal point.},
author = {Heng, Lionel and Meier, Lorenz and Tanskanen, Petri and Fraundorfer, Friedrich and Pollefeys, Marc},
doi = {10.1109/ICRA.2011.5980095},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Heng et al. - 2011 - Autonomous obstacle avoidance and maneuvering on a vision-guided MAV using on-board processing.pdf:pdf},
isbn = {9781612843865},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
keywords = {cartesian map,deliberate obstacle avoidance,low complexity,obstacle avoidance},
mendeley-tags = {cartesian map,deliberate obstacle avoidance,low complexity,obstacle avoidance},
pages = {2472--2477},
title = {{Autonomous obstacle avoidance and maneuvering on a vision-guided MAV using on-board processing}},
year = {2011}
}
@inproceedings{Abeywardena2016,
archivePrefix = {arXiv},
arxivId = {1607.01486},
author = {Abeywardena, Dinuka and {Shoudong Huang} and Barnes, Ben and Dissanayake, Gamini and Kodagoda, Sarath},
booktitle = {2016 IEEE International Conference on Robotics and Automation (ICRA)},
doi = {10.1109/ICRA.2016.7487290},
eprint = {1607.01486},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Abeywardena et al. - 2016 - Fast, on-board, model-aided visual-inertial odometry system for quadrotor micro aerial vehicles.pdf:pdf},
isbn = {978-1-4673-8026-3},
month = {may},
pages = {1530--1537},
publisher = {IEEE},
title = {{Fast, on-board, model-aided visual-inertial odometry system for quadrotor micro aerial vehicles}},
url = {http://ieeexplore.ieee.org/document/7487290/},
year = {2016}
}
@incollection{Wedel2009a,
author = {Wedel, Andreas and Pock, Thomas and Zach, Christopher and Bischof, Horst and Cremers, Daniel},
booktitle = {Statistical and Geometrical Approaches to Visual Motion Analysis. Lecture Notes in Computer Science, vol 5604},
doi = {10.1007/978-3-642-03061-1_2},
editor = {Cremers, D. and Rosenhahn, B. and Yuille, A.L. and Schmidt, F.R.},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Wedel et al. - 2009 - An Improved Algorithm for TV-L 1 Optical Flow.pdf:pdf},
pages = {23--45},
publisher = {Springer, Berlin, Heidelberg},
title = {{An Improved Algorithm for TV-}},
volume = {1},
year = {2009}
}
@article{Humenberger2010,
abstract = {This paper introduces a new segmentation-based approach for disparity optimization in stereo vision. The main contribution is a significant enhancement of the matching quality at occlusions and textureless areas by segmenting either the left color image or the calculated texture image. The local cost calculation is done with a Census-based correlation method and is compared with standard sum of absolute differences. The confidence of a match is measured and only non-confident or non-textured pixels are estimated by calculating a disparity plane for the corresponding segment. The quality of the local optimized matches is increased by a modified Semi-Global Matching (SGM) step with subpixel accuracy. In contrast to standard SGM, not the whole image is used for disparity optimization but horizontal stripes of the image. It is shown that this modification significantly reduces the memory consumption by nearly constant matching quality and thus enables embedded realization. Using the Middlebury ranking as evaluation criterion, it is shown that the proposed algorithm performs well in comparison to the pure Census correlation. It reaches a top ten rank if subpixel accuracy is supposed. Furthermore, the matching quality of the algorithm, especially of the texture-based plane fitting, is shown on two real-world scenes where a significant enhancement could be achieved.},
author = {Humenberger, Martin and Engelke, Tobias and Kubinger, Wilfried},
doi = {10.1109/CVPRW.2010.5543769},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Humenberger, Engelke, Kubinger - 2010 - A Census-based stereo vision algorithm using modified Semi-Global Matching and plane fitting to.pdf:pdf},
isbn = {9781424470297},
issn = {2160-7508},
journal = {2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition - Workshops, CVPRW 2010},
pages = {77--84},
title = {{A Census-based stereo vision algorithm using modified Semi-Global Matching and plane fitting to improve matching quality}},
year = {2010}
}
@article{Marzat2017,
author = {Marzat, Julien and Bertrand, Sylvain and Eudes, Alexandre},
doi = {10.1016/j.ifacol.2017.08.1910},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Marzat, Bertrand, Eudes - 2017 - Reactive MPC for Autonomous MAV Navigation in Indoor Cluttered Environments Flight Experiments.pdf:pdf},
issn = {24058963},
keywords = {flight experiments,localization and mapping,micro-air vehicles,model predictive control,vision-based},
title = {{Reactive MPC for Autonomous MAV Navigation in Indoor Cluttered Environments : Flight Experiments}},
year = {2017}
}
@article{Barron1994,
author = {Barron, J L and Fleet, D J and Beauchemin, S S},
doi = {10.1007/BF01420984},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Barron, Fleet, Beauchemin - 1994 - Performance of optical flow techniques.pdf:pdf},
issn = {0920-5691},
journal = {International Journal of Computer Vision},
month = {feb},
number = {1},
pages = {43--77},
title = {{Performance of optical flow techniques}},
url = {http://link.springer.com/10.1007/BF01420984},
volume = {12},
year = {1994}
}
@article{Schmid2014,
author = {Schmid, Korbinian and Lutz, Philipp and Tomi{\'{c}}, Teodor and Mair, Elmar and Hirschm{\"{u}}ller, Heiko},
doi = {10.1002/rob.21506},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Schmid et al. - 2014 - Autonomous Vision-based Micro Air Vehicle for Indoor and Outdoor Navigation.pdf:pdf},
journal = {Journal of Field Robotics},
number = {4},
pages = {537--570},
title = {{Autonomous Vision-based Micro Air Vehicle for Indoor and Outdoor Navigation}},
volume = {31},
year = {2014}
}
@article{Younes2016,
archivePrefix = {arXiv},
arxivId = {1607.00470v1},
author = {Younes, Georges and Asmar, Daniel and Shammas, Elie},
eprint = {1607.00470v1},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Younes, Asmar, Shammas - 2016 - A survey on non-filter-based monocular Visual SLAM systems.pdf:pdf},
journal = {arXiv preprint arXiv:1607.00470},
keywords = {monocular,non-filter based,visual slam},
title = {{A survey on non-filter-based monocular Visual SLAM systems}},
year = {2016}
}
@article{Shen2013a,
abstract = {This paper addresses the development of a light-weight autonomous quadrotor that uses cameras and an inexpensive IMU as its only sensors and onboard processors for estimation and control. We describe a fully-functional, integrated system with a focus on robust visual-inertial state estimation, and demonstrate the quadrotor's ability to autonomously travel at speeds up to 4 m/s and roll and pitch angles exceeding 20 degrees. The performance of the proposed system is demonstrated via challenging experiments in three dimensional indoor environments.},
author = {Shen, Shaojie and Mulgaonkar, Y and Michael, Nathan and Kumar, V},
doi = {10.15607/RSS.2013.IX.03},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Shen et al. - 2013 - Vision-based state estimation and trajectory control towards high-speed flight with a quadrotor.pdf:pdf},
journal = {Robotics: Science and {\ldots}},
title = {{Vision-based state estimation and trajectory control towards high-speed flight with a quadrotor}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.309.7992{\&}rep=rep1{\&}type=pdf},
year = {2013}
}
@article{Scharstein2002,
abstract = {Stereo matching is one of the most active research areas in computer vision. While a large number of algorithms for stereo correspondence have been developed, relatively little work has been done on characterizing their performance. In this paper, we present a taxonomy of dense, two-frame stereo methods designed to assess the different components and design decisions made in individual stereo algorithms. Using this taxonomy, we compare existing stereo methods and present experiments evaluating the performance of many different variants. In order to establish a common software platform and a collection of data sets for easy evaluation, we have designed a stand-alone, flexible C++ implementation that enables the evaluation of individual components and that can be easily extended to include new algorithms. We have also produced several new multiframe stereo data sets with ground truth, and are making both the code and data sets available on the Web},
author = {Scharstein, Daniel and Szeliski, Richard},
doi = {10.1023/A:1014573219977},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Scharstein, Szeliski - 2002 - A Taxonomy and Evaluation of Dense Two-Frame Stereo Correspondence Algorithms(2).pdf:pdf},
isbn = {0769513271},
issn = {09205691},
journal = {International Journal of Computer Vision},
keywords = {evaluation of performance,stereo correspondence software,stereo matching survey},
number = {1/3},
pages = {7--42},
pmid = {350},
title = {{A Taxonomy and Evaluation of Dense Two-Frame Stereo Correspondence Algorithms}},
url = {http://link.springer.com/10.1023/A:1014573219977},
volume = {47},
year = {2002}
}
@article{DeWagter2014,
abstract = {Autonomous flight of Flapping Wing Micro Air Vehicles (FWMAVs) is a major challenge in the field of robotics, due to their light weight and the flapping-induced body motions. In this article, we present the first FWMAV with onboard vision processing for autonomous flight in generic environments. In particular, we introduce the DelFly ‘Explorer', a 20-gram FWMAV equipped with a 0.98-gram autopilot and a 4.0-gram onboard stereo vision system. We explain the design choices that permit carrying the extended payload, while retaining the DelFly's hover capabilities. In addition, we introduce a novel stereo vision algorithm, LongSeq, designed specifically to cope with the flapping motion and the desire to attain a computational effort tuned to the frame rate. The onboard stereo vision system is illustrated in the context of an obstacle avoidance task in an environment with sparse obstacles.},
author = {{De Wagter}, C. and Tijmons, S. and Remes, B. D W and {De Croon}, G. C H E},
doi = {10.1109/ICRA.2014.6907589},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/De Wagter et al. - 2014 - Autonomous flight of a 20-gram Flapping Wing MAV with a 4-gram onboard stereo vision system.pdf:pdf},
isbn = {9781479936847},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
pages = {4982--4987},
title = {{Autonomous flight of a 20-gram Flapping Wing MAV with a 4-gram onboard stereo vision system}},
year = {2014}
}
@phdthesis{Nous2016a,
author = {Nous, C.W.M.},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Nous - 2016 - Performance in Obstacle Avoidance.pdf:pdf},
school = {Delft University of Technology},
title = {{Performance in Obstacle Avoidance}},
type = {MSc},
year = {2016}
}
@article{Baker2011,
abstract = {The quantitative evaluation of optical flow algorithms by Barron et al. led to significant advances in the performance of optical flow methods. The challenges for optical flow today go beyond the datasets and evaluation methods proposed in that paper and center on problems associated with nonrigid motion, real sensor noise, complex natural scenes, and motion discontinuities. Our goal is to establish a new set of benchmarks and evaluation methods for the next generation of optical flow algorithms. To that end, we contribute four types of data to test different aspects of optical flow algorithms: sequences with nonrigid motion where the ground-truth flow is determined by tracking hidden fluorescent texture; realistic synthetic sequences; high frame-rate video used to study interpolation error; and modified stereo sequences of static scenes. In addition to the average angular error used in Barron et al., we compute the absolute flow endpoint error, measures for frame interpolation error, improved statistics, and flow accuracy at motion boundaries and in textureless regions. We evaluate the performance of several well-known methods on this data to establish the current state of the art. Our database is freely available on the Web together with scripts for scoring and publication of the results at http://vision.middlebury.edu/flow/.},
archivePrefix = {arXiv},
arxivId = {1412.0767},
author = {Baker, Simon and Scharstein, Daniel and Lewis, J. P. and Roth, Stefan and Black, Michael J. and Szeliski, Richard},
doi = {10.1007/s11263-010-0390-2},
eprint = {1412.0767},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Baker et al. - 2011 - A database and evaluation methodology for optical flow.pdf:pdf},
isbn = {978-1-4244-1631-8},
issn = {09205691},
journal = {International Journal of Computer Vision},
keywords = {Algorithms,Benchmarks,Database,Evaluation,Metrics,Optical flow,Survey},
number = {1},
pages = {1--31},
pmid = {24356354},
title = {{A database and evaluation methodology for optical flow}},
volume = {92},
year = {2011}
}
@article{Magree2014,
author = {Magree, Daniel and Mooney, John G. and Johnson, Eric N.},
doi = {10.1007/s10846-013-9967-7},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Magree, Mooney, Johnson - 2014 - Monocular Visual Mapping for Obstacle Avoidance on UAVs.pdf:pdf},
journal = {J Intell Robot Syst},
keywords = {extended,kalman filter,monocualar vision,obstacle avoidance,terrain mapping,visual estimation},
pages = {17--26},
title = {{Monocular Visual Mapping for Obstacle Avoidance on UAVs}},
volume = {74},
year = {2014}
}
@article{Bibuli2007,
abstract = {It is anticipatedthat theMars ScienceLaboratory rover, namedCuriosity,will traverse 10–20 kmon the surface of Mars during its primary mission. In preparation for this traverse, Earth-based tests were performed using Mars weight vehicles. These vehicles were driven over Mars analog bedrock, cohesive soil, and cohesionless sand at various slopes. Vehicle slip was characterized on each of these terrains versus slope for direct upslope driving. Results show that slopes up to 22 degrees are traversable on smooth bedrock and that slopes up to 28 degrees are traversable on some cohesive soils. In cohesionless sand, results show a sharp transition between moderate slip on 10 degree slopes and vehicle embedding at 17 degrees. For cohesionless sand, data are also presented showing the relationship between vehicle slip and wheel sinkage. Side by side testing of the Mars Exploration Rover test vehicle and the Mars Science Laboratory test vehicle show how increased wheel diameter leads to better slope climbing ability in sand for vehicles with nearly identical ground pressure. Lastly, preliminary data from Curiosity's initial driving on Mars are presented and compared to the Earth-based testing, showing good agreement for the driving done during the first 250 Martian days.},
archivePrefix = {arXiv},
arxivId = {10.1.1.91.5767},
author = {Weiss, Stephan and Achtelik, Markus W. and Lynen, Simon and Achtelik, Michael C. and Kneip, Laurent and Chli, Margarita and Siegwart, Roland},
doi = {10.1002/rob.21466},
eprint = {10.1.1.91.5767},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Weiss et al. - 2013 - Monocular Vision for Long-term Micro Aerial Vehicle State Estimation A Compendium.pdf:pdf},
isbn = {9783902661623},
issn = {15564959},
journal = {Journal of Field Robotics},
keywords = {Backstepping,Non-linear control,Path-following},
month = {sep},
number = {5},
pages = {803--831},
pmid = {22164016},
title = {{Monocular Vision for Long-term Micro Aerial Vehicle State Estimation: A Compendium}},
url = {http://doi.wiley.com/10.1002/rob.21466},
volume = {30},
year = {2013}
}
@inproceedings{Marzat2009,
author = {Marzat, J. and Dumortier, Y. and Ducrot, Andre},
booktitle = {WSCG '2009: Full Papers Proceedings: The 17th International Conference in Central Europe on Computer Graphics, Visualization and Computer Vision in co-operation with EUROGRAPHICS},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Marzat, Dumortier, Ducrot - 2009 - Real-time dense and accurate parallel optical flow using CUDA.pdf:pdf},
isbn = {9788086943930},
keywords = {cuda,gpu,image processing,monocular vision,optical flow,parallel processing},
pages = {105--112},
title = {{Real-time dense and accurate parallel optical flow using CUDA}},
url = {http://julien.marzat.free.fr/2008{\_}Stage{\_}Ingenieur{\_}INRIA/WSCG09{\_}Marzat{\_}Dumortier{\_}Ducrot.pdf},
year = {2009}
}
@inproceedings{Usenko2016,
author = {Usenko, Vladyslav and Engel, Jakob and St{\"{u}}ckler, J{\"{o}}rg and Cremers, Daniel},
booktitle = {Robotics and Automation (ICRA), 2016 IEEE International Conference on},
doi = {10.1109/ICRA.2016.7487335},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Usenko et al. - 2016 - Direct Visual-Inertial Odometry with Stereo Cameras.pdf:pdf},
isbn = {9781467380263},
pages = {1885--1892},
publisher = {IEEE},
title = {{Direct Visual-Inertial Odometry with Stereo Cameras}},
year = {2016}
}
@inproceedings{Alcantarilla2012,
abstract = {In this paper, we introduce the concept of dense scene flow for visual SLAM applications. Traditional visual SLAM methods assume static features in the environment and that a dominant part of the scene changes only due to camera egomotion. These assumptions make traditional visual SLAM methods prone to failure in crowded real-world dynamic environments with many independently moving objects, such as the typical environments for the visually impaired. By means of a dense scene flow representation, moving objects can be detected. In this way, the visual SLAM process can be improved considerably, by not adding erroneous measurements into the estimation, yielding more consistent and improved localization and mapping results. We show large-scale visual SLAM results in challenging indoor and outdoor crowded environments with real visually impaired users. In particular, we performed experiments inside the Atocha railway station and in the city-center of Alcalá de Henares, both in Madrid, Spain. Our results show that the combination of visual SLAM and dense scene flow allows to obtain an accurate localization, improving considerably the results of traditional visual SLAM methods and GPS-based approaches.},
author = {Alcantarilla, P F and Yebes, J J and Almaz{\'{a}}n, J and Bergasa, L M},
booktitle = {2012 IEEE International Conference on Robotics and Automation},
doi = {10.1109/ICRA.2012.6224690},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Alcantarilla et al. - 2012 - On combining visual SLAM and dense scene flow to increase the robustness of localization and mapping in dyn.pdf:pdf},
issn = {1050-4729},
keywords = {Alcalá de Henares,Atocha railway station,Cameras,Feature extraction,GPS-based approach,Global Positioning System,Madrid,Optical imaging,Robustness,Simultaneous localization and mapping,Spain,Vectors,Visualization,camera egomotion,cameras,crowded real-world dynamic environments,dense scene flow representation,feature extraction,handicapped aids,image motion analysis,localization-mapping robustness,moving object detection,object detection,static features,visual SLAM applications,visually impaired users},
pages = {1290--1297},
title = {{On combining visual SLAM and dense scene flow to increase the robustness of localization and mapping in dynamic environments}},
year = {2012}
}
@article{Engel2014,
abstract = {We propose a direct (feature-less) monocular SLAM algorithm which, in contrast to current state-of-the-art regarding direct meth- ods, allows to build large-scale, consistent maps of the environment. Along with highly accurate pose estimation based on direct image alignment, the 3D environment is reconstructed in real-time as pose-graph of keyframes with associated semi-dense depth maps. These are obtained by filtering over a large number of pixelwise small-baseline stereo comparisons. The explicitly scale-drift aware formulation allows the approach to operate on challenging sequences including large variations in scene scale. Major enablers are two key novelties: (1) a novel direct tracking method which operates on sim(3), thereby explicitly detecting scale-drift, and (2) an elegant probabilistic solution to include the effect of noisy depth values into tracking. The resulting direct monocular SLAM system runs in real-time on a CPU.},
author = {Engel, Jakob and Schops, Thomas and Cremers, Daniel},
doi = {10.1007/978-3-319-10605-2_54},
file = {:home/tom/Documents/phd/mendeley{\_}pdf//Engel, Sch{\"{o}}ps, Cremers - 2014 - LSD-SLAM Large-Scale Direct monocular SLAM.pdf:pdf},
isbn = {9783319106045},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {monocular},
mendeley-tags = {monocular},
number = {PART 2},
pages = {834--849},
title = {{LSD-SLAM: Large-Scale Direct monocular SLAM}},
volume = {8690 LNCS},
year = {2014}
}
@article{Heng2011,
abstract = {We present a novel stereo-based obstacle avoidance system on a vision-guided micro air vehicle (MAV) that is capable of fully autonomous maneuvers in unknown and dynamic environments. All algorithms run exclusively on the vehicle's on-board computer, and at high frequencies that allow the MAV to react quickly to obstacles appearing in its flight trajectory. Our MAV platform is a quadrotor aircraft equipped with an inertial measurement unit and two stereo rigs. An obstacle mapping algorithm processes stereo images, producing a 3D map representation of the environment; at the same time, a dynamic anytime path planner plans a collision-free path to a goal point.},
author = {Heng, Lionel and Meier, Lorenz and Tanskanen, Petri and Fraundorfer, Friedrich and Pollefeys, Marc},
doi = {10.1109/ICRA.2011.5980095},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Heng et al. - 2011 - Autonomous obstacle avoidance and maneuvering on a vision-guided MAV using on-board processing.pdf:pdf},
isbn = {9781612843865},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
keywords = {cartesian map,deliberate obstacle avoidance,low complexity,obstacle avoidance},
mendeley-tags = {cartesian map,deliberate obstacle avoidance,low complexity,obstacle avoidance},
pages = {2472--2477},
title = {{Autonomous obstacle avoidance and maneuvering on a vision-guided MAV using on-board processing}},
year = {2011}
}
@article{Wang2017,
abstract = {We propose Stereo Direct Sparse Odometry (Stereo DSO) as a novel method for highly accurate real-time visual odometry estimation of large-scale environments from stereo cameras. It jointly optimizes for all the model parameters within the active window, including the intrinsic/extrinsic camera parameters of all keyframes and the depth values of all selected pixels. In particular, we propose a novel approach to integrate constraints from static stereo into the bundle adjustment pipeline of temporal multi-view stereo. Real-time optimization is realized by sampling pixels uniformly from image regions with sufficient intensity gradient. Fixed-baseline stereo resolves scale drift. It also reduces the sensitivities to large optical flow and to rolling shutter effect which are known shortcomings of direct image alignment methods. Quantitative evaluation demonstrates that the proposed Stereo DSO outperforms existing state-of-the-art visual odometry methods both in terms of tracking accuracy and robustness. Moreover, our method delivers a more precise metric 3D reconstruction than previous dense/semi-dense direct approaches while providing a higher reconstruction density than feature-based methods.},
archivePrefix = {arXiv},
arxivId = {1708.07878},
author = {Wang, Rui and Schw{\"{o}}rer, Martin and Cremers, Daniel},
doi = {10.1109/ICCV.2017.421},
eprint = {1708.07878},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Wang, Schw{\"{o}}rer, Cremers - 2017 - Stereo DSO Large-Scale Direct Sparse Visual Odometry with Stereo Cameras.pdf:pdf},
title = {{Stereo DSO: Large-Scale Direct Sparse Visual Odometry with Stereo Cameras}},
url = {http://arxiv.org/abs/1708.07878},
year = {2017}
}
@inproceedings{Forster,
author = {Forster, Christian and Carlone, Luca and Dellaert, Frank and Scaramuzza, Davide},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Forster et al. - 2015 - IMU Preintegration on Manifold for Efficient Visual-Inertial Maximum-a-Posteriori Estimation.pdf:pdf},
publisher = {Georgia Institute of Technology},
title = {{IMU Preintegration on Manifold for Efficient Visual-Inertial Maximum-a-Posteriori Estimation}},
year = {2015}
}
@inproceedings{Nous2016,
author = {Nous, Clint and Meertens, Roland and de Wagter, Christophe and de Croon, Guido},
booktitle = {IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Nous et al. - 2016 - Performance Evaluation in Obstacle Avoidance.pdf:pdf},
isbn = {9781509037629},
pages = {3614--3619},
title = {{Performance Evaluation in Obstacle Avoidance}},
year = {2016}
}
@inproceedings{Letouzey2011,
address = {Dundee, United Kingdom},
author = {Letouzey, Antoine and Petit, Benjamin and Boyer, Edmond},
booktitle = {BMVC 2011 - British Machine Vision Conference},
doi = {10.5244/C.25.46},
editor = {Hoey, Jesse and McKenna, Stephen and Trucco, Emanuele},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Letouzey, Petit, Boyer - 2011 - Scene Flow from Depth and Color Images.pdf:pdf},
month = {aug},
pages = {46:1--11},
publisher = {BMVA Press},
series = {Proceedings of the British Machine Vision Conference},
title = {{Scene Flow from Depth and Color Images}},
url = {https://hal.inria.fr/inria-00616353},
year = {2011}
}
@inbook{Wedel2009,
abstract = {A look at the Middlebury optical flow benchmark [5] reveals that nowadays variational methods yield the most accurate optical flow fields between two image frames. In this work we propose an improvement variant of the original duality based TV-L 1 optical flow algorithm in [31] and provide implementation details. This formulation can preserve discontinuities in the flow field by employing total variation (TV) regularization. Furthermore, it offers robustness against outliers by applying the robust L 1 norm in the data fidelity term.},
address = {Berlin, Heidelberg},
author = {Wedel, Andreas and Pock, Thomas and Zach, Christopher and Bischof, Horst and Cremers, Daniel},
booktitle = {Statistical and Geometrical Approaches to Visual Motion Analysis: International Dagstuhl Seminar, Dagstuhl Castle, Germany, July 13-18, 2008. Revised Papers},
doi = {10.1007/978-3-642-03061-1_2},
editor = {Cremers, Daniel and Rosenhahn, Bodo and Yuille, Alan L and Schmidt, Frank R},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Wedel et al. - 2009 - An Improved Algorithm for TV-L 1 Optical Flow.pdf:pdf},
isbn = {978-3-642-03061-1},
pages = {23--45},
publisher = {Springer Berlin Heidelberg},
title = {{An Improved Algorithm for TV-L 1 Optical Flow}},
url = {https://doi.org/10.1007/978-3-642-03061-1{\_}2},
year = {2009}
}
@phdthesis{Tijmons2012,
annote = {semi global block matching stereo. Verschillende avoidance technieken incl (voorloper van?) droplet.},
author = {Tijmons, S},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Tijmons - 2012 - Stereo Vision for Flapping Wing MAVs.pdf:pdf},
keywords = {TU Delft,low complexity,obstacle avoidance,reactive obstacle avoidance,safe region,scenario: room,stereo vision},
mendeley-tags = {TU Delft,low complexity,obstacle avoidance,reactive obstacle avoidance,safe region,scenario: room,stereo vision},
pages = {96},
title = {{Stereo Vision for Flapping Wing MAVs}},
year = {2012}
}
@inproceedings{Brockers2016,
author = {Brockers, Roland and Fragoso, Anthony and Rothrock, Brandon and Lee, Connor and Matthies, Larry},
booktitle = {International Symposium on Experimental Robotics},
doi = {10.1007/978-3-319-50115-4},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Brockers et al. - 2016 - Vision-Based Obstacle Avoidance for Micro Air Vehicles Using an Egocylindrical Depth Map.pdf:pdf},
isbn = {9783319501154},
pages = {505--514},
publisher = {Springer},
title = {{Vision-Based Obstacle Avoidance for Micro Air Vehicles Using an Egocylindrical Depth Map}},
year = {2016}
}
@article{Marzat2015,
abstract = {To cite this version: Julien Marzat, Julien Moras, Aur{\'{e}}lien Plyer, Alexandre Eudes, Pascal Morin. Vision-based lo-calization, mapping and control for autonomous MAV: EuRoC challenge results. 15th ONERA-DLR Aerospace Symposium (ODAS 2015), May 2015, Toulouse, France. {\textless}hal-01178916{\textgreater}},
author = {Marzat, Julien and Moras, Julien and Eudes, Alexandre and Morin, Pascal and Marzat, Julien and Moras, Julien and Eudes, Alexandre and Morin, Pascal},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Marzat et al. - 2015 - Vision-based localization, mapping and control for autonomous MAV EuRoC challenge results.pdf:pdf},
journal = {15th ONERADLR Aerospace Symposium (ODAS 2015)},
title = {{Vision-based localization, mapping and control for autonomous MAV: EuRoC challenge results}},
url = {https://hal.archives-ouvertes.fr/hal-01178916},
year = {2015}
}
@article{Steinbrucker2014,
abstract = {In this paper we propose a novel volumetric multi-resolution mapping system for RGB-D images that runs on a standard CPU in real-time. Our approach generates a textured triangle mesh from a signed distance function that it continuously updates as new RGB-D images arrive. We propose to use an octree as the primary data structure which allows us to represent the scene at multiple scales. Furthermore, it allows us to grow the reconstruction volume dynamically. As most space is either free or unknown, we allocate and update only those voxels that are located in a narrow band around the observed surface. In contrast to a regular grid, this approach saves enormous amounts of memory and computation time. The major challenge is to generate and maintain a consistent triangle mesh, as neighboring cells in the octree are more difficult to find and may have different resolutions. To remedy this, we present in this paper a novel algorithm that keeps track of these dependencies, and efficiently updates corresponding parts of the triangle mesh. In our experiments, we demonstrate the real-time capability on a large set of RGB-D sequences. As our approach does not require a GPU, it is well suited for applications on mobile or flying robots with limited computational resources. I.},
author = {Steinbr{\"{u}}cker, Frank and Sturm, J{\"{u}}rgen and Cremers, Daniel},
doi = {10.1109/ICRA.2014.6907127},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Steinbr{\"{u}}cker, Sturm, Cremers - 2014 - Volumetric 3D mapping in real-time on a CPU.pdf:pdf},
isbn = {9781479936847},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
pages = {2021--2028},
pmid = {8190083},
title = {{Volumetric 3D mapping in real-time on a CPU}},
year = {2014}
}
@article{Cvisic2017,
author = {Cvi{\v{s}}i{\'{c}}, Igor and {\'{C}}esi{\'{c}}, Josip and Markovi{\'{c}}, Ivan and Petrovi{\'{c}}, Ivan},
doi = {10.1002/rob.21762},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Cvi{\v{s}}i{\'{c}} et al. - 2017 - SOFT-SLAM Computationally efficient stereo visual simultaneous localization and mapping for autonomous unmanned.pdf:pdf},
journal = {Journal of Field Robotics},
number = {August},
pages = {1--18},
title = {{SOFT-SLAM: Computationally efficient stereo visual simultaneous localization and mapping for autonomous unmanned aerial vehicles}},
year = {2017}
}
@inproceedings{Kerl2013,
author = {Kerl, Christian and Sturm, Jurgen and Cremers, Daniel},
booktitle = {2013 IEEE International Conference on Robotics and Automation},
doi = {10.1109/ICRA.2013.6631104},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Kerl, Sturm, Cremers - 2013 - Robust odometry estimation for RGB-D cameras.pdf:pdf},
isbn = {978-1-4673-5643-5},
month = {may},
pages = {3748--3754},
publisher = {IEEE},
title = {{Robust odometry estimation for RGB-D cameras}},
url = {http://ieeexplore.ieee.org/document/6631104/},
year = {2013}
}
@phdthesis{Call2006a,
author = {Call, Brandon R.},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Call - 2006 - Obstacle Avoidance for Unmanned Air Vehicles.pdf:pdf},
number = {December},
school = {Brigham Young University},
title = {{Obstacle Avoidance for Unmanned Air Vehicles}},
type = {MSc},
year = {2006}
}
@article{DeWagter2014,
abstract = {Autonomous flight of Flapping Wing Micro Air Vehicles (FWMAVs) is a major challenge in the field of robotics, due to their light weight and the flapping-induced body motions. In this article, we present the first FWMAV with onboard vision processing for autonomous flight in generic environments. In particular, we introduce the DelFly ‘Explorer', a 20-gram FWMAV equipped with a 0.98-gram autopilot and a 4.0-gram onboard stereo vision system. We explain the design choices that permit carrying the extended payload, while retaining the DelFly's hover capabilities. In addition, we introduce a novel stereo vision algorithm, LongSeq, designed specifically to cope with the flapping motion and the desire to attain a computational effort tuned to the frame rate. The onboard stereo vision system is illustrated in the context of an obstacle avoidance task in an environment with sparse obstacles.},
author = {{De Wagter}, C. and Tijmons, S. and Remes, B. D W and {De Croon}, G. C H E},
doi = {10.1109/ICRA.2014.6907589},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/De Wagter et al. - 2014 - Autonomous flight of a 20-gram Flapping Wing MAV with a 4-gram onboard stereo vision system.pdf:pdf},
isbn = {9781479936847},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
pages = {4982--4987},
title = {{Autonomous flight of a 20-gram Flapping Wing MAV with a 4-gram onboard stereo vision system}},
year = {2014}
}
@inproceedings{DeWagter2005,
author = {de Wagter, C. and Mulder, J.A.},
booktitle = {AIAA Guidance, Navigation, and Control Conference and Exhibit},
doi = {10.2514/6.2005-5872},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/de Wagter, Mulder - 2005 - Towards Vision-Based UAV Situation Awareness.pdf:pdf},
number = {August},
pages = {1--16},
title = {{Towards Vision-Based UAV Situation Awareness}},
year = {2005}
}
@article{Humenberger2010a,
author = {Humenberger, Martin and Zinner, Christian and Weber, Michael and Kubinger, Wilfried and Vincze, Markus},
doi = {10.1016/j.cviu.2010.03.012},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Humenberger et al. - 2010 - A fast stereo matching algorithm suitable for embedded real-time systems.pdf:pdf},
issn = {1077-3142},
journal = {Computer Vision and Image Understanding},
number = {11},
pages = {1180--1202},
publisher = {Elsevier Inc.},
title = {{A fast stereo matching algorithm suitable for embedded real-time systems}},
url = {http://dx.doi.org/10.1016/j.cviu.2010.03.012},
volume = {114},
year = {2010}
}
@phdthesis{TomasCardosoRezioMartins2017,
author = {{Tom{\'{a}}s Cardoso R{\'{e}}zio Martins}, Diogo},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Tom{\'{a}}s Cardoso R{\'{e}}zio Martins - 2017 - Fusion of stereo and monocular depth estimates in a self-supervised learning context.pdf:pdf},
school = {Delft University of Technology},
title = {{Fusion of stereo and monocular depth estimates in a self-supervised learning context}},
type = {MSc thesis},
year = {2017}
}
@inbook{Mordohai2012,
abstract = {This paper surveys the state of the art in evaluating the performance of scene flow estimation and points out the difficulties in generating benchmarks with ground truth which have not allowed the development of general, reliable solutions. Hopefully, the renewed interest in dynamic 3D content, which has led to increased research in this area, will also lead to more rigorous evaluation and more effective algorithms. We begin by classifying methods that estimate depth, motion or both from multi-view sequences according to their parameterization of shape and motion. Then, we present several criteria for their evaluation, discuss their strengths and weaknesses and conclude with recommendations.},
address = {Berlin, Heidelberg},
author = {Mordohai, Philippos},
booktitle = {Computer Vision -- ECCV 2012. Workshops and Demonstrations: Florence, Italy, October 7-13, 2012, Proceedings, Part II},
doi = {10.1007/978-3-642-33868-7_15},
editor = {Fusiello, Andrea and Murino, Vittorio and Cucchiara, Rita},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Mordohai - 2012 - On the Evaluation of Scene Flow Estimation.pdf:pdf},
isbn = {978-3-642-33868-7},
pages = {148--157},
publisher = {Springer Berlin Heidelberg},
title = {{On the Evaluation of Scene Flow Estimation}},
url = {https://doi.org/10.1007/978-3-642-33868-7{\_}15},
year = {2012}
}
@article{Scherer2008,
author = {Scherer, Sebastian and Singh, Sanjiv and Chamberlain, Lyle and Elgersma, Mike},
doi = {10.1177/0278364908090949},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Scherer et al. - 2008 - Flying Fast and Low Among Obstacles Methodology and Experiments.pdf:pdf},
isbn = {0278364908090},
journal = {The International Journal of Robotics Research},
keywords = {aerial robotics,learning},
number = {5},
pages = {549--574},
title = {{Flying Fast and Low Among Obstacles: Methodology and Experiments}},
volume = {27},
year = {2008}
}
@inproceedings{Merrell2004,
author = {Merrell, Paul Clark and Lee, Dah-jye and Beard, Randal W},
booktitle = {Proc. SPIE 5609, Mobile Robots XVII},
doi = {10.1117/12.571554},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Merrell, Lee, Beard - 2004 - Obstacle Avoidance for Unmanned Air Vehicles Using Optical Flow Probability Distributions.pdf:pdf},
keywords = {motion parallax,obstacle avoidance,optical flow,structure from motion,unmanned air vehicles},
title = {{Obstacle Avoidance for Unmanned Air Vehicles Using Optical Flow Probability Distributions}},
year = {2004}
}
@article{Wedel2011,
abstract = {Building upon recent developments in optical flow and stereo matching estimation, we propose a variational framework for the estimation of stereoscopic scene flow, i.e., the motion of points in the three-dimensional world from stereo image sequences. The proposed algorithm takes into account image pairs from two consecutive times and computes both depth and a 3D motion vector associated with each point in the image. In contrast to previous works, we partially decouple the depth estimation from the motion estimation, which has many practical advantages. The variational formulation is quite flexible and can handle both sparse or dense disparity maps. The proposed method is very efficient; with the depth map being computed on an FPGA, and the scene flow computed on the GPU, the proposed algorithm runs at frame rates of 20 frames per second on QVGA images (320{\{}$\backslash$texttimes{\}}240 pixels). Furthermore, we present solutions to two important problems in scene flow estimation: violations of intensity consistency between input images, and the uncertainty measures for the scene flow result.},
author = {Wedel, Andreas and Brox, Thomas and Vaudrey, Tobi and Rabe, Clemens and Franke, Uwe and Cremers, Daniel},
doi = {10.1007/s11263-010-0404-0},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Wedel et al. - 2011 - Stereoscopic Scene Flow Computation for 3D Motion Understanding.pdf:pdf},
issn = {1573-1405},
journal = {International Journal of Computer Vision},
number = {1},
pages = {29--51},
title = {{Stereoscopic Scene Flow Computation for 3D Motion Understanding}},
url = {https://doi.org/10.1007/s11263-010-0404-0},
volume = {95},
year = {2011}
}
@article{Engel2017,
author = {Engel, Jakob and Koltun, Vladlen and Cremers, Daniel},
doi = {10.1109/TPAMI.2017.2658577},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Engel, Koltun, Cremers - 2017 - Direct Sparse Odometry.pdf:pdf},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
title = {{Direct Sparse Odometry}},
year = {2017}
}
@incollection{Dey2016a,
archivePrefix = {arXiv},
arxivId = {1411.6326v1},
author = {Dey, Debadeepta and Shankar, Kumar Shaurya and Zeng, Sam and Mehta, Rupesh and Agcayazi, M Talha and Eriksen, Christopher and Daftry, Shreyansh and Hebert, Martial and Bagnell, J Andrew},
booktitle = {Springer Tracts in Advanced Robotics},
editor = {Wettergreen, D. and Barfoot, T.},
eprint = {1411.6326v1},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Dey et al. - 2016 - Vision and Learning for Deliberative Monocular Cluttered Flight.pdf:pdf},
number = {Field and Service Robotics},
pages = {391--409},
publisher = {Springer},
title = {{Vision and Learning for Deliberative Monocular Cluttered Flight}},
volume = {113},
year = {2016}
}
@article{VanHecke2016,
abstract = {Self-Supervised Learning (SSL) is a reliable learning mechanism in which a robot uses an original, trusted sensor cue for training to recognize an additional, complementary sensor cue. We study for the first time in SSL how a robot's learning behavior should be organized, so that the robot can keep performing its task in the case that the original cue becomes unavailable. We study this persistent form of SSL in the context of a flying robot that has to avoid obstacles based on distance estimates from the visual cue of stereo vision. Over time it will learn to also estimate distances based on monocular appearance cues. A strategy is introduced that has the robot switch from stereo vision based flight to monocular flight, with stereo vision purely used as 'training wheels' to avoid imminent collisions. This strategy is shown to be an effective approach to the 'feedback-induced data bias' problem as also experienced in learning from demonstration. Both simulations and real-world experiments with a stereo vision equipped AR drone 2.0 show the feasibility of this approach, with the robot successfully using monocular vision to avoid obstacles in a 5 x 5 room. The experiments show the potential of persistent SSL as a robust learning approach to enhance the capabilities of robots. Moreover, the abundant training data coming from the own sensors allows to gather large data sets necessary for deep learning approaches.},
archivePrefix = {arXiv},
arxivId = {1603.08047},
author = {van Hecke, Kevin and de Croon, Guido and van der Maaten, Laurens and Hennes, Daniel and Izzo, Dario},
eprint = {1603.08047},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/van Hecke et al. - 2016 - Persistent self-supervised learning principle from stereo to monocular vision for obstacle avoidance.pdf:pdf},
keywords = {TU Delft,monoc-,obstacle detection,persistent self-supervised learning,robotics,stereo vision,ular depth estimation},
mendeley-tags = {TU Delft,obstacle detection,stereo vision},
pages = {1--17},
title = {{Persistent self-supervised learning principle: from stereo to monocular vision for obstacle avoidance}},
url = {http://arxiv.org/abs/1603.08047},
year = {2016}
}
@article{VanHecke2017,
author = {van Hecke, Kevin and de Croon, Guido C.H.E. and Hennes, Daniel and Setterfield, Timothy P. and Saenz-Otero, Alvar and Izzo, Dario},
doi = {10.1016/j.actaastro.2017.07.038},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/van Hecke et al. - 2017 - Self-supervised learning as an enabling technology for future space exploration robots ISS experiments on mono.pdf:pdf},
issn = {00945765},
journal = {Acta Astronautica},
keywords = {persistent self-supervised learning},
month = {nov},
number = {February},
pages = {1--9},
title = {{Self-supervised learning as an enabling technology for future space exploration robots: ISS experiments on monocular distance learning}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0094576517302862},
volume = {140},
year = {2017}
}
@article{Mahjourian2018,
abstract = {We present a novel approach for unsupervised learning of depth and ego-motion from monocular video. Unsupervised learning removes the need for separate supervisory signals (depth or ego-motion ground truth, or multi-view video). Prior work in unsupervised depth learning uses pixel-wise or gradient-based losses, which only consider pixels in small local neighborhoods. Our main contribution is to explicitly consider the inferred 3D geometry of the scene, enforcing consistency of the estimated 3D point clouds and ego-motion across consecutive frames. This is a challenging task and is solved by a novel (approximate) backpropagation algorithm for aligning 3D structures. We combine this novel 3D-based loss with 2D losses based on photometric quality of frame reconstructions using estimated depth and ego-motion from adjacent frames. We also incorporate validity masks to avoid penalizing areas in which no useful information exists. We test our algorithm on the KITTI dataset and on a video dataset captured on an uncalibrated mobile phone camera. Our proposed approach consistently improves depth estimates on both datasets, and outperforms the state-of-the-art for both depth and ego-motion. Because we only require a simple video, learning depth and ego-motion on large and varied datasets becomes possible. We demonstrate this by training on the low quality uncalibrated video dataset and evaluating on KITTI, ranking among top performing prior methods which are trained on KITTI itself.},
archivePrefix = {arXiv},
arxivId = {1802.05522},
author = {Mahjourian, Reza and Wicke, Martin and Angelova, Anelia},
eprint = {1802.05522},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Mahjourian, Wicke, Angelova - 2018 - Unsupervised Learning of Depth and Ego-Motion from Monocular Video Using 3D Geometric Constraints.pdf:pdf},
title = {{Unsupervised Learning of Depth and Ego-Motion from Monocular Video Using 3D Geometric Constraints}},
url = {http://arxiv.org/abs/1802.05522},
year = {2018}
}
@phdthesis{Lamers2016,
author = {Lamers, K},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Lamers - 2016 - Self-Supervised Monocular Distance Learning on a Lightweight Micro Air Vehicle.pdf:pdf},
school = {Delft University of Technology},
title = {{Self-Supervised Monocular Distance Learning on a Lightweight Micro Air Vehicle}},
type = {MSc},
year = {2016}
}
@article{DeBoer,
abstract = {—In recent years optical-flow-aided position measurement solutions have been used in both commercial and academic applications. These systems are used for navigating unmanned aerial vehicles (UAVs) in GPS-deprived environments. Movement in sequential images is detected and converted to real world position change. Multiple approaches have been suggested, ranging from using an optical mouse sensor to the use of a stereo camera setup. Our research focuses on single camera solutions. Previous research has used a variety of optical flow algorithms for single camera solutions. This paper presents a comparison on three algorithms to check if using different algorithms yield different results in terms of quality and CPU time. This paper also provides insight into the general theory behind using single camera optical flow for UAV navigation. The compared algorithms are the Lucas-Kanade method, Gunnar Farne ack's algorithm and Block Matching. A testing framework and custom indoor and outdoor datasets were created to measure algorithms flow estimation quality and computation time. Characteristic differences were found between the performance of the algorithms in terms of both computation time and quality. Choosing between algorithms therefore can increase flow estimation quality or reduce CPU time usage. Also different winners per test set were found in terms of estimation quality.},
author = {{De Boer}, Jasper and Kalksma, Mathieu},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/De Boer, Kalksma - Unknown - Choosing between optical flow algorithms for UAV position change measurement.pdf:pdf},
keywords = {Block Matching,Gunnar Farne ack method,Index Terms—UAV navigation,Lucas-Kanade method,optical flow},
title = {{Choosing between optical flow algorithms for UAV position change measurement}}
}
@article{Shen2013a,
abstract = {This paper addresses the development of a light-weight autonomous quadrotor that uses cameras and an inexpensive IMU as its only sensors and onboard processors for estimation and control. We describe a fully-functional, integrated system with a focus on robust visual-inertial state estimation, and demonstrate the quadrotor's ability to autonomously travel at speeds up to 4 m/s and roll and pitch angles exceeding 20 degrees. The performance of the proposed system is demonstrated via challenging experiments in three dimensional indoor environments.},
author = {Shen, Shaojie and Mulgaonkar, Y and Michael, Nathan and Kumar, V},
doi = {10.15607/RSS.2013.IX.03},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Shen et al. - 2013 - Vision-based state estimation and trajectory control towards high-speed flight with a quadrotor.pdf:pdf},
journal = {Robotics: Science and {\ldots}},
title = {{Vision-based state estimation and trajectory control towards high-speed flight with a quadrotor}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.309.7992{\&}rep=rep1{\&}type=pdf},
year = {2013}
}
@article{Scheper2016,
archivePrefix = {arXiv},
arxivId = {1411.7267v2},
author = {Scheper, Kirk Y.W. and Tijmons, Sjoerd and de Visser, Coen C. and de Croon, Guido C.H.E.},
doi = {10.1162/ARTL_a_00192},
eprint = {1411.7267v2},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Scheper et al. - 2016 - Behaviour Trees for Evolutionary Robotics.pdf:pdf},
journal = {Artificial Life},
keywords = {Behaviour Tree,Evolutionary Robotics,MAVs,Reality Gap},
number = {1},
pages = {23--48},
title = {{Behaviour Trees for Evolutionary Robotics}},
volume = {22},
year = {2016}
}
@article{Vedula2005,
abstract = {Just as optical flow is the two-dimensional motion of points in an image, scene flow is the three-dimensional motion of points in the world. The fundamental difficulty with optical flow is that only the normal flow can be computed directly from the image measurements, without some form of smoothing or regularization. In this paper, we begin by showing that the same fundamental limitation applies to scene flow; however, many cameras are used to image the scene. There are then two choices when computing scene flow: 1) perform the regularization in the images or 2) perform the regularization on the surface of the object in the scene. In this paper, we choose to compute scene flow using regularization in the images. We describe three algorithms, the first two for computing scene flow from optical flows and the third for constraining scene structure from the inconsistencies in multiple optical flows.},
author = {Vedula, S and Rander, P and Collins, R and Kanade, T},
doi = {10.1109/TPAMI.2005.63},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Vedula et al. - 2005 - Three-dimensional scene flow.pdf:pdf},
issn = {0162-8828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Algorithms,Artificial Intelligence,Automated,Cameras,Cluster Analysis,Computer Graphics,Computer vision,Computer-Assisted,Deformable models,Fluid flow measurement,Image Enhancement,Image Interpretation,Image motion analysis,Imaging,Index Terms- Scene flow,Information Storage and Retrieval,Layout,Motion estimation,Movement,Numerical Analysis,Optical computing,Pattern Recognition,Photogrammetry,Reflectivity,Reproducibility of Results,Sensitivity and Specificity,Signal Processing,Smoothing methods,Three-Dimensional,Video Recording,cameras,computational geometry,image measurements,image motion analysis,image regularization,image sequences,multiple optical flows,normal flow,optical flow,scene structure constraint,the brightness constancy constraint,three dimensional motion,three dimensional scene flow computing,three-dimensional dense nonrigid motion,three-dimensional normal flow.},
number = {3},
pages = {475--480},
title = {{Three-dimensional scene flow}},
volume = {27},
year = {2005}
}
@inproceedings{Mayer2016,
author = {Mayer, Nikolaus and Ilg, Eddy and Hausser, Philip and Fischer, Philipp and Cremers, Daniel and Dosovitskiy, Alexey and Brox, Thomas},
booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Mayer et al. - 2016 - A Large Dataset to Train Convolutional Networks for Disparity, Optical Flow, and Scene Flow Estimation.pdf:pdf},
title = {{A Large Dataset to Train Convolutional Networks for Disparity, Optical Flow, and Scene Flow Estimation}},
year = {2016}
}
@inproceedings{Mcgee2005,
author = {Mcgee, Timothy G and Sengupta, Raja and Hedrick, Karl},
booktitle = {International Conference on Robotics and Automation},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Mcgee, Sengupta, Hedrick - 2005 - Obstacle Detection for Small Autonomous Aircraft Using Sky Segmentation.pdf:pdf},
isbn = {078038914X},
number = {April},
pages = {4679--4684},
title = {{Obstacle Detection for Small Autonomous Aircraft Using Sky Segmentation}},
year = {2005}
}
@article{Tijmons2016,
abstract = {The development of autonomous lightweight MAVs, capable of navigating in unknown indoor environments, is one of the major challenges in robotics. The complexity of this challenge comes from constraints on weight and power consumption of onboard sensing and processing devices. In this paper we propose the "Droplet" strategy, an avoidance strategy that outperforms reactive avoidance strategies by allowing constant speed maneuvers while being computationally extremely efficient. The strategy deals with nonholonomic motion constraints of most fixed and flapping wing platforms, and with the limited field-of-view of stereo camera systems. It guarantees obstacle-free flight in the absence of sensor and motor noise. We first analyze the strategy in simulation, and then show its robustness in real-world conditions by implementing it on a 21-gram flapping wing MAV.},
archivePrefix = {arXiv},
arxivId = {1604.00833},
author = {Tijmons, Sjoerd and de Croon, Guido and Remes, Bart and {De Wagter}, Christophe and Mulder, Max},
eprint = {1604.00833},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Tijmons et al. - 2016 - Obstacle Avoidance Strategy using Onboard Stereo Vision on a Flapping Wing MAV.pdf:pdf},
keywords = {TU Delft,experiment,low complexity,nieuwe referenties,obstacle avoidance,reactive obstacle avoidance,safe region,scenario: room,simulation,stereo vision},
mendeley-tags = {TU Delft,experiment,low complexity,nieuwe referenties,obstacle avoidance,reactive obstacle avoidance,safe region,scenario: room,simulation,stereo vision},
pages = {1--13},
title = {{Obstacle Avoidance Strategy using Onboard Stereo Vision on a Flapping Wing MAV}},
url = {http://arxiv.org/abs/1604.00833},
year = {2016}
}
@article{Saxena2006,
abstract = {We consider the task of depth estimation from a single monocular image. We take a supervised learning approach to this problem, in which we begin by collecting a training set of monocular images (of unstructured outdoor environments which include forests, trees, buildings, etc.) and their corresponding ground-truth depthmaps. Then, we apply supervised learning to predict the depthmap as a function of the image. Depth estimation is a challenging problem, since local features alone are insufficient to estimate depth at a point, and one needs to consider the global context of the image. Our model uses a discriminatively-trained Markov Random Field (MRF) that incorporates multiscale local- and global-image features, and models both depths at individual points as well as the relation between depths at different points. We show that, even on unstructured scenes, our algorithm is frequently able to recover fairly accurate depthmaps.},
author = {Saxena, Ashutosh and Chung, Sung H and Ng, Andrew Y},
doi = {10.1007/s11263-007-0071-y},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Saxena, Chung, Ng - 2006 - Learning Depth from Single Monocular Images.pdf:pdf},
isbn = {9780262232531},
issn = {0920-5691},
journal = {Advances in Neural Information Processing Systems},
keywords = {monocular,obstacle detection},
mendeley-tags = {monocular,obstacle detection},
pages = {1161--1168},
title = {{Learning Depth from Single Monocular Images}},
volume = {18},
year = {2006}
}
@inproceedings{Marzat2009,
author = {Marzat, J. and Dumortier, Y. and Ducrot, Andre},
booktitle = {WSCG '2009: Full Papers Proceedings: The 17th International Conference in Central Europe on Computer Graphics, Visualization and Computer Vision in co-operation with EUROGRAPHICS},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Marzat, Dumortier, Ducrot - 2009 - Real-time dense and accurate parallel optical flow using CUDA.pdf:pdf},
isbn = {9788086943930},
keywords = {cuda,gpu,image processing,monocular vision,optical flow,parallel processing},
pages = {105--112},
title = {{Real-time dense and accurate parallel optical flow using CUDA}},
url = {http://julien.marzat.free.fr/2008{\_}Stage{\_}Ingenieur{\_}INRIA/WSCG09{\_}Marzat{\_}Dumortier{\_}Ducrot.pdf},
year = {2009}
}
@inproceedings{Call2006,
author = {Call, Brandon and Beard, Randy and Taylor, Clark and Barber, Blake},
booktitle = {AIAA Guidance, Navigation, and Control Conference and Exhibit},
doi = {10.2514/6.2006-6541},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Call et al. - 2006 - Obstacle Avoidance For Unmanned Air Vehicles Using Image Feature Tracking.pdf:pdf},
number = {August},
pages = {3406--3414},
title = {{Obstacle Avoidance For Unmanned Air Vehicles Using Image Feature Tracking}},
year = {2006}
}
@inproceedings{Brockers2016,
author = {Brockers, Roland and Fragoso, Anthony and Rothrock, Brandon and Lee, Connor and Matthies, Larry},
booktitle = {International Symposium on Experimental Robotics},
doi = {10.1007/978-3-319-50115-4},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Brockers et al. - 2016 - Vision-Based Obstacle Avoidance for Micro Air Vehicles Using an Egocylindrical Depth Map.pdf:pdf},
isbn = {9783319501154},
pages = {505--514},
publisher = {Springer},
title = {{Vision-Based Obstacle Avoidance for Micro Air Vehicles Using an Egocylindrical Depth Map}},
year = {2016}
}
@article{Godarda,
archivePrefix = {arXiv},
arxivId = {1609.03677v3},
author = {Godard, Cl{\'{e}}ment and Aodha, Oisin Mac and Brostow, Gabriel J.},
eprint = {1609.03677v3},
file = {:home/tom/Documents/phd/mendeley{\_}pdf//Godard, Mac Aodha, Brostow - 2017 - Unsupervised Monocular Depth Estimation with Left-Right Consistency.pdf:pdf},
title = {{Unsupervised Monocular Depth Estimation with Left-Right Consistency}}
}
@article{Marzat2017,
author = {Marzat, Julien and Bertrand, Sylvain and Eudes, Alexandre},
doi = {10.1016/j.ifacol.2017.08.1910},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Marzat, Bertrand, Eudes - 2017 - Reactive MPC for Autonomous MAV Navigation in Indoor Cluttered Environments Flight Experiments.pdf:pdf},
issn = {24058963},
keywords = {flight experiments,localization and mapping,micro-air vehicles,model predictive control,vision-based},
title = {{Reactive MPC for Autonomous MAV Navigation in Indoor Cluttered Environments : Flight Experiments}},
year = {2017}
}
@incollection{Dey2016a,
archivePrefix = {arXiv},
arxivId = {1411.6326v1},
author = {Dey, Debadeepta and Shankar, Kumar Shaurya and Zeng, Sam and Mehta, Rupesh and Agcayazi, M Talha and Eriksen, Christopher and Daftry, Shreyansh and Hebert, Martial and Bagnell, J Andrew},
booktitle = {Springer Tracts in Advanced Robotics},
editor = {Wettergreen, D. and Barfoot, T.},
eprint = {1411.6326v1},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Dey et al. - 2016 - Vision and Learning for Deliberative Monocular Cluttered Flight.pdf:pdf},
number = {Field and Service Robotics},
pages = {391--409},
publisher = {Springer},
title = {{Vision and Learning for Deliberative Monocular Cluttered Flight}},
volume = {113},
year = {2016}
}
@phdthesis{Nous2016a,
author = {Nous, C.W.M.},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Nous - 2016 - Performance in Obstacle Avoidance.pdf:pdf},
school = {Delft University of Technology},
title = {{Performance in Obstacle Avoidance}},
type = {MSc},
year = {2016}
}
@inproceedings{Matthies2014,
abstract = {We address obstacle avoidance for outdoor flight of micro air vehicles. The highly textured nature of outdoor scenes enables camera-based perception, which will scale to very small size, weight, and power with very wide, two-axis field of regard. In this paper, we use forward-looking stereo cameras for obstacle detection and a downward-looking camera as an input to state estimation. For obstacle representation, we use image space with the stereo disparity map itself. We show that a C-space-like obstacle expansion can be done with this representation and that collision checking can be done by projecting candidate 3-D trajectories into image space and performing a z-buffer-like operation with the disparity map. This approach is very efficient in memory and computing time. We do motion planning and trajectory generation with an adaptation of a closed-loop RRT planner to quadrotor dynamics and full 3D search. We validate the performance of the system with Monte Carlo simulations in virtual worlds and flight tests of a real quadrotor through a grove of trees. The approach is designed to support scalability to high speed flight and has numerous possible generalizations to use other polar or hybrid polar/Cartesian representations and to fuse data from additional sensors, such as peripheral optical flow or radar.},
annote = {From Duplicate 1 (Stereo vision-based obstacle avoidance for micro air vehicles using disparity space - Matthies, Larry; Brockers, Roland; Kuwata, Yoshiaki; Weiss, Stephan)

Obstacle avoidance met RRT, collision checking in image space met inverted range (disparity).

Dual-core CPU.},
author = {Matthies, Larry and Brockers, Roland and Kuwata, Yoshiaki and Weiss, Stephan},
booktitle = {2014 IEEE International Conference on Robotics and Automation (ICRA)},
doi = {10.1109/ICRA.2014.6907325},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Matthies et al. - 2014 - Stereo vision-based obstacle avoidance for micro air vehicles using disparity space(2).pdf:pdf},
isbn = {978-1-4799-3685-4},
issn = {1050-4729},
keywords = {3-D trajectories,3D search,C-space-like obstacle expansion,Cameras,Monte Carlo simulations,Optical imaging,Optical sensors,Planning,Three-dimensional displays,Trajectory,Vehicles,autonomous aerial vehicles,camera-based perception,closed-loop RRT planner,collision avoidance,collision checking,deliberate obstacle avoidance,disparity space,downward-looking camera,egocylinder,experiment,flight tests,forward-looking stereo cameras,helicopters,high speed flight,highlight,image space,low complexity,micro air vehicles,motion planning,nieuwe referenties,obstacle avoidance,obstacle detection,obstacle representation,outdoor flight,quadrotor dynamics,robot vision,scenario: forest,scenario: window,simulation,state estimation,stereo disparity map,stereo image processing,stereo vision,stereo vision-based obstacle avoidance,trajectory generation,tree grove,virtual worlds,vision,z-buffer-like operation},
mendeley-tags = {deliberate obstacle avoidance,experiment,highlight,image space,low complexity,nieuwe referenties,obstacle avoidance,scenario: forest,scenario: window,simulation,stereo vision},
month = {may},
pages = {3242--3249},
publisher = {IEEE},
shorttitle = {2014 IEEE International Conference on Robotics and},
title = {{Stereo vision-based obstacle avoidance for micro air vehicles using disparity space}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6907325},
volume = {9836},
year = {2014}
}
@article{Fortun2015,
annote = {Image Understanding for Real-world Distributed Video Networks},
author = {Fortun, Denis and Bouthemy, Patrick and Kervrann, Charles},
doi = {https://doi.org/10.1016/j.cviu.2015.02.008},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Fortun, Bouthemy, Kervrann - 2015 - Optical flow modeling and computation A survey.pdf:pdf},
issn = {1077-3142},
journal = {Computer Vision and Image Understanding},
keywords = {Feature matching,Motion estimation,Occlusions,Optical flow,Optimization,Parametric models,Regularization},
number = {Supplement C},
pages = {1--21},
title = {{Optical flow modeling and computation: A survey}},
url = {http://www.sciencedirect.com/science/article/pii/S1077314215000429},
volume = {134},
year = {2015}
}
@article{Schmid2013a,
abstract = {We introduce our new quadrotor platform for re- alizing autonomous navigation in unknown indoor/outdoor en- vironments. Autonomous waypoint navigation, obstacle avoid- ance and flight control is implemented on-board. The system does not require a special environment, artificial markers or an external reference system. We developed a monolithic, mechanically damped perception unit which is equipped with a stereo camera pair, an Inertial Measurement Unit (IMU), two processor-and an FPGA board. Stereo images are processed on the FPGA by the Semi-Global Matching algorithm. Keyframe- based stereo odometry is fused with IMU data compensating for time delays that are induced by the vision pipeline. The system state estimate is used for control and on-board 3D mapping. An operator can set waypoints in the map, while the quadrotor autonomously plans its path avoiding obstacles. We show experiments with the quadrotor flying from inside a building to the outside and vice versa, traversing a window and a door respectively. A video of the experiments is part of this work. To the best of our knowledge, this is the first autonomously flying system with complete on-board processing that performs waypoint navigation with obstacle avoidance in geometrically unconstrained, complex indoor/outdoor environments.},
author = {Schmid, Korbinian and Tomic, Teodor and Ruess, Felix and Hirschmuller, Heiko and Suppa, Michael},
doi = {10.1109/IROS.2013.6696922},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Schmid et al. - 2013 - Stereo vision based indooroutdoor navigation for flying robots.pdf:pdf},
isbn = {9781467363587},
issn = {21530858},
journal = {IEEE International Conference on Intelligent Robots and Systems},
pages = {3955--3962},
title = {{Stereo vision based indoor/outdoor navigation for flying robots}},
year = {2013}
}
@article{Mur-Artal2015,
abstract = {The gold standard method for tridimensional reconstruction and camera localization from a set of images is well known to be Bundle Adjustment (BA). Although BA was regarded for years as a costly method restricted to the offline domain, several real time algorithms based on BA flourished in the last decade. However those algorithms were limited to perform SLAM in small scenes or only Visual Odometry. In this work we built on excellent algorithms of the last years to design from scratch a Monocular SLAM system that operates in real time, in small and large, indoor and outdoor environments, with the capability of wide baseline loop closing and relocalization, and including full automatic initialization. Our survival of the fittest approach to select the points and keyframes of the reconstruction generates a compact and trackable map that only grows if the scene content changes, enhancing lifelong operation. We present an exhaustive evaluation in 27 sequences from the most popular datasets achieving unprecedented performance with a typical localization accuracy from 0.2{\%} to 1{\%} of the trajectory dimension in scenes from a desk to several city blocks. We make public a ROS implementation.},
archivePrefix = {arXiv},
arxivId = {1502.00956},
author = {Mur-Artal, Raul and Montiel, JMM M M and Tardos, Juan D},
doi = {10.1109/TRO.2015.2463671},
eprint = {1502.00956},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Mur-Artal, Montiel, Tardos - 2015 - ORB-SLAM A Versatile and Accurate Monocular SLAM System.pdf:pdf},
isbn = {1552-3098},
issn = {1552-3098},
journal = {IEEE Transactions on Robotics},
month = {oct},
number = {5},
pages = {1147--1163},
title = {{ORB-SLAM: A Versatile and Accurate Monocular SLAM System}},
url = {http://ieeexplore.ieee.org/document/7219438/},
volume = {31},
year = {2015}
}
@inproceedings{Gomez-Ojeda2016,
author = {Gomez-Ojeda, Ruben and Briales, Jesus and Gonzalez-Jimenez, Javier},
booktitle = {Intelligent Robots and Systems (IROS), 2016 IEEE/RSJ International Conference on},
doi = {10.1109/IROS.2016.7759620},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Gomez-Ojeda, Briales, Gonzalez-Jimenez - 2016 - PL-SVO Semi-Direct Monocular Visual Odometry by Combining Points and Line Segments.pdf:pdf},
isbn = {9781509037629},
pages = {4211--4216},
publisher = {IEEE},
title = {{PL-SVO: Semi-Direct Monocular Visual Odometry by Combining Points and Line Segments}},
year = {2016}
}
@article{Kendoul2012,
author = {Kendoul, Farid},
doi = {10.1002/rob},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Kendoul - 2012 - Survey of Advances in Guidance, Navigation, and Control of Unmanned Rotorcraft Systems.pdf:pdf},
journal = {Journal of Field Robotics},
number = {2},
pages = {315--378},
title = {{Survey of Advances in Guidance, Navigation, and Control of Unmanned Rotorcraft Systems}},
volume = {29},
year = {2012}
}
@article{Jaegle2016,
abstract = {We propose robust methods for estimating camera egomotion in noisy, real-world monocular image sequences in the general case of unknown observer rotation and translation with two views and a small baseline. This is a difficult problem because of the nonconvex cost function of the perspective camera motion equation and because of non-Gaussian noise arising from noisy optical flow estimates and scene non-rigidity. To address this problem, we introduce the expected residual likelihood method (ERL), which estimates confidence weights for noisy optical flow data using likelihood distributions of the residuals of the flow field under a range of counterfactual model parameters. We show that ERL is effective at identifying outliers and recovering appropriate confidence weights in many settings. We compare ERL to a novel formulation of the perspective camera motion equation using a lifted kernel, a recently proposed optimization framework for joint parameter and confidence weight estimation with good empirical properties. We incorporate these strategies into a motion estimation pipeline that avoids falling into local minima. We find that ERL outperforms the lifted kernel method and baseline monocular egomotion estimation strategies on the challenging KITTI dataset, while adding almost no runtime cost over baseline egomotion methods.},
archivePrefix = {arXiv},
arxivId = {1602.04886},
author = {Jaegle, Andrew and Phillips, Stephen and Daniilidis, Kostas},
doi = {10.1109/ICRA.2016.7487206},
eprint = {1602.04886},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Jaegle, Phillips, Daniilidis - 2016 - Fast, robust, continuous monocular egomotion computation.pdf:pdf},
isbn = {9781467380263},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
pages = {773--780},
title = {{Fast, robust, continuous monocular egomotion computation}},
volume = {2016-June},
year = {2016}
}
@inproceedings{Petrides2017,
author = {Petrides, P and Kyrkou, C and Kolios, P and Theocharides, T and Panayiotou, C},
booktitle = {International Conference on Unmanned Aircraft Systems (ICUAS)},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Petrides et al. - 2017 - Towards a Holistic Performance Evaluation Framework for Drone-Based Object Detection.pdf:pdf},
isbn = {9781509044948},
pages = {1785--1793},
title = {{Towards a Holistic Performance Evaluation Framework for Drone-Based Object Detection}},
year = {2017}
}
@article{Vijayanarasimhan2017,
archivePrefix = {arXiv},
arxivId = {1704.07804v1},
author = {Vijayanarasimhan, Sudheendra and Ricco, Susanna and Schmid, Cordelia and Sukthankar, Rahul and Fragkiadaki, Katerina},
eprint = {1704.07804v1},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Vijayanarasimhan et al. - 2017 - SfM-Net Learning of Structure and Motion from Video.pdf:pdf},
journal = {CoRR},
title = {{SfM-Net: Learning of Structure and Motion from Video}},
volume = {abs/1704.0},
year = {2017}
}
@article{Geiger2011a,
abstract = {Accurate 3d perception from video sequences is a core subject in computer vision and robotics, since it forms the basis of subsequent scene analysis. In practice however, online requirements often severely limit the utilizable camera resolution and hence also reconstruction accuracy. Furthermore, real-time systems often rely on heavy parallelism which can prevent applications in mobile devices or driver assistance systems, especially in cases where FPGAs cannot be employed. This paper proposes a novel approach to build 3d maps from high-resolution stereo sequences in real-time. Inspired by recent progress in stereo matching, we propose a sparse feature matcher in conjunction with an efficient and robust visual odometry algorithm. Our reconstruction pipeline combines both techniques with efficient stereo matching and a multi-view linking scheme for generating consistent 3d point clouds. In our experiments we show that the proposed odometry method achieves state-of-the-art accuracy. Including feature matching, the visual odometry part of our algorithm runs at 25 frames per second, while - at the same time - we obtain new depth maps at 3-4 fps, sufficient for online 3d reconstructions.},
author = {Geiger, Andreas and Ziegler, Julius and Stiller, Christoph},
doi = {10.1109/IVS.2011.5940405},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Geiger, Ziegler, Stiller - 2011 - StereoScan Dense 3d reconstruction in real-time.pdf:pdf},
isbn = {9781457708909},
issn = {1931-0587},
journal = {IEEE Intelligent Vehicles Symposium, Proceedings},
number = {Iv},
pages = {963--968},
pmid = {24344074},
title = {{StereoScan: Dense 3d reconstruction in real-time}},
year = {2011}
}
@article{Hinzmann,
archivePrefix = {arXiv},
arxivId = {arXiv:1712.06837v1},
author = {Hinzmann, Timo and Taubner, Tim and Siegwart, Roland},
eprint = {arXiv:1712.06837v1},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Hinzmann, Taubner, Siegwart - Unknown - Flexible Stereo Constrained, Non-rigid, Wide-baseline Stereo Vision for Fixed-wing Aerial Platfo.pdf:pdf},
title = {{Flexible Stereo: Constrained, Non-rigid, Wide-baseline Stereo Vision for Fixed-wing Aerial Platforms}}
}
@book{Goerzen2010,
abstract = {A fundamental aspect of autonomous vehicle guidance is planning trajectories. Historically, two fields have contributed to trajectory or motion planning methods: robotics and dynamics and control. The former typically have a stronger focus on computational issues and real-time robot control, while the latter emphasize the dynamic behavior and more specific aspects of trajectory performance. Guidance for Unmanned Aerial Vehicles (UAVs), including fixed- and rotary-wing aircraft, involves significant differences from most traditionally defined mobile and manipulator robots. Qualities characteristic to UAVs include non-trivial dynamics, three-dimensional environments, disturbed operating conditions, and high levels of uncertainty in state knowledge. Otherwise, UAV guidance shares qualities with typical robotic motion planning problems, including partial knowledge of the environment and tasks that can range from basic goal interception, which can be precisely specified, to more general tasks like surveillance and reconnaissance, which are harder to specify. These basic planning problems involve continual interaction with the environment. The purpose of this paper is to provide an overview of existing motion planning algorithms while adding perspectives and practical examples from UAV guidance approaches.},
author = {Goerzen, C. and Kong, Z. and Mettler, B.},
booktitle = {Journal of Intelligent and Robotic Systems: Theory and Applications},
doi = {10.1007/s10846-009-9383-1},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Goerzen, Kong, Mettler - 2010 - A survey of motion planning algorithms from the perspective of autonomous UAV guidance.pdf:pdf},
isbn = {0921-0296},
issn = {09210296},
keywords = {Algorithm,Autonomous,Complexity,Guidance,Heuristics,Motion planning,Optimization,Trajectory,UAV},
number = {1-4},
pages = {65--100},
title = {{A survey of motion planning algorithms from the perspective of autonomous UAV guidance}},
volume = {57},
year = {2010}
}
@article{Tippetts2016,
abstract = {A significant amount of research in the field of stereo vision has been published in the past decade. Considerable progress has been made in improving accuracy of results as well as achieving real-time performance in obtaining those results. This work provides a comprehensive review of stereo vision algorithms with specific emphasis on real-time performance to identify those suitable for resource-limited systems. An attempt has been made to compile and present accuracy and runtime performance data for all stereo vision algorithms developed in the past decade. Algorithms are grouped into three categories: (1) those that have published results of real-time or near real-time performance on standard processors, (2) those that have real-time performance on specialized hardware (i.e. GPU, FPGA, DSP, ASIC), and (3) those that have not been shown to obtain near real-time performance. This review is intended to aid those seeking algorithms suitable for real-time implementation on resource-limited systems, and to encourage further research and development of the same by providing a snapshot of the status quo.},
author = {Tippetts, Beau and Lee, Dah Jye and Lillywhite, Kirt and Archibald, James},
doi = {10.1007/s11554-012-0313-2},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Tippetts et al. - 2016 - Review of stereo vision algorithms and their suitability for resource-limited systems.pdf:pdf},
isbn = {1861-8200},
issn = {18618200},
journal = {Journal of Real-Time Image Processing},
keywords = {low complexity,stereo vision,survey},
mendeley-tags = {low complexity,stereo vision,survey},
number = {1},
pages = {5--25},
publisher = {Springer-Verlag},
title = {{Review of stereo vision algorithms and their suitability for resource-limited systems}},
url = {http://dx.doi.org/10.1007/s11554-012-0313-2},
volume = {11},
year = {2016}
}
@inproceedings{Engel2015,
author = {Engel, Jakob and St{\"{u}}ckler, J{\"{o}}rg and Cremers, Daniel},
booktitle = {Intelligent Robots and Systems (IROS), 2015 IEEE/RSJ International Conference on},
doi = {10.1109/IROS.2015.7353631},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Engel, St{\"{u}}ckler, Cremers - 2015 - Large-Scale Direct SLAM with Stereo Cameras.pdf:pdf},
isbn = {9781479999941},
pages = {1935--1942},
publisher = {IEEE},
title = {{Large-Scale Direct SLAM with Stereo Cameras}},
year = {2015}
}
@inproceedings{Hornung2012,
author = {Hornung, Armin and Phillips, Mike and Jones, E Gil and Bennewitz, Maren and Likhachev, Maxim and Chitta, Sachin},
booktitle = {Robotics and Automation (ICRA), 2012 IEEE International Conference on},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Hornung et al. - 2012 - Navigation in Three-Dimensional Cluttered Environments for Mobile Manipulation.pdf:pdf},
pages = {423--429},
publisher = {IEEE},
title = {{Navigation in Three-Dimensional Cluttered Environments for Mobile Manipulation}},
year = {2012}
}
@book{Goerzen2010,
abstract = {A fundamental aspect of autonomous vehicle guidance is planning trajectories. Historically, two fields have contributed to trajectory or motion planning methods: robotics and dynamics and control. The former typically have a stronger focus on computational issues and real-time robot control, while the latter emphasize the dynamic behavior and more specific aspects of trajectory performance. Guidance for Unmanned Aerial Vehicles (UAVs), including fixed- and rotary-wing aircraft, involves significant differences from most traditionally defined mobile and manipulator robots. Qualities characteristic to UAVs include non-trivial dynamics, three-dimensional environments, disturbed operating conditions, and high levels of uncertainty in state knowledge. Otherwise, UAV guidance shares qualities with typical robotic motion planning problems, including partial knowledge of the environment and tasks that can range from basic goal interception, which can be precisely specified, to more general tasks like surveillance and reconnaissance, which are harder to specify. These basic planning problems involve continual interaction with the environment. The purpose of this paper is to provide an overview of existing motion planning algorithms while adding perspectives and practical examples from UAV guidance approaches.},
author = {Goerzen, C. and Kong, Z. and Mettler, B.},
booktitle = {Journal of Intelligent and Robotic Systems: Theory and Applications},
doi = {10.1007/s10846-009-9383-1},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Goerzen, Kong, Mettler - 2010 - A survey of motion planning algorithms from the perspective of autonomous UAV guidance.pdf:pdf},
isbn = {0921-0296},
issn = {09210296},
keywords = {Algorithm,Autonomous,Complexity,Guidance,Heuristics,Motion planning,Optimization,Trajectory,UAV},
number = {1-4},
pages = {65--100},
title = {{A survey of motion planning algorithms from the perspective of autonomous UAV guidance}},
volume = {57},
year = {2010}
}
@article{Barron1994,
author = {Barron, J L and Fleet, D J and Beauchemin, S S},
doi = {10.1007/BF01420984},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Barron, Fleet, Beauchemin - 1994 - Performance of optical flow techniques.pdf:pdf},
issn = {0920-5691},
journal = {International Journal of Computer Vision},
month = {feb},
number = {1},
pages = {43--77},
title = {{Performance of optical flow techniques}},
url = {http://link.springer.com/10.1007/BF01420984},
volume = {12},
year = {1994}
}
@article{Schmid2013a,
abstract = {We introduce our new quadrotor platform for re- alizing autonomous navigation in unknown indoor/outdoor en- vironments. Autonomous waypoint navigation, obstacle avoid- ance and flight control is implemented on-board. The system does not require a special environment, artificial markers or an external reference system. We developed a monolithic, mechanically damped perception unit which is equipped with a stereo camera pair, an Inertial Measurement Unit (IMU), two processor-and an FPGA board. Stereo images are processed on the FPGA by the Semi-Global Matching algorithm. Keyframe- based stereo odometry is fused with IMU data compensating for time delays that are induced by the vision pipeline. The system state estimate is used for control and on-board 3D mapping. An operator can set waypoints in the map, while the quadrotor autonomously plans its path avoiding obstacles. We show experiments with the quadrotor flying from inside a building to the outside and vice versa, traversing a window and a door respectively. A video of the experiments is part of this work. To the best of our knowledge, this is the first autonomously flying system with complete on-board processing that performs waypoint navigation with obstacle avoidance in geometrically unconstrained, complex indoor/outdoor environments.},
author = {Schmid, Korbinian and Tomic, Teodor and Ruess, Felix and Hirschmuller, Heiko and Suppa, Michael},
doi = {10.1109/IROS.2013.6696922},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Schmid et al. - 2013 - Stereo vision based indooroutdoor navigation for flying robots.pdf:pdf},
isbn = {9781467363587},
issn = {21530858},
journal = {IEEE International Conference on Intelligent Robots and Systems},
pages = {3955--3962},
title = {{Stereo vision based indoor/outdoor navigation for flying robots}},
year = {2013}
}
@inproceedings{Holzmann2016,
author = {Holzmann, Thomas and Fraundorfer, Friedrich and Bischof, Horst},
booktitle = {Proceedings of the 11th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications, 2016},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Holzmann, Fraundorfer, Bischof - 2016 - Direct Stereo Visual Odometry Based on Lines.pdf:pdf},
pages = {1--11},
title = {{Direct Stereo Visual Odometry Based on Lines}},
year = {2016}
}
@inproceedings{Junell,
author = {Junell, J and van Kampen, E},
booktitle = {IMAV},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Junell, van Kampen - Unknown - Adaptive Path Planning for a Vision-Based quadrotor in an Obstacle Field.pdf:pdf},
title = {{Adaptive Path Planning for a Vision-Based quadrotor in an Obstacle Field}}
}
@article{Griffiths2006,
abstract = {Despite the tremendous potential demonstrated by miniature aerial vehicles (MAV) in numerous applications, they are currently limited to operations in open air space, far away from obstacles and terrain. To broaden the range of applications for MAVs, methods to enable operation in environments of increased complexity must be developed. In this article, we presented two strategies for obstacle and terrain avoidance that provide a means for avoiding obstacles in the flight path and for staying centered in a winding corridor. Flight tests have validated the feasibility of these approaches and demonstrated promise for further refinement},
author = {Griffiths, Stephen and Saunders, Jeff and Curtis, Andrew and Barber, Blake and McLain, Tim and Bread, Randy},
doi = {10.1109/MRA.2006.1678137},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Griffiths et al. - 2006 - Maximizing miniature aerial vehicles.pdf:pdf},
isbn = {1070-9932},
issn = {10709932},
journal = {IEEE Robotics and Automation Magazine},
keywords = {Autonomous flight,Miniature aerial vehicle,Obstacle avoidance,Terrain navigation},
number = {3},
pages = {34--43},
title = {{Maximizing miniature aerial vehicles}},
volume = {13},
year = {2006}
}
@article{Forster2014,
abstract = {We propose a semi-direct monocular visual odometry algorithm that is precise, robust, and faster than current state-of-the-art methods. The semi-direct approach eliminates the need of costly feature extraction and robust matching techniques for motion estimation. Our algorithm operates directly on pixel intensities, which results in subpixel precision at high frame-rates. A probabilistic mapping method that explicitly models outlier measurements is used to estimate 3D points, which results in fewer outliers and more reliable points. Precise and high frame-rate motion estimation brings increased robustness in scenes of little, repetitive, and high-frequency texture. The algorithm is applied to micro-aerial-vehicle state- estimation in GPS-denied environments and runs at 55 frames per second on the onboard embedded computer and at more than 300 frames per second on a consumer laptop. We call our approach SVO (Semi-direct Visual Odometry) and release our implementation as open-source software.},
author = {Forster, Christian and Pizzoli, Matia and Scaramuzza, Davide},
doi = {10.1109/ICRA.2014.6906584},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Forster, Pizzoli, Scaramuzza - 2014 - SVO Fast semi-direct monocular visual odometry.pdf:pdf},
isbn = {9781479936847},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
keywords = {Forster2014},
mendeley-tags = {Forster2014},
pages = {15--22},
pmid = {6576973927449638915},
title = {{SVO: Fast semi-direct monocular visual odometry}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6906584},
year = {2014}
}
@article{Fortun2015a,
author = {Fortun, Denis and Bouthemy, Patrick and Kervrann, Charles},
doi = {10.1016/j.cviu.2015.02.008},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Fortun, Bouthemy, Kervrann - 2015 - Optical flow modeling and computation A survey.pdf:pdf},
issn = {1077-3142},
journal = {Computer Vision and Image Understanding},
pages = {1--21},
publisher = {Elsevier Inc.},
title = {{Optical flow modeling and computation: A survey}},
url = {http://dx.doi.org/10.1016/j.cviu.2015.02.008},
volume = {134},
year = {2015}
}
@inproceedings{Matthies2014,
abstract = {We address obstacle avoidance for outdoor flight of micro air vehicles. The highly textured nature of outdoor scenes enables camera-based perception, which will scale to very small size, weight, and power with very wide, two-axis field of regard. In this paper, we use forward-looking stereo cameras for obstacle detection and a downward-looking camera as an input to state estimation. For obstacle representation, we use image space with the stereo disparity map itself. We show that a C-space-like obstacle expansion can be done with this representation and that collision checking can be done by projecting candidate 3-D trajectories into image space and performing a z-buffer-like operation with the disparity map. This approach is very efficient in memory and computing time. We do motion planning and trajectory generation with an adaptation of a closed-loop RRT planner to quadrotor dynamics and full 3D search. We validate the performance of the system with Monte Carlo simulations in virtual worlds and flight tests of a real quadrotor through a grove of trees. The approach is designed to support scalability to high speed flight and has numerous possible generalizations to use other polar or hybrid polar/Cartesian representations and to fuse data from additional sensors, such as peripheral optical flow or radar.},
annote = {From Duplicate 1 (Stereo vision-based obstacle avoidance for micro air vehicles using disparity space - Matthies, Larry; Brockers, Roland; Kuwata, Yoshiaki; Weiss, Stephan)

Obstacle avoidance met RRT, collision checking in image space met inverted range (disparity).

Dual-core CPU.},
author = {Matthies, Larry and Brockers, Roland and Kuwata, Yoshiaki and Weiss, Stephan},
booktitle = {2014 IEEE International Conference on Robotics and Automation (ICRA)},
doi = {10.1109/ICRA.2014.6907325},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Matthies et al. - 2014 - Stereo vision-based obstacle avoidance for micro air vehicles using disparity space(2).pdf:pdf},
isbn = {978-1-4799-3685-4},
issn = {1050-4729},
keywords = {3-D trajectories,3D search,C-space-like obstacle expansion,Cameras,Monte Carlo simulations,Optical imaging,Optical sensors,Planning,Three-dimensional displays,Trajectory,Vehicles,autonomous aerial vehicles,camera-based perception,closed-loop RRT planner,collision avoidance,collision checking,deliberate obstacle avoidance,disparity space,downward-looking camera,egocylinder,experiment,flight tests,forward-looking stereo cameras,helicopters,high speed flight,highlight,image space,low complexity,micro air vehicles,motion planning,nieuwe referenties,obstacle avoidance,obstacle detection,obstacle representation,outdoor flight,quadrotor dynamics,robot vision,scenario: forest,scenario: window,simulation,state estimation,stereo disparity map,stereo image processing,stereo vision,stereo vision-based obstacle avoidance,trajectory generation,tree grove,virtual worlds,vision,z-buffer-like operation},
mendeley-tags = {deliberate obstacle avoidance,experiment,highlight,image space,low complexity,nieuwe referenties,obstacle avoidance,scenario: forest,scenario: window,simulation,stereo vision},
month = {may},
pages = {3242--3249},
publisher = {IEEE},
shorttitle = {2014 IEEE International Conference on Robotics and},
title = {{Stereo vision-based obstacle avoidance for micro air vehicles using disparity space}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6907325},
volume = {9836},
year = {2014}
}
@inproceedings{Werlberger2009,
author = {Werlberger, Manuel and Trobin, Werner and Pock, Thomas and Wedel, Andreas and Cremers, Daniel and Bischof, Horst},
booktitle = {BMVC},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Werlberger et al. - 2009 - Anisotropic Huber-L 1 Optical Flow.pdf:pdf},
pages = {1--11},
title = {{Anisotropic Huber-L 1 Optical Flow}},
year = {2009}
}
@article{Lee2011,
author = {Lee, Jeong-Oog and Lee, Keun-Hwan and Park, Sang-Heon and Im, Sung-Gyu and Park, Jungkeun},
doi = {10.1108/00022661111173270},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Lee et al. - 2011 - Obstacle avoidance for small UAVs using monocular vision.pdf:pdf},
isbn = {0002266111117},
journal = {Aircraft Engineering and Aerospace Technology: An International Journal},
keywords = {collisions,image processing,monocular vision,mops,obstacle avoidance,paper type research paper,sift algorithm,small uavs},
number = {6},
pages = {397--406},
title = {{Obstacle avoidance for small UAVs using monocular vision}},
volume = {83},
year = {2011}
}
@article{TomasCardosoRezioMartins2017a,
author = {{Tom{\'{a}}s Cardoso R{\'{e}}zio Martins}, Diogo and van Hecke, Kevin and de Croon, Guido},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Tom{\'{a}}s Cardoso R{\'{e}}zio Martins, van Hecke, de Croon - 2017 - Fusion of stereo and monocular depth estimates in a self-supervised learning.pdf:pdf},
title = {{Fusion of stereo and monocular depth estimates in a self-supervised learning context}},
year = {2017}
}
@phdthesis{VanHecke2015,
author = {van Hecke, Kevin},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/van Hecke - 2015 - Persistent self-supervised learning principle study and demonstration on flying robots.pdf:pdf},
school = {Delft University of Technology},
title = {{Persistent self-supervised learning principle: study and demonstration on flying robots}},
year = {2015}
}
@article{Minguez2016,
author = {Minguez, Javier and Lamiraux, Florant and Laumond, Jean-Paul},
doi = {10.1007/978-3-319-32552-1_47},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Minguez, Lamiraux, Laumond - 2016 - Motion Planning and Obstacle Avoidance.pdf:pdf},
journal = {Springer Handbook of Robotics},
pages = {1177--1202},
title = {{Motion Planning and Obstacle Avoidance}},
url = {http://link.springer.com/10.1007/978-3-319-32552-1{\_}47},
year = {2016}
}
@article{Oleynikova2015,
abstract = {High speed, low latency obstacle avoidance is essential for enabling Micro Aerial Vehicles (MAVs) to function in cluttered and dynamic environments. While other systems exist that do high-level mapping and 3D path planning for obstacle avoidance, most of these systems require high-powered CPUs on-board or off-board control from a ground station. We present a novel entirely on-board approach, leveraging a light-weight low power stereo vision system on FPGA. Our approach runs at a frame rate of 60 frames a second on VGA- sized images and minimizes latency between image acquisition and performing reactive maneuvers, allowing MAVs to fly more safely and robustly in complex environments. We also suggest our system as a light-weight safety layer for systems undertak- ing more complex tasks, like mapping the environment. Finally, we show our algorithm implemented on a light- weight, very computationally constrained platform, and demon- strate obstacle avoidance in a variety of environments.},
annote = {Focus op obstacle segmentation mbv U-map. Vrij eenvoudige oplossing voor obstacle avoidance. Gebruikt een tijdelijke map met obstakels.},
author = {Oleynikova, Helen and Honegger, Dominik and Pollefeys, Marc},
doi = {10.1109/ICRA.2015.7138979},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Oleynikova, Honegger, Pollefeys - 2015 - Reactive avoidance using embedded stereo vision for MAV flight.pdf:pdf},
isbn = {978-1-4799-6923-4},
journal = {IEEE International Conference on Robotics and Automation},
keywords = {3D path planning,CPU,Collision avoidance,FPGA,Field programmable gate arrays,MAV flight,Mobile communication,Navigation,Optical sensors,Robots,Stereo vision,VGA-sized images,cartesian map,cluttered environments,collision avoidance,complex environments,deliberate obstacle avoidance,dynamic environments,embedded stereo vision,field programmable gate arrays,ground station,high-level mapping,highlight,image acquisition,light-weight low power stereo vision system,light-weight safety layer,low complexity,micro aerial vehicles,nieuwe referenties,obstacle avoidance,obstacle detection,off-board control,on-board control,reactive avoidance,reactive maneuvers,robot vision,scenario: forest,space vehicles,stereo image processing,stereo vision,uv disparity,visual perception},
mendeley-tags = {cartesian map,deliberate obstacle avoidance,highlight,low complexity,nieuwe referenties,obstacle avoidance,obstacle detection,scenario: forest,stereo vision,uv disparity},
pages = {50--56},
title = {{Reactive avoidance using embedded stereo vision for MAV flight}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7138979},
year = {2015}
}
@article{Kanellakis2017,
author = {Kanellakis, Christoforos and Nikolakopoulos, George},
doi = {10.1007/s10846-017-0483-z},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Kanellakis, Nikolakopoulos - 2017 - Survey on Computer Vision for UAVs Current Developments and Trends.pdf:pdf},
journal = {Journal of Intelligent and Robotic Systems},
keywords = {Obstacle avoidance,SLAM,Target tracking,UAVs,Visual servoing,obstacle avoidance,slam,target tracking,uavs,visual servoing},
pages = {141--168},
publisher = {Journal of Intelligent {\&} Robotic Systems},
title = {{Survey on Computer Vision for UAVs: Current Developments and Trends}},
volume = {87},
year = {2017}
}
@article{Mur-Artal2017,
author = {Mur-Artal, R. and Tard{\'{o}}s, J.D.},
doi = {10.1109/TRO.2017.2705103},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Mur-Artal, Tard{\'{o}}s - 2017 - ORB-SLAM2 An Open-Source SLAM System for Monocular, Stereo, and RGB-D Cameras.pdf:pdf},
journal = {IEEE Transactions on Robotics},
number = {5},
pages = {1255--1262},
title = {{ORB-SLAM2: An Open-Source SLAM System for Monocular, Stereo, and RGB-D Cameras}},
volume = {33},
year = {2017}
}
@article{Younes2016,
archivePrefix = {arXiv},
arxivId = {1607.00470v1},
author = {Younes, Georges and Asmar, Daniel and Shammas, Elie},
eprint = {1607.00470v1},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Younes, Asmar, Shammas - 2016 - A survey on non-filter-based monocular Visual SLAM systems.pdf:pdf},
journal = {arXiv preprint arXiv:1607.00470},
keywords = {monocular,non-filter based,visual slam},
title = {{A survey on non-filter-based monocular Visual SLAM systems}},
year = {2016}
}
@article{Thrun2006,
abstract = {It is anticipatedthat theMars ScienceLaboratory rover, namedCuriosity,will traverse 10–20 kmon the surface of Mars during its primary mission. In preparation for this traverse, Earth-based tests were performed using Mars weight vehicles. These vehicles were driven over Mars analog bedrock, cohesive soil, and cohesionless sand at various slopes. Vehicle slip was characterized on each of these terrains versus slope for direct upslope driving. Results show that slopes up to 22 degrees are traversable on smooth bedrock and that slopes up to 28 degrees are traversable on some cohesive soils. In cohesionless sand, results show a sharp transition between moderate slip on 10 degree slopes and vehicle embedding at 17 degrees. For cohesionless sand, data are also presented showing the relationship between vehicle slip and wheel sinkage. Side by side testing of the Mars Exploration Rover test vehicle and the Mars Science Laboratory test vehicle show how increased wheel diameter leads to better slope climbing ability in sand for vehicles with nearly identical ground pressure. Lastly, preliminary data from Curiosity's initial driving on Mars are presented and compared to the Earth-based testing, showing good agreement for the driving done during the first 250 Martian days.},
archivePrefix = {arXiv},
arxivId = {10.1.1.91.5767},
author = {Thrun, Sebastian and Montemerlo, Mike and Dahlkamp, Hendrik and Stavens, David and Aron, Andrei and Diebel, James and Fong, Philip and Gale, John and Halpenny, Morgan and Hoffmann, Gabriel and Lau, Kenny and Oakley, Celia and Palatucci, Mark and Pratt, Vaughan and Stang, Pascal and Strohband, Sven and Dupont, Cedric and Jendrossek, Lars-Erik and Koelen, Christian and Markey, Charles and Rummel, Carlo and van Niekerk, Joe and Jensen, Eric and Alessandrini, Philippe and Bradski, Gary and Davies, Bob and Ettinger, Scott and Kaehler, Adrian and Nefian, Ara and Mahoney, Pamela},
doi = {10.1002/rob.20147},
eprint = {10.1.1.91.5767},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Thrun et al. - 2006 - Stanley The robot that won the DARPA Grand Challenge.pdf:pdf},
isbn = {9783902661623},
issn = {15564959},
journal = {Journal of Field Robotics},
keywords = {Backstepping,Non-linear control,Path-following},
month = {sep},
number = {9},
pages = {661--692},
pmid = {22164016},
title = {{Stanley: The robot that won the DARPA Grand Challenge}},
url = {http://doi.wiley.com/10.1002/rob.20147},
volume = {23},
year = {2006}
}
@inproceedings{Saha2014,
author = {Saha, Suman and Natraj, Ashutosh and Waharte, Sonia},
booktitle = {IEEE International Conference on Aerospace Electronics and Remote Sensing Technology (ICARES) A},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Saha, Natraj, Waharte - 2014 - A Real-Time Monocular Vision-based Frontal Obstacle Detection and Avoidance for Low Cost UAVs in GPS Deni.pdf:pdf},
isbn = {9781479961887},
pages = {189--195},
title = {{A Real-Time Monocular Vision-based Frontal Obstacle Detection and Avoidance for Low Cost UAVs in GPS Denied Environment}},
year = {2014}
}
@article{Milford2012a,
abstract = {In this paper we use a sequence-based visual localization algorithm to reveal surprising answers to the question, how much visual information is actually needed to conduct effective navigation? The algorithm actively searches for the best local image matches within a sliding window of short route segments or 'sub-routes', and matches sub-routes by searching for coherent sequences of local image matches. In contract to many existing techniques, the technique requires no pre-training or camera parameter calibration. We compare the algorithm's performance to the state-of-the-art FAB-MAP 2.0 algorithm on a 70 km benchmark dataset. Performance matches or exceeds the state of the art feature-based localization technique using images as small as 4 pixels, fields of view reduced by a factor of 250, and pixel bit depths reduced to 2 bits. We present further results demonstrating the system localizing in an office environment with near 100{\%} precision using two 7 bit Lego light sensors, as well as using 16 and 32 pixel images from a motorbike race and a mountain rally car stage. By demonstrating how little image information is required to achieve localization along a route, we hope to stimulate future 'low fidelity' approaches to visual navigation that complement probabilistic feature-based techniques.},
author = {Milford, Michael},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Milford - 2012 - Visual Route Recognition with a Handful of Bits.pdf:pdf},
isbn = {9780262519687},
issn = {2330765X},
journal = {Proceedings of Robotics Science and Systems Conference,University of Sydney},
keywords = {-localization,highlight,low complexity,route recognition,visual navigation},
mendeley-tags = {highlight,low complexity},
title = {{Visual Route Recognition with a Handful of Bits}},
url = {http://eprints.qut.edu.au/51415},
volume = {2012},
year = {2012}
}
@article{Mair2014,
abstract = {Map-based navigation is a crucial task for any mobile robot. Usually, in an unknown environment, this problem is addressed by applying Simultaneous Localization and Mapping based on metric grid-maps. However, such maps are in general rather computational expensive and do not scale well. Insects are able to cover large distances and reliably find back to their nests, although they are quite limited in their resources. Inspired by theories on insect navigation, we developed a data structure which is highly scalable and efficiently adapts to the available memory during run-time. Positions in space are memorized as snapshots, which are unique configurations of landmarks. Unlike conventional snapshot or visual map approaches, we do not simply store the landmarks as a set, but we arrange them in a tree-like structure according to the relevance of their information. The resulting navigation solely relies on the direction measurements of arbitrary landmarks. In this work, we present the concept of the Landmark-Tree (LT) map and apply it to a mobile platform equipped with an omnidirectional camera. We verify the reliability and robustness of the LT-map concept in simulations as well as by experiments with the robotic platform.$\backslash$nMap-based navigation is a crucial task for any mobile robot. Usually, in an unknown environment, this problem is addressed by applying Simultaneous Localization and Mapping based on metric grid-maps. However, such maps are in general rather computational expensive and do not scale well. Insects are able to cover large distances and reliably find back to their nests, although they are quite limited in their resources. Inspired by theories on insect navigation, we developed a data structure which is highly scalable and efficiently adapts to the available memory during run-time. Positions in space are memorized as snapshots, which are unique configurations of landmarks. Unlike conventional snapshot or visual map approaches, we do not simply store the landmarks as a set, but we arrange them in a tree-like structure according to the relevance of their information. The resulting navigation solely relies on the direction measurements of arbitrary landmarks. In this work, we present the concept of the Landmark-Tree (LT) map and apply it to a mobile platform equipped with an omnidirectional camera. We verify the reliability and robustness of the LT-map concept in simulations as well as by experiments with the robotic platform.},
author = {Mair, Elmar and Augustine, Marcus and J{\"{a}}ger, Bastian and Stelzer, Annett and Brand, Christoph and Burschka, Darius and Suppa, Michael},
doi = {10.1080/01691864.2013.871770},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Mair et al. - 2014 - A biologically inspired navigation concept based on the Landmark-Tree map for efficient long-distance robot navigat.pdf:pdf},
issn = {0169-1864},
journal = {Advanced Robotics},
keywords = {bio-inspired,highlight,landmark-tree,low complexity,lt-map,roadmap,slam,topological  slam,topological navigation},
mendeley-tags = {highlight,low complexity,slam,topological  slam},
number = {5},
pages = {289--302},
title = {{A biologically inspired navigation concept based on the Landmark-Tree map for efficient long-distance robot navigation}},
url = {http://dx.doi.org/10.1080/01691864.2013.871770},
volume = {28},
year = {2014}
}
@article{Okutami1994,
author = {Okutami, M and Kanade, T},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Okutami, Kanade - 1994 - A Stereo Matching Algorithm with an Adaptive Window Theory and Experiment.pdf:pdf},
journal = {Pami},
keywords = {stereo vision},
mendeley-tags = {stereo vision},
number = {9},
pages = {920--932},
title = {{A Stereo Matching Algorithm with an Adaptive Window: Theory and Experiment}},
volume = {16},
year = {1994}
}
@article{Kiryati1991,
author = {Kiryati, N and Eldar, Y and Bruckstein, A.M.},
doi = {10.1016/0031-3203(91)90073-E},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Kiryati, Eldar, Bruckstein - 1991 - A probabilistic Hough transform.pdf:pdf},
issn = {00313203},
journal = {Pattern Recognition},
month = {jan},
number = {4},
pages = {303--316},
title = {{A probabilistic Hough transform}},
url = {http://linkinghub.elsevier.com/retrieve/pii/003132039190073E},
volume = {24},
year = {1991}
}
@article{Scheper2016,
archivePrefix = {arXiv},
arxivId = {1411.7267v2},
author = {Scheper, Kirk Y.W. and Tijmons, Sjoerd and de Visser, Coen C. and de Croon, Guido C.H.E.},
doi = {10.1162/ARTL_a_00192},
eprint = {1411.7267v2},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Scheper et al. - 2016 - Behaviour Trees for Evolutionary Robotics.pdf:pdf},
journal = {Artificial Life},
keywords = {Behaviour Tree,Evolutionary Robotics,MAVs,Reality Gap},
number = {1},
pages = {23--48},
title = {{Behaviour Trees for Evolutionary Robotics}},
volume = {22},
year = {2016}
}
@article{Crall2015,
abstract = {Locomotion through structurally complex environments is fundamental to the life history of most flying animals, and the costs associated with movement through clutter have important consequences for the ecology and evolution of volant taxa. However, few studies have directly investigated how flying animals navigate through cluttered environments, or examined which aspects of flight performance are most critical for this challenging task. Here, we examined how body size, acceleration and obstacle orientation affect the flight of bumblebees in an artificial, cluttered environment. Non-steady flight performance is often predicted to decrease with body size, as a result of a presumed reduction in acceleration capacity, but few empirical tests of this hypothesis have been performed in flying animals. We found that increased body size is associated with impaired flight performance (specifically transit time) in cluttered environments, but not with decreased peak accelerations. In addition, previous studies have shown that flying insects can produce higher accelerations along the lateral body axis, suggesting that if maneuvering is constrained by acceleration capacity, insects should perform better when maneuvering around objects laterally rather than vertically. Our data show that bumblebees do generate higher accelerations in the lateral direction, but we found no difference in their ability to pass through obstacle courses requiring lateral versus vertical maneuvering. In sum, our results suggest that acceleration capacity is not a primary determinant of flight performance in clutter, as is often assumed. Rather than being driven by the scaling of acceleration, we show that the reduced flight performance of larger bees in cluttered environments is driven by the allometry of both path sinuosity and mean flight speed. Specifically, differences in collision-avoidance behavior underlie much of the variation in flight performance across body size, with larger bees negotiating obstacles more cautiously. Thus, our results show that cluttered environments challenge the flight capacity of insects, but in surprising ways that emphasize the importance of behavioral and ecological context for understanding flight performance in complex environments.},
author = {Crall, J. D. and Ravi, S. and Mountcastle, A. M. and Combes, S. A.},
doi = {10.1242/jeb.121293},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Crall et al. - 2015 - Bumblebee flight performance in cluttered environments effects of obstacle orientation, body size and acceleration.pdf:pdf},
issn = {0022-0949},
journal = {Journal of Experimental Biology},
keywords = {biology,bombus impatiens,clutter,collision avoidance,environmental complexity,insect flight,locomotion},
mendeley-tags = {biology},
number = {17},
pages = {2728--2737},
pmid = {26333927},
title = {{Bumblebee flight performance in cluttered environments: effects of obstacle orientation, body size and acceleration}},
url = {http://jeb.biologists.org/cgi/doi/10.1242/jeb.121293},
volume = {218},
year = {2015}
}
@inproceedings{Sanderson1983,
author = {Sanderson, Arthur C. and Weiss, Lee E.},
booktitle = {Proc. SPIE},
doi = {10.1117/12.934098},
editor = {Casasent, David P.},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Sanderson, Weiss - 1983 - Image-based visual servo control of robots.pdf:pdf},
month = {may},
pages = {164--169},
title = {{Image-based visual servo control of robots}},
url = {http://proceedings.spiedigitallibrary.org/proceeding.aspx?articleid=1234058},
volume = {360},
year = {1983}
}
@article{Sturm2012a,
abstract = {In this paper, we present a novel benchmark for the evaluation of RGB-D SLAM systems. We recorded a large set of image sequences from a Microsoft Kinect with highly accurate and time-synchronized ground truth camera poses from a motion capture system. The sequences contain both the color and depth images in full sensor resolution (640 × 480) at video frame rate (30 Hz). The ground-truth trajectory was obtained from a motion-capture system with eight high-speed tracking cameras (100 Hz). The dataset consists of 39 sequences that were recorded in an office environment and an industrial hall. The dataset covers a large variety of scenes and camera motions. We provide sequences for debugging with slow motions as well as longer trajectories with and without loop closures. Most sequences were recorded from a handheld Kinect with unconstrained 6-DOF motions but we also provide sequences from a Kinect mounted on a Pioneer 3 robot that was manually navigated through a cluttered indoor environment. To stimulate the comparison of different approaches, we provide automatic evaluation tools both for the evaluation of drift of visual odometry systems and the global pose error of SLAM systems. The benchmark website [1] contains all data, detailed descriptions of the scenes, specifications of the data formats, sample code, and evaluation tools.},
author = {Sturm, Jrgen and Engelhard, Nikolas and Endres, Felix and Burgard, Wolfram and Cremers, Daniel},
doi = {10.1109/IROS.2012.6385773},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Sturm et al. - 2012 - A benchmark for the evaluation of RGB-D SLAM systems.pdf:pdf},
isbn = {9781467317375},
issn = {21530858},
journal = {IEEE International Conference on Intelligent Robots and Systems},
pages = {573--580},
pmid = {6385773},
title = {{A benchmark for the evaluation of RGB-D SLAM systems}},
year = {2012}
}
@article{Tapus2008,
annote = {o.a. History of SLAM.},
author = {Tapus, Adriana and Siegwart, Roland},
doi = {10.1007/978-3-540-79007-5_5},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Tapus, Siegwart - 2008 - Topological SLAM.pdf:pdf},
isbn = {9783540790068},
issn = {16107438},
journal = {Springer Tracts in Advanced Robotics},
keywords = {highlight},
mendeley-tags = {highlight},
pages = {99--127},
title = {{Topological SLAM}},
volume = {46},
year = {2008}
}
@article{Tuytelaars2008,
abstract = {In this survey, we give an overview of invariant interest point detectors, how they evolved over time, how they work, and what their respective strengths and weaknesses are. We begin with defining the properties of the ideal local feature detector. This is followed by an overview of the literature over the past four decades organized in different categories of feature extraction methods. We then provide a more detailed analysis of a selection of methods which had a particularly significant impact on the research field. We conclude with a summary and promising future research directions.},
author = {Tuytelaars, Tinne and Mikolajczyk, Krystian},
doi = {10.1561/0600000017},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Tuytelaars, Mikolajczyk - 2008 - Local Invariant Feature Detectors A Survey.pdf:pdf},
isbn = {1601981384},
issn = {1572-2740},
journal = {Computer Graphics and Vision},
keywords = {salient features,survey},
mendeley-tags = {salient features,survey},
number = {3},
pages = {177--280},
title = {{Local Invariant Feature Detectors: A Survey}},
volume = {3},
year = {2008}
}
@article{Werner2009,
abstract = {Perceptual aliasing makes topological navigation a difficult task. In this paper we present a general approach for topological SLAM (simultaneous localisation and mapping) which does not require motion or odometry information but only a sequence of noisy measurements from visited places. We propose a particle filtering technique for topological SLAM which relies on a method for disambiguating places which appear indistinguishable using neighbourhood information extracted from the sequence of observations. The algorithm aims to induce a small topological map which is consistent with the observations and simultaneously estimate the location of the robot. The proposed approach is evaluated using a data set of sonar measurements from an indoor environment which contains several similar places. It is demonstrated that our approach is capable of dealing with severe ambiguities and, and that it infers a small map in terms of vertices which is consistent with the sequence of observations.},
author = {Werner, Felix and Maire, Frederic and Sitte, Joaquin and Choset, Howie and Tully, Stephen and Kantor, George},
doi = {10.1109/IROS.2009.5354748},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Werner et al. - 2009 - Topological SLAM using neighbourhood information of places.pdf:pdf},
isbn = {9781424438044},
journal = {2009 IEEE/RSJ International Conference on Intelligent Robots and Systems, IROS 2009},
keywords = {loop closure,topological slam},
mendeley-tags = {loop closure,topological slam},
pages = {4937--4942},
title = {{Topological SLAM using neighbourhood information of places}},
year = {2009}
}
@article{Shen2011,
abstract = {We are interested in the problem of surveilling and exploring environments that include both indoor and outdoor settings. Aerial vehicles offer mobility and perspective advantages over ground platforms and micro aerial vehicles (MAVs) are particularly applicable to buildings with multiple floors where stairwells can be an obstacle to ground vehicles. A challenge when operating in indoor environments is the lack of an external source of localization such as GPS. For these reasons, in this work we focus on autonomous navigation in buildings with multiple floors without requiring an external source of localization or prior knowledge of the environment. To ensure that the robot is fully autonomous, we require all computation to occur on the robot without need for external infrastructure, communication, or human interaction beyond high-level commands. Therefore, we pursue a system design and methodology capable of autonomous navigation with real-time performance on a mobile processor using only onboard sensors (Fig. 1); where in this work autonomous navigation considers multi-floor mapping with loop closure, localization, planning, and control.},
archivePrefix = {arXiv},
arxivId = {0909.2193v2},
author = {Shen, Shaojie and Michael, Nathan and Kumar, Vijay},
doi = {10.1109/ICRA.2011.5980357},
eprint = {0909.2193v2},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Shen, Michael, Kumar - 2011 - Autonomous multi-floor indoor navigation with a computationally constrained MAV.pdf:pdf},
isbn = {9781612843865},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
pages = {20--25},
pmid = {6973},
title = {{Autonomous multi-floor indoor navigation with a computationally constrained MAV}},
year = {2011}
}
@article{Zeil2003,
author = {Zeil, Jochen and Hofmann, Martin I and Chahl, Javaan S},
doi = {10.1364/JOSAA.20.000450},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Zeil, Hofmann, Chahl - 2003 - Catchment areas of panoramic snapshots in outdoor scenes.pdf:pdf},
journal = {Journal of the Optical Society of America A},
number = {3},
pages = {450--469},
title = {{Catchment areas of panoramic snapshots in outdoor scenes}},
volume = {20},
year = {2003}
}
@inproceedings{Mcgee2005,
author = {Mcgee, Timothy G and Sengupta, Raja and Hedrick, Karl},
booktitle = {International Conference on Robotics and Automation},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Mcgee, Sengupta, Hedrick - 2005 - Obstacle Detection for Small Autonomous Aircraft Using Sky Segmentation.pdf:pdf},
isbn = {078038914X},
number = {April},
pages = {4679--4684},
title = {{Obstacle Detection for Small Autonomous Aircraft Using Sky Segmentation}},
year = {2005}
}
@article{Domingos2012,
abstract = {MACHINE LEARNING SYSTEMS automatically learn programs from data. This is often a very attractive alternative to manually constructing them, and in the last decade the use of machine learning has spread rapidly throughout computer science and beyond. Machine learning is used in Web search, spam filters, recommender systems, ad placement, credit scoring, fraud detection, stock trading, drug design, and many other applications. A recent report from the McKinsey Global Institute asserts that machine learning (a.k.a. data mining or predictive analytics) will be the driver of the next big wave of innovation. Several fine textbooks are available to interested practitioners and researchers (for example, Mitchell and Witten et al.). However, much of the “folk knowledge” that is needed to successfully develop machine learning applications is not readily available in them. As a result, many machine learning projects take much longer than necessary or wind up producing less-than-ideal results. Yet much of this folk knowledge is fairly easy to communicate. This is the purpose of this article.},
archivePrefix = {arXiv},
arxivId = {cs/9605103},
author = {Domingos, Pedro},
doi = {10.1145/2347736.2347755},
eprint = {9605103},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Domingos - 2012 - A few useful things to know about machine learning.pdf:pdf},
isbn = {0001-0782},
issn = {00010782},
journal = {Communications of the ACM},
number = {10},
pages = {78},
pmid = {1000183096},
primaryClass = {cs},
title = {{A few useful things to know about machine learning}},
url = {http://dl.acm.org/citation.cfm?doid=2347736.2347755},
volume = {55},
year = {2012}
}
@article{Chen2018,
archivePrefix = {arXiv},
arxivId = {arXiv:1808.07528v1},
author = {Chen, Richard and Mahmood, Faisal and Yuille, Alan and Durr, Nicholas J},
eprint = {arXiv:1808.07528v1},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Chen et al. - 2018 - Rethinking Monocular Depth Estimation with Adversarial Training.pdf:pdf},
journal = {arXiv preprint arXiv:1808.07528},
title = {{Rethinking Monocular Depth Estimation with Adversarial Training}},
year = {2018}
}
@article{Zemalache2009,
abstract = {The work describes an automatically on-line self-tunable fuzzy inference system (STFIS) of a new configuration of mini-flying called XSF (X4 Stationnary Flyer) drone. A fuzzy controller based on on-line optimization of a zero order Takagi–Sugeno fuzzy inference system (FIS) by a back propagation-like algorithm is successfully applied. It is used to minimize a cost function that is made up of a quadratic error term and a weight decay term that prevents an excessive growth of parameters. Thus, we carried out control for the continuation of simple trajectories such as the follow-up of straight lines, and complex (half circle, corner, and helicoidal) by using the STFIS technique. This permits to prove the effectiveness of the proposed control law. Simulation results and a comparison with a static feedback linearization controller (SFL) are presented and discussed. We studied the robustness of the two controllers used in the presence of disturbances. We presented two types of disturbances, the case of a breakdown of an engine as well as a gust of wind.},
author = {Zemalache, K.M. and Maaref, H.},
doi = {10.1016/j.asoc.2008.08.007},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Zemalache, Maaref - 2009 - Controlling a drone Comparison between a based model method and a fuzzy inference system.pdf:pdf},
isbn = {1568-4946},
issn = {15684946},
journal = {Applied Soft Computing},
keywords = {Drone,Dynamic systems,Self-tunable fuzzy inference system,Static feedback linearization controller,Tracking control},
number = {2},
pages = {553--562},
title = {{Controlling a drone: Comparison between a based model method and a fuzzy inference system}},
url = {http://www.sciencedirect.com/science/article/pii/S1568494608001191},
volume = {9},
year = {2009}
}
@article{Sun2014,
author = {Sun, Deqing and Roth, Stefan and Black, Michael J},
doi = {10.1007/s11263-013-0644-x},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Sun, Roth, Black - 2014 - A Quantitative Analysis of Current Practices in Optical Flow Estimation and the Principles Behind Them.pdf:pdf},
journal = {International Journal of Computer Vision},
keywords = {median filtering,motion boundary,non-local term,optical flow estimation,practices},
number = {2},
pages = {115--137},
title = {{A Quantitative Analysis of Current Practices in Optical Flow Estimation and the Principles Behind Them}},
volume = {106},
year = {2014}
}
@article{Hornung2013,
abstract = {Three-dimensional models provide a volumetric representation of space which is important for a variety of robotic applications including flying robots and robots that are equipped with manipulators. In this paper, we present an open-source framework to generate volumetric 3D environment models. Our mapping approach is based on octrees and uses probabilistic occupancy estimation. It explicitly represents not only occupied space, but also free and unknown areas. Furthermore, we propose an octree map compression method that keeps the 3D models compact. Our framework is available as an open-source C++ library and has already been successfully applied in several robotics projects. We present a series of experimental results carried out with real robots and on publicly available real-world datasets. The results demonstrate that our approach is able to update the representation efficiently and models the data consistently while keeping the memory requirement at a minimum.},
author = {Hornung, Armin and Wurm, Kai M. and Bennewitz, Maren and Stachniss, Cyrill and Burgard, Wolfram},
doi = {10.1007/s10514-012-9321-0},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Hornung et al. - 2013 - OctoMap An efficient probabilistic 3D mapping framework based on octrees.pdf:pdf},
isbn = {0929-5593},
issn = {09295593},
journal = {Autonomous Robots},
keywords = {3D,Mapping,Navigation,Probabilistic},
number = {3},
pages = {189--206},
title = {{OctoMap: An efficient probabilistic 3D mapping framework based on octrees}},
volume = {34},
year = {2013}
}
@article{Gonzalez2011,
abstract = {This paper describes a new architecture for Visual Simultaneous Localization And Mapping (SLAM), aimed at being implemented on different embedded boards on mobile robots. We detail the architecture of our C-coded program, which gives real-time results. We present the different methods used to get a real-time solution for the visual SLAM problem, for image processing, landmark parameterization and reparameterization, Extended Kalman Filter (EKF) and Active Search. We finally expose the results of our algorithm embedded on three different boards which are intended to be mounted on mobile robots.},
author = {Gonzalez, Aur{\'{e}}lien and Codol, Jean Marie and Devy, Michel},
doi = {10.1109/ICECS.2011.6122362},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Gonzalez, Codol, Devy - 2011 - A C-embedded algorithm for real-time monocular SLAM.pdf:pdf},
isbn = {9781457718458},
journal = {2011 18th IEEE International Conference on Electronics, Circuits, and Systems, ICECS 2011},
keywords = {switched-capacitor, SC simulator, CAD, parasitic,},
pages = {665--668},
title = {{A C-embedded algorithm for real-time monocular SLAM}},
year = {2011}
}
@article{Radmanesh2018a,
abstract = {Unmanned aerial vehicles (UAVs) have recently attracted the attention of researchers due to their numerous potential civilian applications. However, current robot navigation technologies need further development for e ffi cient application to various scenarios. One key issue is the $\backslash$ Sense and Avoid " capability, currently of immense interest to researchers. Such a capability is required for safe operation of UAVs in civilian domain. For autonomous decision making and control of UAVs, several path-planning and navigation algorithms have been proposed. This is a challenging task to be carried out in a 3D environment, especially while accounting for sensor noise, uncertainties in operating conditions, and real-time applicability. Heuristic and non-heuristic or exact techniques are the two solution methodologies that categorize path-planning algorithms. The aim of this paper is to carry out a comprehensive and comparative study of existing UAV path- planning algorithms for both methods. Three di ff erent obstacle scenarios test the performance of each algorithm. We have compared the computational time and solution optimality, and tested each algorithm with variations in the availability of global and local obstacle information.},
author = {Radmanesh, Mohammadreza and Kumar, Manish and Guentert, Paul H. and Sarim, Mohammad},
doi = {10.1142/s2301385018400022},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Radmanesh et al. - 2018 - Overview of Path-Planning and Obstacle Avoidance Algorithms for UAVs A Comparative Study.pdf:pdf},
issn = {2301-3850},
journal = {Unmanned Systems},
number = {02},
pages = {95--118},
title = {{Overview of Path-Planning and Obstacle Avoidance Algorithms for UAVs: A Comparative Study}},
volume = {06},
year = {2018}
}
@inproceedings{Laubach1999,
abstract = {With the success of Mars Pathfinder's Sojourner rover, a new era$\backslash$nof planetary exploration has opened, with demand for highly capable$\backslash$nmobile robots. These robots must be able to traverse long distances over$\backslash$nrough, unknown terrain autonomously, under severe resource constraints.$\backslash$nThis paper reviews issues which are critical for successful autonomous$\backslash$nnavigation of planetary rovers. We report on the {\&}ldquo;Wedgebug{\&}rdquo;$\backslash$nalgorithm for planetary rover navigation. This algorithm is complete,$\backslash$ncorrect, requires minimal memory for storage of its world model, and$\backslash$nuses only on-board sensors, which are guided by the algorithm to$\backslash$nefficiently sense only the data needed for motion planning. The$\backslash$nimplementation of a version of Wedgebug on the Rocky7 Mars rover$\backslash$nprototype at the Jet Propulsion Laboratory is described, and$\backslash$nexperimental results from operation in simulated martian terrain are$\backslash$npresented},
author = {Laubach, S.L. and Burdick, J.W.},
booktitle = {Proceedings 1999 IEEE International Conference on Robotics and Automation (Cat. No.99CH36288C)},
doi = {10.1109/ROBOT.1999.770003},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Laubach, Burdick - 1999 - An autonomous sensor-based path-planner for planetary microrovers.pdf:pdf},
isbn = {0-7803-5180-0},
number = {May 1999},
pages = {347--354},
publisher = {IEEE},
title = {{An autonomous sensor-based path-planner for planetary microrovers}},
url = {http://ieeexplore.ieee.org/document/770003/},
volume = {1},
year = {1999}
}
@article{Hong1992,
author = {Hong, J. and Tan, X. and Pinette, B. and Weiss, R. and Riseman, E.M.},
doi = {10.1109/37.120451},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Hong et al. - 1992 - Image-based homing.pdf:pdf},
issn = {1066-033X},
journal = {IEEE Control Systems},
month = {feb},
number = {1},
pages = {38--45},
title = {{Image-based homing}},
url = {http://ieeexplore.ieee.org/document/120451/},
volume = {12},
year = {1992}
}
@article{Kuipers1978,
author = {KUIPERS, B},
doi = {10.1016/S0364-0213(78)80003-2},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/KUIPERS - 1978 - Modeling spatial knowledge.pdf:pdf},
issn = {03640213},
journal = {Cognitive Science},
month = {jun},
number = {2},
pages = {129--153},
title = {{Modeling spatial knowledge}},
url = {http://doi.wiley.com/10.1016/S0364-0213(78)80003-2},
volume = {2},
year = {1978}
}
@article{Ruder2016,
abstract = {Gradient descent optimization algorithms, while increasingly popular, are often used as black-box optimizers, as practical explanations of their strengths and weaknesses are hard to come by. This article aims to provide the reader with intuitions with regard to the behaviour of different algorithms that will allow her to put them to use. In the course of this overview, we look at different variants of gradient descent, summarize challenges, introduce the most common optimization algorithms, review architectures in a parallel and distributed setting, and investigate additional strategies for optimizing gradient descent.},
archivePrefix = {arXiv},
arxivId = {1609.04747},
author = {Ruder, Sebastian},
doi = {10.1111/j.0006-341X.1999.00591.x},
eprint = {1609.04747},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Ruder - 2016 - An overview of gradient descent optimization algorithms.pdf:pdf},
isbn = {1541-0420},
issn = {0006341X},
pages = {1--14},
pmid = {11318219},
title = {{An overview of gradient descent optimization algorithms}},
url = {http://arxiv.org/abs/1609.04747},
year = {2016}
}
@inproceedings{Cordts2016a,
abstract = {Visual understanding of complex urban street scenes is an enabling factor for a wide range of applications. Object detection has benefited enormously from large-scale datasets, especially in the context of deep learning. For semantic urban scene understanding, however, no current dataset adequately captures the complexity of real-world urban scenes. To address this, we introduce Cityscapes, a benchmark suite and large-scale dataset to train and test approaches for pixel-level and instance-level semantic labeling. Cityscapes is comprised of a large, diverse set of stereo video sequences recorded in streets from 50 different cities. 5000 of these images have high quality pixel-level annotations; 20000 additional images have coarse annotations to enable methods that leverage large volumes of weakly-labeled data. Crucially, our effort exceeds previous attempts in terms of dataset size, annotation richness, scene variability, and complexity. Our accompanying empirical study provides an in-depth analysis of the dataset characteristics, as well as a performance evaluation of several state-of-the-art approaches based on our benchmark.},
archivePrefix = {arXiv},
arxivId = {1604.01685},
author = {Cordts, Marius and Omran, Mohamed and Ramos, Sebastian and Rehfeld, Timo and Enzweiler, Markus and Benenson, Rodrigo and Franke, Uwe and Roth, Stefan and Schiele, Bernt},
booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
doi = {10.1109/CVPR.2016.350},
eprint = {1604.01685},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Cordts et al. - 2016 - The Cityscapes Dataset for Semantic Urban Scene Understanding.pdf:pdf},
isbn = {978-1-4673-8851-1},
issn = {10636919},
pages = {3213--3223},
pmid = {23554596},
title = {{The Cityscapes Dataset for Semantic Urban Scene Understanding}},
year = {2016}
}
@article{Klein2007,
abstract = {This paper presents a method of estimating camera pose in an unknown scene. While this has previously been attempted by adapting SLAM algorithms developed for robotic exploration, we propose a system specifically designed to track a hand-held camera in a small AR workspace. We propose to split tracking and mapping into two separate tasks, processed in parallel threads on a dual-core computer: one thread deals with the task of robustly tracking erratic hand-held motion, while the other produces a 3D map of point features from previously observed video frames. This allows the use of computationally expensive batch optimisation techniques not usually associated with real-time operation: The result is a system that produces detailed maps with thousands of landmarks which can be tracked at frame-rate, with an accuracy and robustness rivalling that of state-of-the-art model-based systems.},
archivePrefix = {arXiv},
arxivId = {arXiv:1407.5736v1},
author = {Klein, Georg and Murray, David},
doi = {10.1109/ISMAR.2007.4538852},
eprint = {arXiv:1407.5736v1},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Klein, Murray - 2007 - Parallel tracking and mapping for small AR workspaces.pdf:pdf},
isbn = {9781424417506},
issn = {00472778},
journal = {2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality, ISMAR},
keywords = {slam},
mendeley-tags = {slam},
pmid = {21736739},
title = {{Parallel tracking and mapping for small AR workspaces}},
year = {2007}
}
@inproceedings{Bloesch2015,
author = {Bloesch, Michael and Omari, Sammy and Hutter, Marco and Siegwart, Roland},
booktitle = {Intelligent Robots and Systems (IROS), 2015 IEEE/RSJ International Conference on},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Bloesch et al. - 2015 - Robust Visual Inertial Odometry Using a Direct EKF-Based Approach.pdf:pdf},
isbn = {9781479999941},
pages = {298--304},
publisher = {IEEE},
title = {{Robust Visual Inertial Odometry Using a Direct EKF-Based Approach}},
year = {2015}
}
@article{Sivaraman2013,
abstract = {This document provides a review of the past decade's literature in on-road vision-based vehicle detection. Over the past decade, vision-based surround perception has matured significantly from its infancy. We detail advances in vehicle detection, discussing representative works from the monocular and stereo-vision domains.We provide discussion on the state-of-the-art, and provide perspective on future research directions in the field.},
author = {Sivaraman, Sayanan and Trivedi, Mohan M.},
doi = {10.1109/IVS.2013.6629487},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Sivaraman, Trivedi - 2013 - A review of recent developments in vision-based vehicle detection.pdf:pdf},
isbn = {978-1-4673-2755-8},
issn = {1931-0587},
journal = {Proc. IEEE Intelligent Vehicles Symposium},
keywords = {Active Safety,Driver Assistance,Feature extraction,Hidden Markov models,Machine Learning,Optical imaging,Real-time Vision,Three-dimensional displays,Tracking,Vehicle detection,Vehicles,active safety,driver assistance,driver information systems,machine learning,monocular-vision domains,object detection,real-time vision,stereo image processing,stereo-vision domains,vehicles,vision-based vehicle detection},
number = {Iv},
pages = {310--315},
title = {{A review of recent developments in vision-based vehicle detection}},
url = {http://ieeexplore.ieee.org/ielx7/6601112/6629437/06629487.pdf?tp={\&}arnumber=6629487{\&}isnumber=6629437{\%}255Cnhttp://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6629487},
year = {2013}
}
@article{Carrio2018,
abstract = {Obstacle avoidance is a key feature for safe Unmanned Aerial Vehicle (UAV) navigation. While solutions have been proposed for static obstacle avoidance, systems enabling avoidance of dynamic objects, such as drones, are hard to implement due to the detection range and field-of-view (FOV) requirements, as well as the constraints for integrating such systems on-board small UAVs. In this work, a dataset of 6k synthetic depth maps of drones has been generated and used to train a state-of-the-art deep learning-based drone detection model. While many sensing technologies can only provide relative altitude and azimuth of an obstacle, our depth map-based approach enables full 3D localization of the obstacle. This is extremely useful for collision avoidance, as 3D localization of detected drones is key to perform efficient collision-free path planning. The proposed detection technique has been validated in several real depth map sequences, with multiple types of drones flying at up to 2 m/s, achieving an average precision of 98.7{\%}, an average recall of 74.7{\%} and a record detection range of 9.5 meters.},
archivePrefix = {arXiv},
arxivId = {1808.00259},
author = {Carrio, Adrian and Vemprala, Sai and Ripoll, Andres and Saripalli, Srikanth and Campoy, Pascual},
eprint = {1808.00259},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Carrio et al. - 2018 - Drone Detection Using Depth Maps.pdf:pdf},
isbn = {9781538680933},
title = {{Drone Detection Using Depth Maps}},
url = {http://arxiv.org/abs/1808.00259},
year = {2018}
}
@article{Wystrach2016,
abstract = {The visual systems of animals have to provide information to guide behaviour and the informational requirements of an animal's behavioural repertoire are often reflected in its sensory system. For insects, this is often evident in the optical array of the compound eye. One behaviour that insects share with many animals is the use of learnt visual information for navigation. As ants are expert visual navigators it may be that their vision is optimised for navigation. Here we take a computational approach in asking how the details of the optical array influence the informational content of scenes used in simple view matching strategies for orientation. We find that robust orientation is best achieved with low-resolution visual information and a large field of view, similar to the optical properties seen for many ant species. A lower resolution allows for a trade-off between specificity and generalisation for stored views. Additionally, our simulations show that orientation performance increases if different portions of the visual field are considered as discrete visual sensors, each giving an independent directional estimate. This suggests that ants might benefit by processing information from their two eyes independently.},
author = {Wystrach, Antoine and Dewar, Alex and Philippides, Andrew and Graham, Paul},
doi = {10.1007/s00359-015-1052-1},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Wystrach et al. - 2016 - How do field of view and resolution affect the information content of panoramic scenes for visual navigation A.pdf:pdf},
issn = {14321351},
journal = {Journal of Comparative Physiology A: Neuroethology, Sensory, Neural, and Behavioral Physiology},
keywords = {Ants,Image matching,Route navigation,Snapshot,View-based homing,biology,scene familiarity},
mendeley-tags = {biology,scene familiarity},
number = {2},
pages = {87--95},
pmid = {26582183},
publisher = {Springer Berlin Heidelberg},
title = {{How do field of view and resolution affect the information content of panoramic scenes for visual navigation? A computational investigation}},
volume = {202},
year = {2016}
}
@article{Hirschmuller2008,
author = {Hirschm{\"{u}}ller, Heiko},
doi = {10.1109/TPAMI.2007.1166},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Hirschm{\"{u}}ller - 2008 - Stereo Processing by Semiglobal Matching and Mutual Information.pdf:pdf},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
number = {2},
pages = {328--341},
title = {{Stereo Processing by Semiglobal Matching and Mutual Information}},
volume = {30},
year = {2008}
}
@article{Khatib2006,
abstract = {This paper presents a unique real-time obstacle avoidance approach for manipulators and mobile robots based on the artificial potential field concept. Collision avoidance, tradi-tionally considered a high level planning problem, can be effectively distributed between different levels of control, al-lowing real-time robot operations in a complex environment. This method has been extended to moving obstacles by using a time-varying artificial patential field. We have applied this obstacle avoidance scheme to robot arm mechanisms and have used a new approach to the general problem of real-time manipulator control. We reformulated the manipulator con-trol problem as direct control of manipulator motion in oper-ational space{\&}mdash;the space in which the task is originally described{\&}mdash;rather than as control of the task's corresponding joint space motion obtained only after geometric and kine-matic transformation. Outside the obstacles ' regions of influ-ence, we caused the end effector to move in a straight line with an upper speed limit. The artificial potential field ap-proach has been extended to collision avoidance for all ma-nipulator links. In addition, a joint space artificial potential field is used to satisfy the manipulator internal joint con-straints. This method has been implemented in the COSMOS system for a PUMA 560 robot. Real-time collision avoidance demonstrations on moving obstacles have been performed by using visual sensing.},
author = {Khatib, Oussama},
doi = {10.1177/027836498600500106},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Khatib - 1986 - Real-Time Obstacle Avoidance for Manipulators and Mobile Robots.pdf:pdf},
isbn = {142440259X},
issn = {0278-3649},
journal = {The International Journal of Robotics Research},
month = {mar},
number = {1},
pages = {90--98},
title = {{Real-Time Obstacle Avoidance for Manipulators and Mobile Robots}},
url = {http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.910.1287 http://journals.sagepub.com/doi/10.1177/027836498600500106},
volume = {5},
year = {1986}
}
@article{Liu2016,
author = {Liu, Guanxiong and Geng, Yishuang and Pahlavan, Kaveh},
doi = {10.1109/UIC-ATC-ScalCom-CBDCom-IoP.2015.334},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Liu, Geng, Pahlavan - 2016 - Direction estimation error model of embedded magnetometer in indoor navigation environment.pdf:pdf},
isbn = {9781467372114},
journal = {Proceedings - 2015 IEEE 12th International Conference on Ubiquitous Intelligence and Computing, 2015 IEEE 12th International Conference on Advanced and Trusted Computing, 2015 IEEE 15th International Conference on Scalable Computing and Communications, 20},
keywords = {Error model,Inertial navigation system (INS),Magnetometer,Smart-phone localization,Wireless localization},
pages = {1842--1846},
title = {{Direction estimation error model of embedded magnetometer in indoor navigation environment}},
year = {2016}
}
@inproceedings{Ulrich2000a,
archivePrefix = {arXiv},
arxivId = {10.1109/ROBOT.2000.846405},
author = {Ulrich, I and Borenstein, J},
booktitle = {Proceedings 2000 ICRA. Millennium Conference. IEEE International Conference on Robotics and Automation. Symposia Proceedings (Cat. No.00CH37065)},
doi = {10.1109/ROBOT.2000.846405},
eprint = {ROBOT.2000.846405},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Ulrich, Borenstein - 2000 - VFH local obstacle avoidance with look-ahead verification.pdf:pdf},
isbn = {0-7803-5886-4},
issn = {1050-4729},
keywords = {*file-import-10-04-21},
number = {April},
pages = {2505--2511},
primaryClass = {10.1109},
publisher = {IEEE},
title = {{VFH*: local obstacle avoidance with look-ahead verification}},
url = {http://ieeexplore.ieee.org/document/846405/},
volume = {3},
year = {2000}
}
@article{Moller2007,
author = {M{\"{o}}ller, Ralf and Vardy, Andrew and Kreft, Sven and Ruwisch, Sebastian},
doi = {10.1007/s10514-007-9043-x},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/M{\"{o}}ller et al. - 2007 - Visual homing in environments with anisotropic landmark distribution.pdf:pdf},
issn = {09295593},
journal = {Autonomous Robots},
keywords = {Descent in images distances,Matched filter,Newton's method,Visual homing},
number = {3},
pages = {231--245},
pmid = {1000302753},
title = {{Visual homing in environments with anisotropic landmark distribution}},
volume = {23},
year = {2007}
}
@inproceedings{Angeli2008,
author = {Angeli, Adrien and Doncieux, S. and Meyer, J.-A. and Filliat, D.},
booktitle = {2008 IEEE/RSJ International Conference on Intelligent Robots and Systems},
doi = {10.1109/IROS.2008.4650675},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Angeli et al. - 2008 - Incremental vision-based topological SLAM.pdf:pdf},
isbn = {978-1-4244-2057-5},
keywords = {bagofwords,highlight},
mendeley-tags = {bagofwords,highlight},
pages = {1031--1036},
publisher = {IEEE},
title = {{Incremental vision-based topological SLAM}},
url = {http://ieeexplore.ieee.org/document/4650675/},
year = {2008}
}
@article{Cheung2008,
author = {Cheung, Allen and St{\"{u}}rzl, Wolfgang and Zeil, Jochen and Cheng, Ken},
doi = {10.1037/0097-7403.34.1.15},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Cheung et al. - 2008 - The Information Content of Panoramic Images II View-Based Navigation in Nonrectangular Experimental Arenas.pdf:pdf},
journal = {Journal of Experimental Psychology: Animal Behavior Processes},
keywords = {0097-7403,1,10,1037,15,34,been tested in,computer simulation,doi,dx,geometry,http,laboratory rats often have,maze with arms radi-,org,rats,small experimental spaces,spatial learning,such as a radial,supp,supplemental materials,the spatial abilities of,view-based navigation},
number = {1},
pages = {15--30},
title = {{The Information Content of Panoramic Images II: View-Based Navigation in Nonrectangular Experimental Arenas}},
volume = {34},
year = {2008}
}
@inproceedings{Whelan2015,
author = {Whelan, Thomas and Leutenegger, Stefan and Salas-moreno, Renato F and Glocker, Ben and Davison, Andrew J},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Whelan et al. - 2015 - ElasticFusion Dense SLAM Without A Pose Graph.pdf:pdf},
publisher = {Robotics: Science and Systems},
title = {{ElasticFusion: Dense SLAM Without A Pose Graph}},
year = {2015}
}
@article{Hinzmann,
archivePrefix = {arXiv},
arxivId = {arXiv:1712.06837v1},
author = {Hinzmann, Timo and Taubner, Tim and Siegwart, Roland},
eprint = {arXiv:1712.06837v1},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Hinzmann, Taubner, Siegwart - Unknown - Flexible Stereo Constrained, Non-rigid, Wide-baseline Stereo Vision for Fixed-wing Aerial Platfo.pdf:pdf},
title = {{Flexible Stereo: Constrained, Non-rigid, Wide-baseline Stereo Vision for Fixed-wing Aerial Platforms}}
}
@article{Moller2006,
author = {M{\"{o}}ller, Ralf and Vardy, Andrew},
doi = {10.1007/s00422-006-0095-3},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/M{\"{o}}ller, Vardy - 2006 - Local visual homing by matched-filter descent in image distances.pdf:pdf},
issn = {0340-1200},
journal = {Biological Cybernetics},
month = {oct},
number = {5},
pages = {413--430},
title = {{Local visual homing by matched-filter descent in image distances}},
url = {http://link.springer.com/10.1007/s00422-006-0095-3},
volume = {95},
year = {2006}
}
@article{Geiger2012,
author = {Geiger, Andreas and Lenz, Philip and Urtasun, Raquel},
doi = {10.1109/CVPR.2012.6248074},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Geiger, Lenz, Urtasun - 2012 - Are we ready for Autonomous Driving The KITTI Vision Benchmark Suite.pdf:pdf},
isbn = {9781467312288},
journal = {Computer Vision and Pattern Recognition},
pages = {3354--3361},
title = {{Are we ready for Autonomous Driving? The KITTI Vision Benchmark Suite}},
year = {2012}
}
@article{Zingg2010,
abstract = {Safe navigation through corridors plays a major role in the autonomous use of Micro Aerial Vehicles (MAVs) in indoor environments. In this paper, we present an approach for wall collision avoidance using a depth map based on optical flow from on board camera images. An omnidirectional fisheye camera is used as a primary sensor, while IMU data is needed for compensating rotational effects of the optical flow. The here presented approach is designed for safely maneuvering a helicopter through an indoor corridor. Results based on real images taken in a corridor with textured walls are shown at the end of this paper.},
author = {Zingg, Simon and Scaramuzza, Davide and Weiss, Stephan and Siegwart, Roland},
doi = {10.1109/ROBOT.2010.5509777},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Zingg et al. - 2010 - MAV navigation through indoor corridors using optical flow.pdf:pdf},
isbn = {9781424450381},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
keywords = {experiment,low complexity,monocular,optical flow,scenario: corridor,turn away from flow},
mendeley-tags = {experiment,low complexity,monocular,optical flow,scenario: corridor,turn away from flow},
pages = {3361--3368},
title = {{MAV navigation through indoor corridors using optical flow}},
year = {2010}
}
@article{Zhu2017,
abstract = {Image-to-image translation is a class of vision and graphics problems where the goal is to learn the mapping between an input image and an output image using a training set of aligned image pairs. However, for many tasks, paired training data will not be available. We present an approach for learning to translate an image from a source domain {\$}X{\$} to a target domain {\$}Y{\$} in the absence of paired examples. Our goal is to learn a mapping {\$}G: X \backslashrightarrow Y{\$} such that the distribution of images from {\$}G(X){\$} is indistinguishable from the distribution {\$}Y{\$} using an adversarial loss. Because this mapping is highly under-constrained, we couple it with an inverse mapping {\$}F: Y \backslashrightarrow X{\$} and introduce a cycle consistency loss to push {\$}F(G(X)) \backslashapprox X{\$} (and vice versa). Qualitative results are presented on several tasks where paired training data does not exist, including collection style transfer, object transfiguration, season transfer, photo enhancement, etc. Quantitative comparisons against several prior methods demonstrate the superiority of our approach.},
archivePrefix = {arXiv},
arxivId = {1703.10593v3},
author = {Zhu, Jun-yan and Park, Taesung and Isola, Phillip and Efros, Alexei A},
doi = {10.1109/ICCV.2017.244},
eprint = {1703.10593v3},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Zhu et al. - 2017 - Unpaired image-to-image translation using cycle-consistent adversarial networks.pdf:pdf},
isbn = {978-1-5386-1032-9},
issn = {15505499},
journal = {arXiv preprint},
pages = {2223--2232},
title = {{Unpaired image-to-image translation using cycle-consistent adversarial networks}},
url = {http://arxiv.org/abs/1703.10593v3},
year = {2017}
}
@article{Goerzen2010,
abstract = {A fundamental aspect of autonomous vehicle guidance is planning trajectories. Historically, two fields have contributed to trajectory or motion planning methods: robotics and dynamics and control. The former typically have a stronger focus on computational issues and real-time robot control, while the latter emphasize the dynamic behavior and more specific aspects of trajectory performance. Guidance for Unmanned Aerial Vehicles (UAVs), including fixed- and rotary-wing aircraft, involves significant differences from most traditionally defined mobile and manipulator robots. Qualities characteristic to UAVs include non-trivial dynamics, three-dimensional environments, disturbed operating conditions, and high levels of uncertainty in state knowledge. Otherwise, UAV guidance shares qualities with typical robotic motion planning problems, including partial knowledge of the environment and tasks that can range from basic goal interception, which can be precisely specified, to more general tasks like surveillance and reconnaissance, which are harder to specify. These basic planning problems involve continual interaction with the environment. The purpose of this paper is to provide an overview of existing motion planning algorithms while adding perspectives and practical examples from UAV guidance approaches.},
author = {Goerzen, C. and Kong, Z. and Mettler, B.},
doi = {10.1007/s10846-009-9383-1},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Goerzen, Kong, Mettler - 2010 - A survey of motion planning algorithms from the perspective of autonomous UAV guidance.pdf:pdf},
isbn = {0921-0296},
issn = {09210296},
journal = {Journal of Intelligent and Robotic Systems},
keywords = {Algorithm,Autonomous,Complexity,Guidance,Heuristics,Motion planning,Optimization,Trajectory,UAV},
number = {1-4},
pages = {65},
title = {{A survey of motion planning algorithms from the perspective of autonomous UAV guidance}},
volume = {57},
year = {2010}
}
@article{Weber,
author = {Weber, Jason and Penn, Joseph},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Weber, Penn - Unknown - Creation and Rendering of Realistic Trees.pdf:pdf},
title = {{Creation and Rendering of Realistic Trees}}
}
@inproceedings{Zhang2000,
abstract = {Scene flow is the 3D motion field of points in the world. Given N (N{\textgreater}1) image sequences gathered with a N-eye stereo camera or N calibrated cameras, we present a novel system which integrates 3D scene flow and structure recovery in order to complement each other's performance. We do not assume rigidity of the scene motion, thus allowing for non-rigid motion in the scene. In our work, images are segmented into small regions. We assume that each small region is undergoing similar motion, represented by a 3D affine model. Nonlinear motion model fitting based on both optical flow constraints and stereo constraints is then carried over each image region in order to simultaneously estimate 3D motion correspondences and structure. To ensure the robustness, several regularization constraints are also introduced. A recursive algorithm is designed to incorporate the local and regularization constraints. Experimental results on both synthetic and real data demonstrate the effectiveness of our integrated 3D motion and structure analysis scheme},
author = {Zhang, Y and Kambhamettu, C},
booktitle = {Proceedings IEEE Conference on Computer Vision and Pattern Recognition. CVPR 2000 (Cat. No.PR00662)},
doi = {10.1109/CVPR.2000.854939},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Zhang, Kambhamettu - 2000 - Integrated 3D scene flow and structure recovery from multiview image sequences.pdf:pdf},
issn = {1063-6919},
keywords = {3D affine model,3D motion correspondences,3D motion field,Algorithm design and analysis,Cameras,Image motion analysis,Image segmentation,Image sequences,Layout,Motion analysis,Motion estimation,N-eye stereo camera,Nonlinear optics,Robustness,calibrated cameras,cameras,image region,image restoration,image segmentation,image sequences,integrated 3D motion,integrated 3D scene flow,motion estimation,multiview image sequences,non-rigid motion,nonlinear motion model fitting,optical flow constraints,recursive algorithm,regularization constraints,scene motion,stereo constraints,structure analysis scheme,structure recovery},
pages = {674--681 vol.2},
title = {{Integrated 3D scene flow and structure recovery from multiview image sequences}},
volume = {2},
year = {2000}
}
@article{McCarley2000,
abstract = {Attneave (1954) and Barlow (1961) proposed that the visual system might increase efficiency of representation by preferentially encoding spatiotemporally redundant patterns of the external world. The present experiments tested the application of this principle to three-dimensional (3-D) perceptual organization, capitalizing on the ecological constraint that human observers must frequently interact with objects arranged on the ground or on a surface parallel to it (Gibson, 1950). Observers performed a task that required them to perceptually segregate and search multiple items distributed in depth and embedded within a larger, 3-D array of distractors. Stimulus displays were organized to globally recede top-away in depth, as if attached to an underlying ground-like surface, or bottom-away, as if attached to an overhanging ceiling-like surface; ground-like and ceiling-like displays differed only in the direction of disparity gradient within the displays. Primary findings revealed superior performance with ground-like displays, suggesting that spatially and stereoscopically distributed items are more easily organized to represent an ecologically representative pattern, even when no inherent physical regularities favor that pattern.},
author = {McCarley, Jason S. and He, Zijiang J.},
doi = {10.3758/BF03212105},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/McCarley, He - 2000 - Asymmetry in 3-D perceptual organization Ground-like surface superior to ceiling-like surface.pdf:pdf},
isbn = {0031-5117 (Print)$\backslash$r0031-5117 (Linking)},
issn = {00315117},
journal = {Perception and Psychophysics},
number = {3},
pages = {540--549},
pmid = {10909244},
title = {{Asymmetry in 3-D perceptual organization: Ground-like surface superior to ceiling-like surface}},
volume = {62},
year = {2000}
}
@incollection{Singh2010,
author = {Singh, Gautam and Ko{\v{s}}eck{\'{a}}, Jana},
booktitle = {Omnidirectional Robot Vision workshop, held with IEEE ICRA},
doi = {10.1.1.404.4051},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Singh, Ko{\v{s}}eck{\'{a}} - 2010 - Visual Loop Closing using Gist Descriptors in Manhattan World.pdf:pdf},
title = {{Visual Loop Closing using Gist Descriptors in Manhattan World}},
year = {2010}
}
@article{Carreira2017,
abstract = {The paucity of videos in current action classification datasets (UCF-101 and HMDB-51) has made it difficult to identify good video architectures, as most methods obtain similar performance on existing small-scale benchmarks. This paper re-evaluates state-of-the-art architectures in light of the new Kinetics Human Action Video dataset. Kinetics has two orders of magnitude more data, with 400 human action classes and over 400 clips per class, and is collected from realistic, challenging YouTube videos. We provide an analysis on how current architectures fare on the task of action classification on this dataset and how much performance improves on the smaller benchmark datasets after pre-training on Kinetics. We also introduce a new Two-Stream Inflated 3D ConvNet (I3D) that is based on 2D ConvNet inflation: filters and pooling kernels of very deep image classification ConvNets are expanded into 3D, making it possible to learn seamless spatio-temporal feature extractors from video while leveraging successful ImageNet architecture designs and even their parameters. We show that, after pre-training on Kinetics, I3D models considerably improve upon the state-of-the-art in action classification, reaching 80.7{\%} on HMDB-51 and 98.0{\%} on UCF-101.},
archivePrefix = {arXiv},
arxivId = {1705.07750},
author = {Carreira, Joao and Zisserman, Andrew},
doi = {10.1109/CVPR.2017.502},
eprint = {1705.07750},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Carreira, Zisserman - 2017 - Quo Vadis, Action Recognition A New Model and the Kinetics Dataset.pdf:pdf},
isbn = {978-1-5386-0457-1},
journal = {Cvpr},
pages = {6299--6308},
title = {{Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset}},
url = {http://arxiv.org/abs/1705.07750},
year = {2017}
}
@article{Tijmons2016,
abstract = {The development of autonomous lightweight MAVs, capable of navigating in unknown indoor environments, is one of the major challenges in robotics. The complexity of this challenge comes from constraints on weight and power consumption of onboard sensing and processing devices. In this paper we propose the "Droplet" strategy, an avoidance strategy that outperforms reactive avoidance strategies by allowing constant speed maneuvers while being computationally extremely efficient. The strategy deals with nonholonomic motion constraints of most fixed and flapping wing platforms, and with the limited field-of-view of stereo camera systems. It guarantees obstacle-free flight in the absence of sensor and motor noise. We first analyze the strategy in simulation, and then show its robustness in real-world conditions by implementing it on a 21-gram flapping wing MAV.},
archivePrefix = {arXiv},
arxivId = {1604.00833},
author = {Tijmons, Sjoerd and de Croon, Guido and Remes, Bart and {De Wagter}, Christophe and Mulder, Max},
eprint = {1604.00833},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Tijmons et al. - 2016 - Obstacle Avoidance Strategy using Onboard Stereo Vision on a Flapping Wing MAV.pdf:pdf},
keywords = {TU Delft,experiment,low complexity,nieuwe referenties,obstacle avoidance,reactive obstacle avoidance,safe region,scenario: room,simulation,stereo vision},
mendeley-tags = {TU Delft,experiment,low complexity,nieuwe referenties,obstacle avoidance,reactive obstacle avoidance,safe region,scenario: room,simulation,stereo vision},
pages = {1--13},
title = {{Obstacle Avoidance Strategy using Onboard Stereo Vision on a Flapping Wing MAV}},
url = {http://arxiv.org/abs/1604.00833},
year = {2016}
}
@article{Srinivasan2015,
abstract = {Animals that travel large distances in search of food need to be equipped with navigation systems that are capable of keeping track of the distance and direction of travel throughout their outbound journey, so that they may return home expeditiously and without losing their way. The challenge of homing is especially acute when the environment is devoid of landmarks. Desert ants and honeybees are able to meet this challenge, despite their minuscule brains and restricted computational capacity. This article reviews some of the processes and mechanisms that underlie the homing abilities of these creatures, which are among the best-understood navigators in the animal kingdom.},
author = {Srinivasan, Mandyam V.},
doi = {10.1007/s00359-015-1000-0},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Srinivasan - 2015 - Where paths meet and cross navigation by path integration in the desert ant and the honeybee.pdf:pdf},
issn = {14321351},
journal = {Journal of Comparative Physiology A: Neuroethology, Sensory, Neural, and Behavioral Physiology},
keywords = {Apis mellifera,Cataglyphis,Dead reckoning,Insect,Navigation,Odometry,biology,path integration},
mendeley-tags = {biology,path integration},
number = {6},
pages = {533--546},
pmid = {25971358},
publisher = {Springer Berlin Heidelberg},
title = {{Where paths meet and cross: navigation by path integration in the desert ant and the honeybee}},
volume = {201},
year = {2015}
}
@article{Baker,
abstract = {Conventional video cameras have limited fields of view which make$\backslash$nthem restrictive for certain applications in computational vision. A$\backslash$ncatadioptric sensor uses a combination of lenses and mirrors placed in a$\backslash$ncarefully arranged configuration to capture a much wider field of view.$\backslash$nWhen designing a catadioptric sensor, the shape of the mirror(s) should$\backslash$nideally be selected to ensure that the complete catadioptric system has$\backslash$na single effective viewpoint. In this paper, we derive the complete$\backslash$nclass of single-lens single-mirror catadioptric sensors which have a$\backslash$nsingle viewpoint and an expression for the spatial resolution of a$\backslash$ncatadioptric sensor in terms of the resolution of the camera used to$\backslash$nconstruct it. We also include a preliminary analysis of the defocus blur$\backslash$ncaused by the use of a curved mirror},
author = {Baker, S. and Nayar, S.K.},
doi = {10.1109/ICCV.1998.710698},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Baker, Nayar - Unknown - A theory of catadioptric image formation.pdf:pdf},
isbn = {81-7319-221-9},
journal = {Sixth International Conference on Computer Vision (IEEE Cat. No.98CH36271)},
pages = {35--42},
title = {{A theory of catadioptric image formation}},
url = {http://ieeexplore.ieee.org/document/710698/}
}
@article{Hrabar2012,
abstract = {It is anticipatedthat theMars ScienceLaboratory rover, namedCuriosity,will traverse 10–20 kmon the surface of Mars during its primary mission. In preparation for this traverse, Earth-based tests were performed using Mars weight vehicles. These vehicles were driven over Mars analog bedrock, cohesive soil, and cohesionless sand at various slopes. Vehicle slip was characterized on each of these terrains versus slope for direct upslope driving. Results show that slopes up to 22 degrees are traversable on smooth bedrock and that slopes up to 28 degrees are traversable on some cohesive soils. In cohesionless sand, results show a sharp transition between moderate slip on 10 degree slopes and vehicle embedding at 17 degrees. For cohesionless sand, data are also presented showing the relationship between vehicle slip and wheel sinkage. Side by side testing of the Mars Exploration Rover test vehicle and the Mars Science Laboratory test vehicle show how increased wheel diameter leads to better slope climbing ability in sand for vehicles with nearly identical ground pressure. Lastly, preliminary data from Curiosity's initial driving on Mars are presented and compared to the Earth-based testing, showing good agreement for the driving done during the first 250 Martian days.},
archivePrefix = {arXiv},
arxivId = {10.1.1.91.5767},
author = {Hrabar, Stefan},
doi = {10.1002/rob.21404},
eprint = {10.1.1.91.5767},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Hrabar - 2012 - An evaluation of stereo and laser-based range sensing for rotorcraft unmanned aerial vehicle obstacle avoidance.pdf:pdf},
isbn = {9783902661623},
issn = {15564959},
journal = {Journal of Field Robotics},
keywords = {Backstepping,Non-linear control,Path-following},
month = {mar},
number = {2},
pages = {215--239},
pmid = {22164016},
title = {{An evaluation of stereo and laser-based range sensing for rotorcraft unmanned aerial vehicle obstacle avoidance}},
url = {http://doi.wiley.com/10.1002/rob.21404},
volume = {29},
year = {2012}
}
@article{Faessler2016,
author = {Faessler, Matthias and Fontana, Flavio and Forster, Christian and Mueggler, Elias and Pizzoli, Matia and Scaramuzza, Davide},
doi = {10.1002/rob},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Faessler et al. - 2016 - Autonomous, Vision-based Flight and Live Dense 3D Mapping with a Quadrotor Micro Aerial Vehicle.pdf:pdf},
journal = {Journal of Field Robotics},
number = {4},
pages = {431--450},
title = {{Autonomous, Vision-based Flight and Live Dense 3D Mapping with a Quadrotor Micro Aerial Vehicle}},
volume = {33},
year = {2016}
}
@inbook{Mordohai2012,
abstract = {This paper surveys the state of the art in evaluating the performance of scene flow estimation and points out the difficulties in generating benchmarks with ground truth which have not allowed the development of general, reliable solutions. Hopefully, the renewed interest in dynamic 3D content, which has led to increased research in this area, will also lead to more rigorous evaluation and more effective algorithms. We begin by classifying methods that estimate depth, motion or both from multi-view sequences according to their parameterization of shape and motion. Then, we present several criteria for their evaluation, discuss their strengths and weaknesses and conclude with recommendations.},
address = {Berlin, Heidelberg},
author = {Mordohai, Philippos},
booktitle = {Computer Vision -- ECCV 2012. Workshops and Demonstrations: Florence, Italy, October 7-13, 2012, Proceedings, Part II},
doi = {10.1007/978-3-642-33868-7_15},
editor = {Fusiello, Andrea and Murino, Vittorio and Cucchiara, Rita},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Mordohai - 2012 - On the Evaluation of Scene Flow Estimation.pdf:pdf},
isbn = {978-3-642-33868-7},
pages = {148--157},
publisher = {Springer Berlin Heidelberg},
title = {{On the Evaluation of Scene Flow Estimation}},
url = {https://doi.org/10.1007/978-3-642-33868-7{\_}15},
year = {2012}
}
@article{Croon2012,
annote = {NULL},
author = {Croon, G C H E De and Wagter, C De and Remes, B D W and Ruijsink, R},
doi = {10.1016/j.robot.2011.10.001},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Croon et al. - 2012 - Sub-sampling Real-time vision for micro air vehicles.pdf:pdf},
issn = {0921-8890},
journal = {Robotics and Autonomous Systems},
number = {2},
pages = {167--181},
publisher = {Elsevier B.V.},
title = {{Sub-sampling: Real-time vision for micro air vehicles}},
url = {http://dx.doi.org/10.1016/j.robot.2011.10.001},
volume = {60},
year = {2012}
}
@article{Milford2004,
abstract = {The paper presents a new approach to the problem of Simultaneous Localization and Mapping - SLAM - inspired by computational models of the hippocampus of rodents. The rodent hippocampus bas been extensively studied with respect to navigation tasks, and displays many of the properties of a desirable SLAM solution. RatSLAM is an implementation of a hippocampal model that can perform SLAM in real time on a real robot. It uses a competitive amactor network to integrate odometric information with landmark sensing to form a consistent representation of the environmeut Experimental results show that RatSLAM can operate with ambiguous landmark information and recover from both minor and major path integration errors.},
author = {Milford, M. J. and Wyeth, G. F. and Prasser, D.},
doi = {10.1109/ROBOT.2004.1307183},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Milford, Wyeth, Prasser - 2004 - RatSLAM A Hippocampal Model for Simultaneous Localization and Mapping.pdf:pdf},
isbn = {0780382323},
issn = {1050-4729},
journal = {Proceeding of the 2004 IEEE international Conference on Robotics {\&} Automation},
keywords = {SLAM,biology,hippocampus,mobile robof,slam},
mendeley-tags = {biology,slam},
pages = {403--408},
title = {{RatSLAM: A Hippocampal Model for Simultaneous Localization and Mapping}},
url = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp={\&}arnumber=1307183{\&}isnumber=29020},
year = {2004}
}
@article{Zhang2017a,
abstract = {We propose split-brain autoencoders, a straightforward modification of the traditional autoencoder architecture, for unsupervised representation learning. The method adds a split to the network, resulting in two disjoint sub-networks. Each sub-network is trained to perform a difficult task -- predicting one subset of the data channels from another. Together, the sub-networks extract features from the entire input signal. By forcing the network to solve cross-channel prediction tasks, we induce a representation within the network which transfers well to other, unseen tasks. This method achieves state-of-the-art performance on several large-scale transfer learning benchmarks.},
archivePrefix = {arXiv},
arxivId = {1611.09842},
author = {Zhang, Richard and Isola, Phillip and Efros, Alexei A.},
doi = {10.1109/CVPR.2017.76},
eprint = {1611.09842},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Zhang, Isola, Efros - 2017 - Split-brain autoencoders Unsupervised learning by cross-channel prediction.pdf:pdf},
isbn = {9781538604571},
journal = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
pages = {645--654},
title = {{Split-brain autoencoders: Unsupervised learning by cross-channel prediction}},
volume = {2017-Janua},
year = {2017}
}
@article{Mingueza,
author = {Minguez, Javier},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Minguez - Unknown - The Obstacle-Restriction Method (ORM) for Robot Obstacle Avoidance in Difficult Environments.pdf:pdf},
title = {{The Obstacle-Restriction Method (ORM) for Robot Obstacle Avoidance in Difficult Environments}}
}
@article{Melorose2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Gonzalez-Banos, Hector H. and Latombe, J.-C.},
doi = {10.1177/0278364902021010834},
eprint = {arXiv:1011.1669v3},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Gonzalez-Banos, Latombe - 2002 - Navigation Strategies for Exploring Indoor Environments.pdf:pdf},
isbn = {9788578110796},
issn = {0278-3649},
journal = {The International Journal of Robotics Research},
keywords = {exploration,icle,slam},
mendeley-tags = {exploration,slam},
month = {oct},
number = {10-11},
pages = {829--848},
pmid = {25246403},
title = {{Navigation Strategies for Exploring Indoor Environments}},
url = {http://ijr.sagepub.com/cgi/doi/10.1177/0278364902021010834},
volume = {21},
year = {2002}
}
@article{Frintrop2006,
abstract = {In this paper, we introduce a new method to automatically detect useful landmarks for visual SLAM. A biologically motivated attention system detects regions of interest which "pop-out" automatically due to strong contrasts and the uniqueness of features. This property makes the regions easily redetectable and thus they are useful candidates for visual landmarks. Matching based on scene prediction and feature similarity allows not only short-term tracking of the regions, but also redetection in loop closing situations. The paper demonstrates how regions are determined and how they are matched reliably. Various experimental results on real-world data show that the landmarks are useful with respect to be tracked in consecutive frames and to enable closing loops},
author = {Frintrop, Simone and Jensfelt, Patric and Christensen, Henrik I.},
doi = {10.1109/IROS.2006.281711},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Frintrop, Jensfelt, Christensen - 2006 - Attentional landmark selection for visual SLAM.pdf:pdf},
isbn = {142440259X},
issn = {1077-3142},
journal = {IEEE International Conference on Intelligent Robots and Systems},
keywords = {slam},
mendeley-tags = {slam},
pages = {2582--2587},
title = {{Attentional landmark selection for visual SLAM}},
year = {2006}
}
@article{Saripalli2005,
abstract = {We present the design and implementation of a real-time vision-based approach to detect and track features in a structured environment using an autonomous helicopter. Using vision as a sensor enables the helicopter to track features in an urban environment. We use vision for feature detection and a combination of vision and GPS for navigation and tracking. The vision algorithm sends high level velocity commands to the helicopter controller which is then able to command the helicopter to track them. We present results obtained from flight trials that demonstrate our algorithms for detection and tracking are applicable in real world scenarios by applying them to the task of tracking rectangular features in structured environments.},
author = {Saripalli, S. and Sukhatme, G.S. and Mejias, L.O. and Cervera, P.C.},
doi = {10.1109/ROBOT.2005.1570728},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Saripalli et al. - 2005 - Detection and Tracking of External Features in an Urban Environment Using an Autonomous Helicopter.pdf:pdf},
isbn = {0-7803-8914-X},
journal = {Proceedings of the 2005 IEEE International Conference on Robotics and Automation},
keywords = {and automation,april 2005,barcelona,ceedings of the 2005,detection and tracking of,external features in an,ieee,international conference on robotics,spain},
number = {April},
pages = {3972--3977},
title = {{Detection and Tracking of External Features in an Urban Environment Using an Autonomous Helicopter}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1570728},
year = {2005}
}
@article{Hernandez-Juarez2016a,
abstract = {Dense, robust and real-time computation of depth information from stereo-camera systems is a computationally demanding requirement for robotics, advanced driver assistance systems (ADAS) and autonomous vehicles. Semi-Global Matching (SGM) is a widely used algorithm that propagates consistency constraints along several paths across the image. This work presents a real-time system producing reliable disparity estimation results on the new embedded energye cient GPU devices. Our design runs on a Tegra X1 at 42 frames per second (fps) for an image size of 640 480, 128 disparity levels, and using 4 path directions for the SGM method.},
archivePrefix = {arXiv},
arxivId = {1610.04121},
author = {Hernandez-Juarez, D. and Chacon, A. and Espinosa, A. and Vazquez, D. and Moure, J. C. and Lopez, A. M.},
doi = {10.1016/j.procs.2016.05.305},
eprint = {1610.04121},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Hernandez-Juarez et al. - 2016 - Embedded real-time stereo estimation via Semi-Global Matching on the GPU.pdf:pdf},
issn = {18770509},
journal = {Procedia Computer Science},
keywords = {Autonomous driving,CUDA,Computer vision,Drive PX,Embedded systems,GPU,Semi-global matching,Stereo},
pages = {143--153},
title = {{Embedded real-time stereo estimation via Semi-Global Matching on the GPU}},
volume = {80},
year = {2016}
}
@article{Oleynikova2015,
abstract = {High speed, low latency obstacle avoidance is essential for enabling Micro Aerial Vehicles (MAVs) to function in cluttered and dynamic environments. While other systems exist that do high-level mapping and 3D path planning for obstacle avoidance, most of these systems require high-powered CPUs on-board or off-board control from a ground station. We present a novel entirely on-board approach, leveraging a light-weight low power stereo vision system on FPGA. Our approach runs at a frame rate of 60 frames a second on VGA- sized images and minimizes latency between image acquisition and performing reactive maneuvers, allowing MAVs to fly more safely and robustly in complex environments. We also suggest our system as a light-weight safety layer for systems undertak- ing more complex tasks, like mapping the environment. Finally, we show our algorithm implemented on a light- weight, very computationally constrained platform, and demon- strate obstacle avoidance in a variety of environments.},
annote = {Focus op obstacle segmentation mbv U-map. Vrij eenvoudige oplossing voor obstacle avoidance. Gebruikt een tijdelijke map met obstakels.},
author = {Oleynikova, Helen and Honegger, Dominik and Pollefeys, Marc},
doi = {10.1109/ICRA.2015.7138979},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Oleynikova, Honegger, Pollefeys - 2015 - Reactive avoidance using embedded stereo vision for MAV flight.pdf:pdf},
isbn = {978-1-4799-6923-4},
journal = {IEEE International Conference on Robotics and Automation},
keywords = {3D path planning,CPU,Collision avoidance,FPGA,Field programmable gate arrays,MAV flight,Mobile communication,Navigation,Optical sensors,Robots,Stereo vision,VGA-sized images,cartesian map,cluttered environments,collision avoidance,complex environments,deliberate obstacle avoidance,dynamic environments,embedded stereo vision,field programmable gate arrays,ground station,high-level mapping,highlight,image acquisition,light-weight low power stereo vision system,light-weight safety layer,low complexity,micro aerial vehicles,nieuwe referenties,obstacle avoidance,obstacle detection,off-board control,on-board control,reactive avoidance,reactive maneuvers,robot vision,scenario: forest,space vehicles,stereo image processing,stereo vision,uv disparity,visual perception},
mendeley-tags = {cartesian map,deliberate obstacle avoidance,highlight,low complexity,nieuwe referenties,obstacle avoidance,obstacle detection,scenario: forest,stereo vision,uv disparity},
pages = {50--56},
title = {{Reactive avoidance using embedded stereo vision for MAV flight}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7138979},
year = {2015}
}
@article{Moller2006a,
annote = {NULL},
author = {M{\"{o}}ller, Ralf and Vardy, Andrew and Kreft, Sven and Ruwisch, Sebastian},
doi = {10.1.1.70.7510},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/M{\"{o}}ller et al. - 2006 - Newton-based matched-filter descent in image distances.pdf:pdf},
journal = {Biological Cybernetics (Submitted)},
title = {{Newton-based matched-filter descent in image distances}},
year = {2006}
}
@article{Neumann1997,
author = {Neumann, Titus R and Huber, Susanne A and B{\"{u}}lthoff, Heinrich H},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Neumann, Huber, B{\"{u}}lthoff - 1997 - Minimalistic Approach to 3D Obstacle Avoidance Behavior from Simulated Evolution.pdf:pdf},
journal = {ICANN '97 Proceedings of the 7th International Conference on Artificial Neural Networks},
keywords = {evolutionary robotics,low complexity,obstacle avoidance,reactive obstacle avoidance,scenario: corridor,simulation},
mendeley-tags = {evolutionary robotics,low complexity,obstacle avoidance,reactive obstacle avoidance,scenario: corridor,simulation},
pages = {715--720},
title = {{Minimalistic Approach to 3D Obstacle Avoidance Behavior from Simulated Evolution}},
year = {1997}
}
@article{Hrabar2008,
abstract = {We present a synthesis of techniques for rotorcraft UAV navigation through unknown environments which may contain obstacles. D* Lite and probabilistic roadmaps are combined for path planning, together with stereo vision for obstacle detection and dynamic path updating. A 3D occupancy map is used to represent the environment, and is updated online using stereo data. The target application is autonomous helicopter-based structure inspections, which require the UAV to fly safely close to the structures it is inspecting. Results are presented from simulation and with real flight hardware mounted onboard a cable array robot, demonstrating successful navigation through unknown environments containing obstacles.},
author = {Hrabar, Stefan},
doi = {10.1109/IROS.2008.4650775},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Hrabar - 2008 - 3D path planning and stereo-based obstacle avoidance for rotorcraft UAVs.pdf:pdf},
isbn = {9781424420582},
issn = {978-1-4244-2057-5},
journal = {2008 IEEE/RSJ International Conference on Intelligent Robots and Systems, IROS},
keywords = {Autonomous helicopter,Obstacle detection,Path planning,Power line inspection,Stereo vision,UAV,cartesian map,deliberate obstacle avoidance,obstacle avoidance,obstacle detection,simulation,stereo vision,voxel map},
mendeley-tags = {cartesian map,deliberate obstacle avoidance,obstacle avoidance,obstacle detection,simulation,stereo vision,voxel map},
pages = {807--814},
title = {{3D path planning and stereo-based obstacle avoidance for rotorcraft UAVs}},
year = {2008}
}
@article{Matsumoto2003,
author = {Matsumoto, Yoshio and Inaba, Masayuki and Inoue, Hirochika},
doi = {10.1007/s00138-002-0104-z},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Matsumoto, Inaba, Inoue - 2003 - View-based navigation using an omniview sequence in a corridor environment.pdf:pdf},
issn = {09328092},
journal = {Machine Vision and Applications},
keywords = {Corridor environment,Map,Omnidirectional vision,Robot navigation,View-based approach},
number = {2},
pages = {121--128},
title = {{View-based navigation using an omniview sequence in a corridor environment}},
volume = {14},
year = {2003}
}
@article{Tapus2005,
abstract = {Even today, robot mapping is one of the biggest challenges in mobile robotics. Geometric or topological maps can be used by a robot to navigate in the environment. Automatic creation of such maps is still problematic if the robot tries to map large environments. This paper presents a new method for incremental mapping using fingerprints of places. This type of representation permits a reliable, compact, and distinctive environment-modeling and makes navigation and localization easier for the robot. Experimental results for incremental mapping using a mobile robot equipped with a multi-sensor system composed of two 180{\&}deg; laser range finders and an omni-directional camera are also reported.},
author = {Tapus, Adriana and Siegwart, Roland},
doi = {10.1109/IROS.2005.1544977},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Tapus, Siegwart - 2005 - Incremental robot mapping with fingerprints of places.pdf:pdf},
isbn = {0780389123},
journal = {2005 IEEE/RSJ International Conference on Intelligent Robots and Systems, IROS},
keywords = {Cognitive representation,Fingerprint of places,Mapping,highlight},
mendeley-tags = {highlight},
pages = {172--177},
title = {{Incremental robot mapping with fingerprints of places}},
year = {2005}
}
@article{Franz1998,
author = {Franz, Matthias O. and Sch{\"{o}}lkopf, Bernhard and Mallot, Hanspeter A. and B{\"{u}}lthoff, Heinrich H.},
doi = {10.1007/s004220050470},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Franz et al. - 1998 - Where did I take that snapshot Scene-based homing by image matching.pdf:pdf},
journal = {Biological Cybernetics},
number = {3},
pages = {191--202},
title = {{Where did I take that snapshot? Scene-based homing by image matching}},
volume = {79},
year = {1998}
}
@article{Chahl1997,
abstract = {A family of reflective surfaces is presented that, when imaged by a camera, can capture a global view of the visual environment. By using these surfaces in conjunction with conventional imaging devices, it is possible to produce fields of view in excess of 180 degrees that are not affected by the distortions and aberrations found in refractive wide-angle imaging devices. By solving a differential equation expressing the camera viewing angle as a function of the angle of incidence on a reflective surface, a family of appropriate surfaces has been derived. The surfaces preserve a linear relationship between the angle of incidence of light onto the surface and the angle of reflection onto the imaging device, as does a normal mirror. However, the gradient of this linear relationship can be varied as desired to produce a larger or smaller field of view. The resulting family of surfaces has a number of applications in surveillance and machine vision.},
author = {Chahl, J S and Srinivasan, M V},
doi = {10.1364/AO.38.001196},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Chahl, Srinivasan - 1997 - Reflective surfaces for panoramic imaging.pdf:pdf},
issn = {0003-6935},
journal = {Applied optics},
number = {31},
pages = {8275--85},
pmid = {18264368},
title = {{Reflective surfaces for panoramic imaging.}},
volume = {36},
year = {1997}
}
@article{Calonder2010,
author = {Calonder, M. and Lepetit, V. and Strecha, C. and Fua, P.},
doi = {10.1007/978-3-642-15561-1_56},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Calonder et al. - 2010 - BRIEF Binary Robust Independent Elementary Features.pdf:pdf},
isbn = {3-642-15560-X, 978-3-642-15560-4},
issn = {978-3-642-15560-4},
journal = {European Conference on Computer Vision (ECCV)},
pages = {778--792},
pmid = {19500939},
title = {{BRIEF : Binary Robust Independent Elementary Features}},
year = {2010}
}
@article{VanHecke2016,
abstract = {Self-Supervised Learning (SSL) is a reliable learning mechanism in which a robot uses an original, trusted sensor cue for training to recognize an additional, complementary sensor cue. We study for the first time in SSL how a robot's learning behavior should be organized, so that the robot can keep performing its task in the case that the original cue becomes unavailable. We study this persistent form of SSL in the context of a flying robot that has to avoid obstacles based on distance estimates from the visual cue of stereo vision. Over time it will learn to also estimate distances based on monocular appearance cues. A strategy is introduced that has the robot switch from stereo vision based flight to monocular flight, with stereo vision purely used as 'training wheels' to avoid imminent collisions. This strategy is shown to be an effective approach to the 'feedback-induced data bias' problem as also experienced in learning from demonstration. Both simulations and real-world experiments with a stereo vision equipped AR drone 2.0 show the feasibility of this approach, with the robot successfully using monocular vision to avoid obstacles in a 5 x 5 room. The experiments show the potential of persistent SSL as a robust learning approach to enhance the capabilities of robots. Moreover, the abundant training data coming from the own sensors allows to gather large data sets necessary for deep learning approaches.},
archivePrefix = {arXiv},
arxivId = {1603.08047},
author = {van Hecke, Kevin and de Croon, Guido and van der Maaten, Laurens and Hennes, Daniel and Izzo, Dario},
eprint = {1603.08047},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/van Hecke et al. - 2016 - Persistent self-supervised learning principle from stereo to monocular vision for obstacle avoidance.pdf:pdf},
keywords = {TU Delft,monoc-,obstacle detection,persistent self-supervised learning,robotics,stereo vision,ular depth estimation},
mendeley-tags = {TU Delft,obstacle detection,stereo vision},
pages = {1--17},
title = {{Persistent self-supervised learning principle: from stereo to monocular vision for obstacle avoidance}},
url = {http://arxiv.org/abs/1603.08047},
year = {2016}
}
@article{Labrosse2007a,
abstract = {In this paper, we present a method that uses panoramic images to perform longrange navigation as a succession of shortrange homing steps along a route specified by appearances of the environment of the robot along the route. Our method is different from others in that it does not extract any features from the images and only performs simple image processing operations. The method does only make weak assumptions about the surroundings of the robot, assumptions that are discussed. Furthermore, the method uses a technique borrowed from computer graphics to simulate the effect in the images of short translations of the robot to compute local motion parameters. Finally, the proposed method shows that it is possible to perform navigation without explicitly knowing where the destination is nor where the robot currently is. Results in our Lab are presented that show the performance of the proposed system.},
author = {Labrosse, Fr{\'{e}}d{\'{e}}ric},
doi = {10.1016/j.robot.2007.05.004},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Labrosse - 2007 - Short and long-range visual navigation using warped panoramic images.pdf:pdf},
journal = {Robotics and Autonomous Systems},
pages = {675684},
title = {{Short and longrange visual navigation using warped panoramic images}},
volume = {55},
year = {2007}
}
@article{Troiani2015,
abstract = {This paper presents low computational-complexity methods for micro-aerial-vehicle localization in GPS-denied environments. All the presented algorithms rely only on the data provided by a single onboard camera and an Inertial Measurement Unit (IMU). This paper deals with outlier rejection and relative-pose estimation. Regarding outlier rejection, we describe two methods. The former only requires the observation of a single feature in the scene and the knowledge of the angular rates from an IMU, under the assumption that the local camera motion lies in a plane perpendicular to the gravity vector. The latter requires the observation of at least two features, but it relaxes the hypothesis on the vehicle motion, being therefore suitable to tackle the outlier detection problem in the case of a 6DoF motion. We show also that if the camera is rigidly attached to the vehicle, motion priors from the IMU can be exploited to discard wrong estimations in the framework of a 2-point-RANSAC-based approach. Thanks to their inherent efficiency, the proposed methods are very suitable for resource-constrained systems. Regarding the pose estimation problem, we introduce a simple algorithm that computes the vehicle pose from the observation of three point features in a single camera image, once that the roll and pitch angles are estimated from IMU measurements. The proposed algorithm is based on the minimization of a cost function. The proposed method is very simple in terms of computational cost and, therefore, very suitable for real-time implementation. All the proposed methods are evaluated on both synthetic and real data.},
author = {Troiani, Chiara and Martinelli, Agostino and Laugier, Christian and Scaramuzza, Davide},
doi = {10.1016/j.robot.2014.08.006},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Troiani et al. - 2015 - Low computational-complexity algorithms for vision-aided inertial navigation of micro aerial vehicles.pdf:pdf},
isbn = {0921-8890},
issn = {09218890},
journal = {Robotics and Autonomous Systems},
keywords = {Camera pose estimation,GPS-denied navigation,Micro aerial vehicle,Outlier detection,Quadrotor,Structure from motion,Vision-aided inertial navigation},
pages = {80--97},
title = {{Low computational-complexity algorithms for vision-aided inertial navigation of micro aerial vehicles}},
url = {http://dx.doi.org/10.1016/j.robot.2014.08.006},
volume = {69},
year = {2015}
}
@article{Xu2013a,
abstract = {This paper presents a novel real-time path planning algorithm, called InsertBug, for an autonomous mobile agent in completely unknown environment. By using the algorithm, all the planned paths are described and stored by vectors. The algorithm combines range sensor data with safety radius to determine the blocking obstacles and calculate the shortest path by choosing the intermediate points. When there is obstacle blocking the current path, the intermediate points will be calculated, and the planned path will be regenerated by inserting the intermediate points. The local optimum avoidance strategy is also considered in this algorithm by specifying a fixed direction. The agent will return to the optimal direction after running out of the local optimum. Different simulation parameters are taken to show the advantages of this algorithm. Moreover, the performance of this algorithm has also been evaluated by comparing with another usual method via simulations. The results show that the safety performance and time requirements of this algorithm are significant superior to the algorithm contrasted with.},
author = {Xu, Qi-Lei and Tang, Gong-You},
doi = {10.1007/s00521-012-1163-3},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Xu, Tang - 2013 - Vectorization path planning for autonomous mobile agent in unknown environment.pdf:pdf},
issn = {0941-0643},
journal = {Neural Computing and Applications},
keywords = {Bug algorithm,Path planning,Unknown environment,Vectorization path},
month = {dec},
number = {7-8},
pages = {2129--2135},
title = {{Vectorization path planning for autonomous mobile agent in unknown environment}},
url = {http://link.springer.com/10.1007/s00521-012-1163-3},
volume = {23},
year = {2013}
}
@article{Simonyan,
abstract = {—The objective of this work is to learn descriptors suitable for the sparse feature detectors used in viewpoint invariant matching. We make a number of novel contributions towards this goal. First, it is shown that learning the pooling regions for the descriptor can be formulated as a convex optimisation problem selecting the regions using sparsity. Second, it is shown that descriptor dimensionality reduction can also be formulated as a convex optimisation problem, using Mahalanobis matrix nuclear norm regularisation. Both formulations are based on discriminative large margin learning constraints. As the third contribution, we evaluate the performance of the compressed descriptors, obtained from the learnt realvalued descriptors by binarisation. Finally, we propose an extension of our learning formulations to a weakly supervised case, which allows us to learn the descriptors from unannotated image collections. It is demonstrated that the new learning methods improve over the state of the art in descriptor learning on the annotated local patches dataset of Brown et al. [3] and unannotated photo collections of Philbin et al. [22].},
author = {Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew},
title = {{Learning Local Feature Descriptors Using Convex Optimisation}}
}
@inproceedings{Allen2015,
abstract = {— In this paper we propose a framework combining techniques from sampling-based motion planning, machine learning, and trajectory optimization to address the kinody-namic motion planning problem in real-time environments. This framework relies on a look-up table that stores precomputed optimal solutions to boundary value problems (assuming no obstacles), which form the directed edges of a precomputed motion planning roadmap. A sampling-based motion planning algorithm then leverages such a precomputed roadmap to compute online an obstacle-free trajectory. Machine learning techniques are employed to minimize the number of online solutions to boundary value problems required to compute the neighborhoods of the start state and goal regions. This approach is demonstrated to reduce online planning times up to six orders of magnitude. Simulation results are presented and discussed. Problem-specific framework modifications are then discussed that would allow further computation time reductions.},
annote = {Doet denken aan nonlinear model predictive control. Very general framework voor path optimization onder diverse constraints. Focus op zo veel mogelijk offline precomputation, en reduction mbv machine learning. Geen expliciete ondersteuning voor bewegende doelen.},
author = {Allen, Ross and Pavone, Marco},
booktitle = {2015 IEEE International Conference on Robotics and Automation (ICRA)},
doi = {10.1109/ICRA.2015.7139288},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Allen, Pavone - 2015 - Toward a real-time framework for solving the kinodynamic motion planning problem.pdf:pdf},
isbn = {978-1-4799-6923-4},
keywords = {delete,deliberate obstacle avoidance,motion planning,nieuwe referenties,obstacle avoidance},
mendeley-tags = {delete,deliberate obstacle avoidance,motion planning,nieuwe referenties,obstacle avoidance},
month = {may},
pages = {928--934},
publisher = {IEEE},
title = {{Toward a real-time framework for solving the kinodynamic motion planning problem}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7139288},
year = {2015}
}
@article{Romero2014,
abstract = {While depth tends to improve network performances, it also makes gradient-based training more difficult since deeper networks tend to be more non-linear. The recently proposed knowledge distillation approach is aimed at obtaining small and fast-to-execute models, and it has shown that a student network could imitate the soft output of a larger teacher network or ensemble of networks. In this paper, we extend this idea to allow the training of a student that is deeper and thinner than the teacher, using not only the outputs but also the intermediate representations learned by the teacher as hints to improve the training process and final performance of the student. Because the student intermediate hidden layer will generally be smaller than the teacher's intermediate hidden layer, additional parameters are introduced to map the student hidden layer to the prediction of the teacher hidden layer. This allows one to train deeper students that can generalize better or run faster, a trade-off that is controlled by the chosen student capacity. For example, on CIFAR-10, a deep student network with almost 10.4 times less parameters outperforms a larger, state-of-the-art teacher network.},
archivePrefix = {arXiv},
arxivId = {1412.6550},
author = {Romero, Adriana and Ballas, Nicolas and Kahou, Samira Ebrahimi and Chassang, Antoine and Gatta, Carlo and Bengio, Yoshua},
doi = {10.1145/1390156.1390294},
eprint = {1412.6550},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Romero et al. - 2014 - FitNets Hints for Thin Deep Nets.pdf:pdf},
isbn = {9781605582054},
issn = {1605582050},
pages = {1--13},
pmid = {15540460},
title = {{FitNets: Hints for Thin Deep Nets}},
url = {http://arxiv.org/abs/1412.6550},
year = {2014}
}
@phdthesis{Call2006a,
author = {Call, Brandon R.},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Call - 2006 - Obstacle Avoidance for Unmanned Air Vehicles.pdf:pdf},
number = {December},
school = {Brigham Young University},
title = {{Obstacle Avoidance for Unmanned Air Vehicles}},
type = {MSc},
year = {2006}
}
@article{Pantilie2010,
abstract = {Mobile robots as well as tomorrows intelligent vehicles acting in complex dynamic environments must be able to detect both static and moving obstacles. In intersections or crowded urban areas this task proves to be highly demanding. Stereo vision has been extensively used for this task, as it provides a large amount of data. Since it does not reveal any motion information, static and dynamic objects immediately next to each other, or closely positioned obstacles moving in different directions are often merged into a single obstacle. In this paper we address these problems through a powerful fusion between 3D position information delivered by the stereo sensor and 3D motion information, derived from optical flow, in a depth-adaptive occupancy grid. The proposed model is presented and then applied for determining obstacle localization, orientation and speed.},
author = {Pantilie, C.D. and Nedevschi, S.},
doi = {10.1109/ITSC.2010.5625174},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Pantilie, Nedevschi - 2010 - Real-time obstacle detection in complex scenarios using dense stereo vision and optical flow.pdf:pdf},
isbn = {978-1-4244-7657-2},
journal = {3th International IEEE Conference on Intelligent Transportation Systems (ITSC)},
keywords = {3D motion information,3D position information,collision avoidance,dense stereo vision,depth-adaptive occupancy grid,experiment,intelligent vehicle,mobile robots,moving obstacle,moving obstacles,nieuwe referenties,obstacle detection,obstacle localization,optical flow,polar map,real-time obstacle detection,real-time systems,robot vision,stereo image processing,stereo sensor,stereo vision},
mendeley-tags = {experiment,moving obstacles,nieuwe referenties,obstacle detection,optical flow,polar map,stereo vision},
pages = {439--444},
title = {{Real-time obstacle detection in complex scenarios using dense stereo vision and optical flow}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=5625174},
year = {2010}
}
@article{Pronobis2010,
abstract = {The capability to learn from experience is a key property for autonomous cognitive systems working in realistic settings. To this end, this paper presents an SVM-based algorithm, capable of learning model representations incrementally while keeping under control memory requirements. We combine an incremental extension of SVMs [43] with a method reducing the number of support vectors needed to build the decision function without any loss in performance [15] introducing a parameter which permits a user-set trade-off between performance and memory. The resulting algorithm is able to achieve the same recognition results as the original incremental method while reducing the memory growth. Our method is especially suited to work for autonomous systems in realistic settings. We present experiments on two common scenarios in this domain: adaptation in presence of dynamic changes and transfer of knowledge between two different autonomous agents, focusing in both cases on the problem of visual place recognition applied to mobile robot topological localization. Experiments in both scenarios clearly show the power of our approach. ?? 2010 Elsevier B.V. All rights reserved.},
author = {Pronobis, Andrzej and Jie, Luo and Caputo, Barbara},
doi = {10.1016/j.imavis.2010.01.015},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Pronobis, Jie, Caputo - 2010 - The more you learn, the less you store Memory-controlled incremental SVM for visual place recognition.pdf:pdf},
isbn = {02628856},
issn = {02628856},
journal = {Image and Vision Computing},
keywords = {Incremental learning,Knowledge transfer,Place recognition,Support vector machines,Visual robot localization,low complexity},
mendeley-tags = {low complexity},
number = {7},
pages = {1080--1097},
publisher = {Elsevier B.V.},
title = {{The more you learn, the less you store: Memory-controlled incremental SVM for visual place recognition}},
url = {http://dx.doi.org/10.1016/j.imavis.2010.01.015},
volume = {28},
year = {2010}
}
@article{Minguez2016,
author = {Minguez, Javier and Lamiraux, Florant and Laumond, Jean-Paul},
doi = {10.1007/978-3-319-32552-1_47},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Minguez, Lamiraux, Laumond - 2016 - Motion Planning and Obstacle Avoidance.pdf:pdf},
journal = {Springer Handbook of Robotics},
pages = {1177--1202},
title = {{Motion Planning and Obstacle Avoidance}},
url = {http://link.springer.com/10.1007/978-3-319-32552-1{\_}47},
year = {2016}
}
@article{Chaumette2007,
abstract = {This article is the second of a two-part tutorial on visual servo control. In this tutorial, we have only considered velocity controllers. It is convenient for most of classical robot arms. However, the dynamics of the robot must of course be taken into account for high speed task, or when we deal with mobile nonholonomic or underactuated robots. As for the sensor, geometrical features coming from a classical perspective camera is considered. Features related to the image motion or coming from other vision sensors necessitate to revisit the modeling issues to select adequate visual features. Finally, fusing visual features with data coming from other sensors at the level of the control scheme will allow to address new research topics},
author = {Chaumette, Fran{\c{c}}ois and Hutchinson, Seth},
doi = {10.1109/MRA.2007.339609},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Chaumette, Hutchinson - 2007 - Visual servo control. II. Advanced approaches Tutorial.pdf:pdf},
isbn = {1070-9932},
issn = {10709932},
journal = {IEEE Robotics and Automation Magazine},
keywords = {Robot control,Visual servo,visual homing,visual servoing},
mendeley-tags = {visual homing,visual servoing},
number = {1},
pages = {109--118},
title = {{Visual servo control. II. Advanced approaches [Tutorial]}},
volume = {14},
year = {2007}
}
@article{Kuipers2000,
abstract = {The Spatial Semantic Hierarchy (SSH) is a model of knowledge of large-scale space consisting of multiple interacting representations, both qualitative and quantitative. It is inspired by the properties of the human cognitive map, and is intended to serve both as a model of the human cognitive map and as a method for robot exploration and map-building. This article describes the assumptions and guarantees behind the generality of the SSH across environments and sensorimotor systems. Evidence is given from several partial implementation of the SSH on simulated and physical robots.},
author = {Kuipers, Benjamin},
doi = {10.1016/S0004-3702(00)00017-5},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Kuipers - 2000 - Spatial semantic hierarchy.pdf:pdf},
isbn = {00043702},
issn = {00043702},
journal = {Artificial Intelligence},
keywords = {9-898,advanced research program,and by the texas,artificial intelligence laboratory,austin,by nasa grant nag,cognitive map,grants,in part by nsf,iri-9504138 and cda 9617327,map learning,navigation strategy,place in the qualitative,qualitative reasoning,reasoning group at the,reasoning group is supported,research of the qualitative,robot exploration,spatial reasoning,the,this work has taken,university of texas at},
mendeley-tags = {navigation strategy},
number = {1},
pages = {191--233},
title = {{Spatial semantic hierarchy}},
volume = {119},
year = {2000}
}
@article{Brockers2012,
abstract = {Direct-lift micro air vehicles have important applications in reconnaissance. In order to conduct persistent surveillance in urban environments, it is essential that these systems can perform autonomous landing maneuvers on elevated surfaces that provide high vantage points without the help of any external sensor and with a fully contained on-board software solution. In this paper, we present a micro air vehicle that uses vision feedback from a single down looking camera to navigate autonomously and detect an elevated landing platform as a surrogate for a roof top. Our method requires no special preparation (labels or markers) of the landing location. Rather, leveraging the planar character of urban structure, the landing platform detection system uses a planar homography decomposition to detect landing targets and produce approach waypoints for autonomous landing. The vehicle control algorithm uses a Kalman filter based approach for pose estimation to fuse visual SLAM (PTAM) position estimates with IMU data to correct for high latency SLAM inputs and to increase the position estimate update rate in order to improve control stability. Scale recovery is achieved using inputs from a sonar altimeter. In experimental runs, we demonstrate a real-time implementation running on-board a micro aerial vehicle that is fully self-contained and independent from any external sensor information. With this method, the vehicle is able to search autonomously for a landing location and perform precision landing maneuvers on the detected targets. � 2012 Copyright Society of Photo-Optical Instrumentation Engineers (SPIE).},
author = {Brockers, Roland and Susca, Sara and Zhu, David and Matthies, Larry},
doi = {10.1117/12.919278},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Brockers et al. - 2012 - Fully self-contained vision-aided navigation and landing of a micro air vehicle independent from external senso.pdf:pdf},
isbn = {9780819490650},
issn = {0277786X},
journal = {SPIE Defense, Security, and Sensing},
pages = {83870Q--83870Q--10},
title = {{Fully self-contained vision-aided navigation and landing of a micro air vehicle independent from external sensor inputs}},
volume = {8387},
year = {2012}
}
@book{Alpen2013,
author = {Alpen, Mirco and Frick, Klaus and Horn, Joachim},
booktitle = {IFAC Proceedings Volumes},
doi = {10.3182/20130626-3-AU-2035.00016},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Alpen, Frick, Horn - 2013 - An Autonomous Indoor UAV with a Real-Time On-Board Orthogonal SLAM.pdf:pdf},
isbn = {9783902823366},
issn = {14746670},
keywords = {Orthogonal SLAM,autonomous flight,highlight,indoor UAV,indoor uav,orthogonal slam,slam},
mendeley-tags = {highlight,slam},
number = {10},
pages = {268--273},
publisher = {IFAC},
title = {{An Autonomous Indoor UAV with a Real-Time On-Board Orthogonal SLAM}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S1474667015349430},
volume = {46},
year = {2013}
}
@book{DeCroon2016,
author = {de Croon, G.C.H.E. and Per{\c{c}}in, M. and Remes, B.D.W. D W and Ruijsink, R. and {De Wagter}, C.},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/de Croon et al. - 2016 - The DelFly.pdf:pdf},
isbn = {9789401792073},
keywords = {TU Delft,obstacle avoidance,obstacle detection,optical flow,stereo vision},
mendeley-tags = {TU Delft,obstacle avoidance,obstacle detection,optical flow,stereo vision},
pages = {221},
title = {{The DelFly}},
year = {2016}
}
@article{Square1996,
author = {Square, Queen},
doi = {10.1007/BF00337264},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Square - 1996 - Biological Cybernetics.pdf:pdf},
isbn = {1432-0770},
issn = {0340-1200},
keywords = {optical flow},
mendeley-tags = {optical flow},
pages = {543--547},
pmid = {1000111957},
title = {{Biological Cybernetics}},
volume = {547},
year = {1996}
}
@inproceedings{Letouzey2011,
address = {Dundee, United Kingdom},
author = {Letouzey, Antoine and Petit, Benjamin and Boyer, Edmond},
booktitle = {BMVC 2011 - British Machine Vision Conference},
doi = {10.5244/C.25.46},
editor = {Hoey, Jesse and McKenna, Stephen and Trucco, Emanuele},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Letouzey, Petit, Boyer - 2011 - Scene Flow from Depth and Color Images.pdf:pdf},
month = {aug},
pages = {46:1--11},
publisher = {BMVA Press},
series = {Proceedings of the British Machine Vision Conference},
title = {{Scene Flow from Depth and Color Images}},
url = {https://hal.inria.fr/inria-00616353},
year = {2011}
}
@incollection{Fragoso2017,
abstract = {Onboard obstacle avoidance is a challenging, yet indespensible component of micro air vehicle (MAV) autonomy. Prior approaches for deliberative motion planning over vehicle dynamics typically rely on 3-D voxel-based world models, which require complex access schemes or extensive memory to manage resolution and maintain an acceptable motion-planning horizon. In this paper, we present a novel, lightweight motion planning method, for micro air vehicles with full configuration flat dynamics, based on perception with stereo vision and a 2.5-D egocylinder obstacle representation. We equip the egocylinder with temporal fusion to enhance obstacle detection and provide a rich, 360 {\$}{\$}{\^{}}{\{}$\backslash$circ {\}}{\$}{\$} representation of the environment well beyond the visible field-of-regard of a stereo camera pair. The natural pixel parameterization of the egocylinder is used to quickly identify dynamically feasible maneuvers onto radial paths, expressed directly in egocylinder coordinates, that enable finely detailed planning at extreme ranges within milliseconds. We have implemented our obstacle avoidance pipeline with an Asctec Pelican quadcopter, and demonstrate the efficiency of our approach experimentally with a set of challenging field scenarios.},
author = {Fragoso, Anthony T. and Cigla, Cevahir and Brockers, Roland and Matthies, Larry H.},
doi = {10.1007/978-3-319-67361-5_28},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Fragoso et al. - 2018 - Dynamically Feasible Motion Planning for Micro Air Vehicles Using an Egocylinder.pdf:pdf},
number = {July},
pages = {433--447},
title = {{Dynamically Feasible Motion Planning for Micro Air Vehicles Using an Egocylinder}},
url = {http://link.springer.com/10.1007/978-3-319-67361-5{\_}28},
year = {2018}
}
@article{Clark,
archivePrefix = {arXiv},
arxivId = {arXiv:1701.08376v2},
author = {Clark, Ronald and Wang, Sen and Wen, Hongkai and Markham, Andrew and Trigoni, Niki},
eprint = {arXiv:1701.08376v2},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Clark et al. - Unknown - VINet Visual-Inertial Odometry as a Sequence-to-Sequence Learning Problem.pdf:pdf},
title = {{VINet: Visual-Inertial Odometry as a Sequence-to-Sequence Learning Problem}}
}
@article{Labrosse2007,
author = {Labrosse, Fr{\'{e}}d{\'{e}}ric},
doi = {10.1016/j.robot.2007.05.004},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Labrosse - 2007 - Short and long-range visual navigation using warped panoramic images.pdf:pdf},
journal = {Robotics and Autonomous Systems},
keywords = {appearance-based vision,mobile robotics,navigation,omni-directional},
number = {9},
pages = {675--684},
title = {{Short and long-range visual navigation using warped panoramic images}},
volume = {55},
year = {2007}
}
@inproceedings{Varma2003,
abstract = {We question the role that large scale filter banks have traditionally played in texture classification. It is demonstrated that textures can be classified using the joint distribution of intensity values over extremely compact neighbourhoods (starting from as small as 3 × 3 pixels square), and that this outperforms classification using filter banks with large support. We develop a novel texton based representation which is suited to modelling this joint neighbourhood distribution for MRFs. The representation is learnt from training images, and then used to classify novel images (with unknown viewpoint and lighting) into texture classes.},
author = {Varma, Manik and Zisserman, Andrew},
booktitle = {IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2003. Proceedings.},
doi = {10.1109/CVPR.2003.1211534},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Varma, Zisserman - 2003 - Texture classification are filter banks necessary.pdf:pdf},
isbn = {0-7695-1900-8},
pages = {II--691--8},
publisher = {IEEE Comput. Soc},
title = {{Texture classification: are filter banks necessary?}},
url = {http://ieeexplore.ieee.org/document/1211534/},
volume = {2},
year = {2003}
}
@inproceedings{Werlberger2009,
author = {Werlberger, Manuel and Trobin, Werner and Pock, Thomas and Wedel, Andreas and Cremers, Daniel and Bischof, Horst},
booktitle = {BMVC},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Werlberger et al. - 2009 - Anisotropic Huber-L 1 Optical Flow.pdf:pdf},
pages = {1--11},
title = {{Anisotropic Huber-L 1 Optical Flow}},
year = {2009}
}
@article{Dijkstra1959,
abstract = {We consider a points (nodes), some or all pairs of which are connected by a branch; the length of each branch is given. We restrict ourselves to the case where at least one path exists between any two nodes. We now consider two problems. Problem 1. Construct the tree of minimum total length between the n nodes. (A tree is a graph with one and only one path between every two nodes.) In the course ol the construction that we present here, the branches are subdivided into three sets: I. the branches definitely assigned to the tree under construction (they will form a subtree) ; IL the branches from which the next branch to be added to set I, will be selected; III. the rernaining branches (rejected or not yet considered)' The nodes are subdivided into two sets: A. the nodes connected by the branches of set I, B. the remaining nodes (one and only one branch of set II will lead to each of these nodes). We stalt the construction by choosing an arbitrary node as the only member of set A, and by placing all branches that end in this node in set II. To start with, set I is empty. From then onwalals we Pedorm the following two steps repeatedly. Step 1. The shortest branch of set II is removed from this set and added to set I. As a result one node is transferred from set B to set I . Slep 2. Consider the branches leading from the node, that has just been trans-ferred to set A, to the nodes that are still in set B. If the branch under con-sideration is longer than the corresponding branch in set II, it is rejected; it it is shorter, it replaces the corresponding branch in set II, and the latter is rejected. We then return to step I and repeat the process until sets II and B are empty. The branches in set I form the tree required, The solution given here is to be prelerred to the solution given by J. B. Knusxar [1] and those given by H. LoBERMAN and A. WrrNgancER [2]. In their solutions all the possibly ]z(z-l) -branches are first of all sorted according to length. Even if the length of the branches is a computable function of the node coordinates, their methods demand that data for al1 branches are stored simultaneously. Our method only requires the simultaneous storing of Nuoer. Math. Bd. I i9 270 E. .w. DTJKSTRA: the data for at most r branches, viz. the branches in sets I and II and the bmnch under consideration in step 2. Problem 2. Find the path of rninimum total lengtl between two given nodes P and Q. We use the fact that, if R is a node on the minimal path from P to Q, knowledge of the latter implies the knowledge of the minimal path from P to {\"{A}}. In the solution presented, the minimal paths from P to the other nodes are constructed in order of increasing length until Q is reached. Iu the cou{\$}e of the solution the nodes are subdivided into three sets: A. the nodes for wbLich the path o{\{} nrinimum lengti from P is known; nodes will be added to this set in order of increasing minimum path length from node P; B. the nodes from which the next node to be added to set A will be selected; this set comprises all those nodes that are connected to at least one node of set A but do not yet belong to A themselves; C. the remaining nodes. The branches are also subdivided into three sets: L the branches occurring in the minimal paths ftom node P to the lodes in set A; IL the branches from which the oext brancl to be placed in set I will be selected; one and only one branch of this set will lead to each node in set B; III. the remaining branches (rejected or not yet considered). To start with, all nodes are iu set C and all branches are in set IIL We now transfer node P to set A and from then onwards repeatedly perform the following steps. Slep 7. Consider all branches z connecting the node just transfeired to set A with nodes R in sets B or C. If node R belongs to set B, we investigate whether the use of branch z gives rise to a shorter path Jrom P to R than the known path that uses the corresponding branch in set II. If this is not so, branch r is rejected; if, however, use of branch z resu.lts in a shorter connexion between P and .R than hitherto obtaiaed, it replaces the corresponding branch in set II and the Latter is rejected. If the node.R belongs to set C, it is added to set B and branch r is added to set II.}},
author = {Dijkstra, E. W.},
doi = {10.1007/BF01386390},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Dijkstra - 1959 - A note on two problems in connexion with graphs.pdf:pdf},
isbn = {0029-599X (Print) 0945-3245 (Online)},
issn = {0029599X},
journal = {Numerische Mathematik},
number = {1},
pages = {269--271},
pmid = {18215627},
title = {{A note on two problems in connexion with graphs}},
volume = {1},
year = {1959}
}
@article{Cvisic2017,
author = {Cvi{\v{s}}i{\'{c}}, Igor and {\'{C}}esi{\'{c}}, Josip and Markovi{\'{c}}, Ivan and Petrovi{\'{c}}, Ivan},
doi = {10.1002/rob.21762},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Cvi{\v{s}}i{\'{c}} et al. - 2017 - SOFT-SLAM Computationally efficient stereo visual simultaneous localization and mapping for autonomous unmanned.pdf:pdf},
journal = {Journal of Field Robotics},
number = {August},
pages = {1--18},
title = {{SOFT-SLAM: Computationally efficient stereo visual simultaneous localization and mapping for autonomous unmanned aerial vehicles}},
year = {2017}
}
@article{Mikolajczyk2004,
author = {Mikolajczyk, Krystian and Schmid, Cordelia},
doi = {10.1023/B:VISI.0000027790.02288.f2},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Mikolajczyk, Schmid - 2004 - Scale {\&} Affine Invariant Interest Point Detectors.pdf:pdf},
journal = {International Journal of Computer Vision},
keywords = {affine invariance,interest points,local features,matching,recognition,scale invariance},
pages = {63--86},
title = {{Scale {\&} Affine Invariant Interest Point Detectors}},
volume = {60},
year = {2004}
}
@article{Rivlin1993,
author = {Kamon, Ishay and Rimon, Elon and Rivlin, Ehud},
doi = {10.1177/027836499801700903},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Kamon, Rimon, Rivlin - 1998 - TangentBug A Range-Sensor-Based Navigation Algorithm.pdf:pdf},
issn = {0278-3649},
journal = {The International Journal of Robotics Research},
month = {sep},
number = {9},
pages = {934--953},
title = {{TangentBug: A Range-Sensor-Based Navigation Algorithm}},
url = {http://journals.sagepub.com/doi/10.1177/027836499801700903},
volume = {17},
year = {1998}
}
@article{Cartwright1983,
author = {Cartwright, B A and Collett, T S},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Cartwright, Collett - 1983 - Landmark Learning in Bees Experiments and Models.pdf:pdf},
journal = {Journal of Comparative Physiology},
keywords = {biology,snapshot model},
mendeley-tags = {biology,snapshot model},
pages = {521--543},
title = {{Landmark Learning in Bees Experiments and Models}},
volume = {151},
year = {1983}
}
@inproceedings{Ross2013,
author = {Ross, St{\'{e}}phane and Melik-Barkhudarov, Narek and Shankar, Kumar Shaurya and Wendel, Andreas and Dey, Debadeepta and Bagnell, J. Andrew and Hebert, Martial},
booktitle = {International Conference on Robotics and Automation (ICRA)},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Ross et al. - 2013 - Learning Monocular Reactive UAV Control in Cluttered Natural Environments.pdf:pdf},
isbn = {9781467356435},
pages = {1765--1772},
publisher = {IEEE},
title = {{Learning Monocular Reactive UAV Control in Cluttered Natural Environments}},
year = {2013}
}
@article{Ke2004,
abstract = {Stable local feature detection and representation is a fundamental component of many image registration and object recognition algorithms. Mikolajczyk and Schmid (June 2003) recently evaluated a variety of approaches and identified the SIFT [D. G. Lowe, 1999] algorithm as being the most resistant to common image deformations. This paper examines (and improves upon) the local image descriptor used by SIFT. Like SIFT, our descriptors encode the salient aspects of the image gradient in the feature point's neighborhood; however, instead of using SIFT's smoothed weighted histograms, we apply principal components analysis (PCA) to the normalized gradient patch. Our experiments demonstrate that the PCA-based local descriptors are more distinctive, more robust to image deformations, and more compact than the standard SIFT representation. We also present results showing that using these descriptors in an image retrieval application results in increased accuracy and faster matching.},
author = {Ke, Yan and Sukthankar, R.},
doi = {10.1109/CVPR.2004.1315206},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Ke, Sukthankar - 2004 - PCA-SIFT a more distinctive representation for local image descriptors.pdf:pdf},
isbn = {0-7695-2158-4},
issn = {1063-6919},
journal = {Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2004. CVPR 2004.},
keywords = {PCA-SIFT},
mendeley-tags = {PCA-SIFT},
pages = {2--9},
title = {{PCA-SIFT: a more distinctive representation for local image descriptors}},
volume = {2},
year = {2004}
}
@article{Dupeyroux,
author = {Dupeyroux, Julien and Serres, Julien R and Viollet, St{\'{e}}phane},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Dupeyroux, Serres, Viollet - Unknown - AntBot A six-legged walking robot able to home like desert ants in outdoor environments.pdf:pdf},
journal = {Science Robotics},
pages = {1--13},
title = {{AntBot : A six-legged walking robot able to home like desert ants in outdoor environments}}
}
@article{Borenstein1991,
abstract = {A real-time obstacle avoidance method for mobile robots which has$\backslash$nbeen developed and implemented is described. This method, named the$\backslash$nvector field histogram (VFH), permits the detection of unknown obstacles$\backslash$nand avoids collisions while simultaneously steering the mobile robot$\backslash$ntoward the target. The VFH method uses a two-dimensional Cartesian$\backslash$nhistogram grid as a world model. This world model is updated$\backslash$ncontinuously with range data sampled by onboard range sensors. The VFH$\backslash$nmethod subsequently uses a two-stage data-reduction process to compute$\backslash$nthe desired control commands for the vehicle. Experimental results from$\backslash$na mobile robot traversing densely cluttered obstacle courses in smooth$\backslash$nand continuous motion and at an average speed of 0.6-0.7 m/s are shown.$\backslash$nA comparison of the VFN method to earlier methods is given},
author = {Borenstein, Johann and Koren, Yoram},
doi = {10.1109/70.88137},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Borenstein, Koren - 1991 - The Vector Field Histogram - Fast Obstacle Avoidance for Mobile Robots.pdf:pdf},
isbn = {1042-296X VO - 7},
issn = {1042296X},
journal = {IEEE Transactions on Robotics and Automation},
keywords = {cartesian map,low complexity,obstacle avoidance,polar map,reactive obstacle avoidance},
mendeley-tags = {cartesian map,low complexity,obstacle avoidance,polar map,reactive obstacle avoidance},
number = {3},
pages = {278--288},
pmid = {88137},
title = {{The Vector Field Histogram - Fast Obstacle Avoidance for Mobile Robots}},
volume = {7},
year = {1991}
}
@article{Yu2010a,
author = {Yu, Seung Eun and Kim, DaeEun},
doi = {10.1109/IROS.2010.5652366},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Yu, Kim - 2010 - Distance estimation method with snapshot landmark images in the robotic homing navigation.pdf:pdf},
isbn = {9781424466757},
journal = {IEEE/RSJ 2010 International Conference on Intelligent Robots and Systems, IROS 2010 - Conference Proceedings},
keywords = {Biologically-Inspired Robots,Localization,Navigation},
pages = {275--280},
title = {{Distance estimation method with snapshot landmark images in the robotic homing navigation}},
year = {2010}
}
@article{Glasius1995,
author = {Glasius, Roy and Komoda, Andrzej and Gielen, Stan C.a.M.},
doi = {10.1016/0893-6080(94)E0045-M},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Glasius, Komoda, Gielen - 1995 - Neural Network Dynamics for Path Planning and Obstacle Avoidance.pdf:pdf},
isbn = {0893-6080},
issn = {08936080},
journal = {Neural Networks},
keywords = {--attractor neural networks,1,a versatility of function,and economy of space,cartesian map,deliberate obstacle avoidance,dynamical systems,h u m a,i n t r,motion planning,moving obstacles,n m o t,o d u c,o r control reveals,obstacle avoidance,path planning,robotics,simulation,t i o n,that is yet beyond,the reach of,topological map,voxel map},
mendeley-tags = {cartesian map,deliberate obstacle avoidance,motion planning,moving obstacles,obstacle avoidance,simulation,voxel map},
pages = {125--133},
title = {{Neural Network Dynamics for Path Planning and Obstacle Avoidance}},
volume = {8},
year = {1995}
}
@article{Yin2018,
abstract = {We propose GeoNet, a jointly unsupervised learning framework for monocular depth, optical flow and ego-motion estimation from videos. The three components are coupled by the nature of 3D scene geometry, jointly learned by our framework in an end-to-end manner. Specifically, geometric relationships are extracted over the predictions of individual modules and then combined as an image reconstruction loss, reasoning about static and dynamic scene parts separately. Furthermore, we propose an adaptive geometric consistency loss to increase robustness towards outliers and non-Lambertian regions, which resolves occlusions and texture ambiguities effectively. Experimentation on the KITTI driving dataset reveals that our scheme achieves state-of-the-art results in all of the three tasks, performing better than previously unsupervised methods and comparably with supervised ones.},
archivePrefix = {arXiv},
arxivId = {1803.02276},
author = {Yin, Zhichao and Shi, Jianping},
eprint = {1803.02276},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Yin, Shi - 2018 - GeoNet Unsupervised Learning of Dense Depth, Optical Flow and Camera Pose.pdf:pdf},
title = {{GeoNet: Unsupervised Learning of Dense Depth, Optical Flow and Camera Pose}},
url = {http://arxiv.org/abs/1803.02276},
year = {2018}
}
@article{Moller2010,
author = {M{\"{o}}ller, Ralf and Krzykawski, Martin and Gerstmayr, Lorenz},
doi = {10.1007/s10514-010-9195-y},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/M{\"{o}}ller, Krzykawski, Gerstmayr - 2010 - Three 2D-warping schemes for visual robot navigation(2).pdf:pdf},
journal = {Autonomous Robots},
keywords = {image warping,navigation,visual homing},
number = {3},
pages = {253--291},
title = {{Three 2D-warping schemes for visual robot navigation}},
volume = {29},
year = {2010}
}
@article{Sunberg2016,
abstract = {Safely integrating unmanned aerial vehicles into civil airspace is contingent upon development of a trustworthy collision avoidance system. This paper proposes an approach whereby a parameterized resolution logic that is considered trusted for a given range of its parameters is adaptively tuned online. Specifically, to address the potential conservatism of the resolution logic with static parameters, we present a dynamic programming approach for adapting the parameters dynamically based on the encounter state. We compute the adaptation policy offline using a simulation-based approximate dynamic programming method that accommodates the high dimensionality of the problem. Numerical experiments show that this approach improves safety and operational performance compared to the baseline resolution logic, while retaining trustworthiness.},
archivePrefix = {arXiv},
arxivId = {1602.04762},
author = {Sunberg, Zachary N. and Kochenderfer, Mykel J. and Pavone, Marco},
eprint = {1602.04762},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Sunberg, Kochenderfer, Pavone - 2016 - Optimized and Trusted Collision Avoidance for Unmanned Aerial Vehicles using Approximate Dynamic.pdf:pdf},
keywords = {moving obstacles,obstacle avoidance},
mendeley-tags = {moving obstacles,obstacle avoidance},
month = {feb},
title = {{Optimized and Trusted Collision Avoidance for Unmanned Aerial Vehicles using Approximate Dynamic Programming (Technical Report)}},
url = {http://arxiv.org/abs/1602.04762},
year = {2016}
}
@article{Lamers2016a,
abstract = {The growing public interest for Unmanned Aircraft Systems (UAS) applications has stimulated the debate over the integration of this kind of aircraft into the civil aviation system. However, the concept of not having a human pilot inside the aircraft presents uncertainties that may impede the creation of proper regulation. Having safety as the main concern for civil aviation, one important principle of aviation to be addressed in an UAS is collision avoidance, a traditionally pilot-dependent functionality. In this regard, as a possible substitute for the pilot in the aircraft, we propose a method for implementing a learning-based autonomous control system focused in guaranteeing collision avoidance. Regarding that safety aspect, we expect such system to be able to compensate for the lack of a human pilot in the aircraft. The proposed approach utilizes the concept of 'Learning from Demonstration' in order to define a behaviour for the autonomous aircraft based on manoeuvres commanded by a human. Therefore, the proposed approach would represent a possible implementation of an autonomous unmanned aircraft that presents the same collision avoidance capabilities observed in (human-based) civil aviation. Additionally, we identify metrics that can be used to select a suitable learning-based method and to compare its performance to those observed in manned aircraft.},
archivePrefix = {arXiv},
arxivId = {1509.06791},
author = {Lamers, Kevin and Tijmons, Sjoerd and {De Wagter}, Christophe and {De Croon}, Guido},
doi = {10.1109/IROS.2016.7759284},
eprint = {1509.06791},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Lamers et al. - 2016 - Self-supervised monocular distance learning on a lightweight micro air vehicle.pdf:pdf},
isbn = {9781509037629},
issn = {21530866},
journal = {IEEE International Conference on Intelligent Robots and Systems},
pages = {1779--1784},
title = {{Self-supervised monocular distance learning on a lightweight micro air vehicle}},
volume = {2016-Novem},
year = {2016}
}
@article{Khaksar2015,
author = {Khaksar, Weria and Salleh, Khairul and Saharia, Mohamed and Ismail, Firas B and Nasional, Universiti Tenaga},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Khaksar et al. - 2015 - A Review on Mobile robots Motion Path Planning in unknown environments.pdf:pdf},
isbn = {9781467371247},
keywords = {artificial intelligence,component,environment,mobile robot,motion path planning,motion planning,survey,unknown},
mendeley-tags = {motion planning,survey},
pages = {295--300},
title = {{A Review on Mobile robots Motion Path Planning in unknown environments}},
year = {2015}
}
@article{Delmerico2018,
author = {Delmerico, Jeffrey and Scaramuzza, Davide},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Delmerico, Scaramuzza - 2018 - A Benchmark Comparison of Monocular Visual-Inertial Odometry Algorithms for Flying Robots.pdf:pdf},
title = {{A Benchmark Comparison of Monocular Visual-Inertial Odometry Algorithms for Flying Robots}},
year = {2018}
}
@article{Kim2007,
author = {Kim, Piljae and Szenher, Matthew D and Webb, Barbara},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Kim, Szenher, Webb - 2007 - Entropy-based visual homing.pdf:pdf},
isbn = {9781424426935},
pages = {2007},
title = {{Entropy-based visual homing}},
year = {2007}
}
@article{Hamel2007,
abstract = {An image-based strategy for visual servo control of a class of dynamic systems is proposed. The class of systems considered includes dynamic models of unmanned aerial vehicles capable of quasi-stationary flight (hover and near hover flight). The control strategy exploits passivity-like properties of the dynamic model to derive a Lyapunov control algorithm using backstepping techniques. The paper extends earlier work (Hamel, T., {\&} Mahony, R. (2002). Visual servoing of an under-actuated dynamic rigid-body system: An image based approach. IEEE Transactions on Robotics and Automation, 18(2), 187-198) where partial pose information was used in the construction of the visual error. In this paper the visual error is defined purely in terms of the image features derived from the camera input. Local exponential stability of the system is proved. An estimate of the basin of attraction for the closed-loop system is provided. ?? 2007 Elsevier Ltd. All rights reserved.},
author = {Hamel, Tarek and Mahony, Robert},
doi = {10.1016/j.automatica.2007.03.030},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Hamel, Mahony - 2007 - Image based visual servo control for a class of aerial robotic systems.pdf:pdf},
issn = {00051098},
journal = {Automatica},
keywords = {Dynamic system,Image based visual servo,Zero dynamics},
number = {11},
pages = {1975--1983},
title = {{Image based visual servo control for a class of aerial robotic systems}},
volume = {43},
year = {2007}
}
@article{Hwang1992,
abstract = {Motion planning is one of the most important areas of robotics research. The complexity of the motion-planning problem has hindered the development of practical algorithms. This paper surveys the work on gross-motion planning, including motion planners for point robots, rigid robots, and manipulators in stationary, time-varying, constrained, and movable-object environments. The general issues in motion planning are explained. Recent approaches and their performances are briefly described, and possible future research directions are discussed.},
author = {Hwang, Yong K. and Ahuja, Narendra},
doi = {10.1145/136035.136037},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Hwang, Ahuja - 1992 - Gross Motion Planning - a Survey.pdf:pdf},
isbn = {0360-0300},
issn = {0360-0300},
journal = {ACM Comput. Surv.},
keywords = {motion planning,survey},
mendeley-tags = {motion planning,survey},
number = {3},
pages = {219--291},
title = {{Gross Motion Planning - a Survey}},
url = {http://doi.acm.org/10.1145/136035.136037{\%}5Cnhttp://dl.acm.org/ft{\_}gateway.cfm?id=136037{\&}type=pdf},
volume = {24},
year = {1992}
}
@inproceedings{Pinggera2015,
author = {Pinggera, Peter and Franke, Uwe and Mester, Rudolf},
booktitle = {2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
doi = {10.1109/IROS.2015.7353537},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Pinggera, Franke, Mester - 2015 - High-Performance Long Range Obstacle Detection Using Stereo Vision.pdf:pdf},
isbn = {9781479999941},
pages = {1308--1313},
title = {{High-Performance Long Range Obstacle Detection Using Stereo Vision}},
year = {2015}
}
@article{Sousa2011,
abstract = {How do we know how far an object is? If an object's size is known, its retinal image size can be used to judge its distance. To some extent, the retinal image size of an unfamiliar object can also be used to judge its distance, because some object sizes are more likely than others. To examine whether assumptions about object size are used to judge distance, we had subjects indicate the distance of virtual cubes in complete darkness. In separate sessions, the simulated cube size either varied slightly or considerably across presentations. Most subjects indicated a further distance when the simulated cube was smaller, showing that they used retinal image size to judge distance. The cube size that was considered to be most likely depended on the simulated cubes on previous trials. Moreover, subjects relied twice as strongly on retinal image size when the range of simulated cube sizes was small. We conclude that the variability in the perceived cube sizes on previous trials influences the range of sizes that are considered to be likely.},
author = {Sousa, R. and Brenner, E. and Smeets, J. B. J. and W., Gogel and J., Predebon and E., Brenner and S., Collett T. and B., Gillam and W., Gogel and A., Lugtigheid and D., McIntosh R. and C., Muller and A., Sedgwick H. and A., Seydell and R., Sousa and R., Sousa and J., Watt S.},
doi = {10.1167/11.9.10.Introduction},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Sousa et al. - 2011 - Judging an unfamiliar object's distance from its retinal image size.pdf:pdf},
isbn = {1534-7362 (Electronic)$\backslash$r1534-7362 (Linking)},
issn = {1534-7362},
journal = {Journal of Vision},
keywords = {body weight,darkness,eye,fingers,index finger,judgment,perception,volume,weight (force)},
number = {9},
pages = {10--10},
pmid = {21859822},
title = {{Judging an unfamiliar object's distance from its retinal image size}},
url = {http://jov.arvojournals.org/Article.aspx?doi=10.1167/11.9.10},
volume = {11},
year = {2011}
}
@inproceedings{Ohno1996,
author = {Ohno, T. and Ohya, A. and Yuta, S.},
booktitle = {Proceedings of IEEE/RSJ International Conference on Intelligent Robots and Systems. IROS '96},
doi = {10.1109/IROS.1996.571034},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Ohno, Ohya, Yuta - 1996 - Autonomous navigation for mobile robots referring pre-recorded image sequence.pdf:pdf},
isbn = {0-7803-3213-X},
pages = {672--679},
publisher = {IEEE},
title = {{Autonomous navigation for mobile robots referring pre-recorded image sequence}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=571034},
volume = {2},
year = {1996}
}
@article{Fortun2015a,
author = {Fortun, Denis and Bouthemy, Patrick and Kervrann, Charles},
doi = {10.1016/j.cviu.2015.02.008},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Fortun, Bouthemy, Kervrann - 2015 - Optical flow modeling and computation A survey.pdf:pdf},
issn = {1077-3142},
journal = {Computer Vision and Image Understanding},
pages = {1--21},
publisher = {Elsevier Inc.},
title = {{Optical flow modeling and computation: A survey}},
url = {http://dx.doi.org/10.1016/j.cviu.2015.02.008},
volume = {134},
year = {2015}
}
@article{Redmon2015,
abstract = {We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is far less likely to predict false detections where nothing exists. Finally, YOLO learns very general representations of objects. It outperforms all other detection methods, including DPM and R-CNN, by a wide margin when generalizing from natural images to artwork on both the Picasso Dataset and the People-Art Dataset.},
archivePrefix = {arXiv},
arxivId = {1506.02640},
author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
doi = {10.1109/CVPR.2016.91},
eprint = {1506.02640},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Redmon et al. - 2015 - You Only Look Once Unified, Real-Time Object Detection.pdf:pdf},
isbn = {978-1-4673-8851-1},
issn = {01689002},
pmid = {27295650},
title = {{You Only Look Once: Unified, Real-Time Object Detection}},
url = {http://arxiv.org/abs/1506.02640},
year = {2015}
}
@article{Moller2012,
abstract = {A model of visual navigation in ants is presented which is based on a simple network predicting the changes of a visual scene under translatory movements. The model contains two behavioral components: the acquisition of multiple snapshots in different orientations during a learning walk, and the selection of a movement direction by a scanning behavior where the ant searches through different headings. Both components fit with observations in experiments with desert ants. The model is in most aspects biologically plausible with respect to the equivalent neural networks, and it produces reliable homing behavior in a simulated environment with a complex random surface texture. The model is closely related to the algorithmic min-warping method for visual robot navigation which shows good homing performance in real-world environments. {\textcopyright} 2012 Elsevier Ltd.},
author = {M{\"{o}}ller, Ralf},
doi = {10.1016/j.jtbi.2012.04.022},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/M{\"{o}}ller - 2012 - A model of ant navigation based on visual prediction.pdf:pdf},
issn = {00225193},
journal = {Journal of Theoretical Biology},
keywords = {Guidance models,Insect homing,Learning walks,Optical flow,Scanning behavior},
pages = {118--130},
pmid = {22554981},
title = {{A model of ant navigation based on visual prediction}},
volume = {305},
year = {2012}
}
@article{Stelzer2018,
author = {Stelzer, Annett and Vayugundla, Mallikarjuna and Mair, Elmar and Suppa, Michael and Burgard, Wolfram},
doi = {10.1177/0278364918761115},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Stelzer et al. - 2018 - Towards efficient and scalable visual homing.pdf:pdf},
isbn = {0278364918761},
issn = {0278-3649},
journal = {The International Journal of Robotics Research},
keywords = {omnidirectional vision,range-free navigation,visual homing},
number = {2-3},
pages = {225--248},
title = {{Towards efficient and scalable visual homing}},
url = {http://journals.sagepub.com/doi/10.1177/0278364918761115},
volume = {37},
year = {2018}
}
@misc{Franceschini1992,
abstract = {Airborne insects are miniature wing-flapping aircraft the visually guided manoeuvres of which depend on analogue, `fly-by-wire' controls. The front-end of their visuomotor system consists of a pair of compound eyes which are masterpieces of integrated optics and neural design. They rely on an array of passive sensors driving an orderly analogue neural network. We explored in concrete terms how motion-detecting neurons might possibly be used to solve navigational tasks involving obstacle avoidance in a creature whose wings are exquisitely guided by eyes with a poor spatial resolution. We designed, simulated, and built a complete terrestrial creature which moves about and avoids obstacles solely by evaluating the relative motion between itself and the environment. The compound eye uses an array of elementary motion detectors (EMDS) as smart, passive ranging sensors. Like its physiological counterpart, the visuomotor system is based on analogue, continuous-time processing and does not make use of conventional computers. It uses hardly any memory to adjust the robot's heading in real time via a local and intermittent visuomotor feedback loop. This paper shows that the understanding of some invertebrate sensory-motor systems has now reached a level able to provide valuable design hints. Our approach brings into prominence the mutual constraints in the designs of a sensory and a motor system, in both living and non-living ambulatory creatures.},
author = {Franceschini, N. and Pichon, J. M. and Blanes, C. and Brady, J. M.},
booktitle = {Philosophical Transactions of the Royal Society B: Biological Sciences},
doi = {10.1098/rstb.1992.0106},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Franceschini et al. - 1992 - From Insect Vision to Robot Vision and Discussion.pdf:pdf},
isbn = {0962843628},
issn = {0962-8436},
number = {1281},
pages = {283--294},
pmid = {20176664},
title = {{From Insect Vision to Robot Vision [and Discussion]}},
volume = {337},
year = {1992}
}
@article{Dacke2016,
abstract = {Ants are known to be capable of homing to their nest after displacement to a novel location. This is widely assumed to involve some form of retinotopic matching between their current view and previously experienced views. One simple algorithm proposed to explain this behavior is continuous retinotopic alignment, in which the ant constantly adjusts its heading by rotating to minimize the pixel-wise difference of its current view from all views stored while facing the nest. However, ants with large prey items will often drag them home while facing backwards. We tested whether displaced ants (Myrmecia croslandi) dragging prey could still home despite experiencing an inverted view of their surroundings under these conditions. Ants moving backwards with food took similarly direct paths to the nest as ants moving forward without food, demonstrating that continuous retinotopic alignment is not a critical component of homing. It is possible that ants use initial or intermittent retinotopic alignment, coupled with some other direction stabilizing cue that they can utilize when moving backward. However, though most ants dragging prey would occasionally look toward the nest, we observed that their heading direction was not noticeably improved afterwards. We assume ants must use comparison of current and stored images for corrections of their path, but suggest they are either able to chose the appropriate visual memory for comparison using an additional mechanism; or can make such comparisons without retinotopic alignment.},
author = {Ardin, Paul B and Mangan, Michael and Webb, Barbara},
doi = {10.3389/fnbeh.2016.00069},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Ardin, Mangan, Webb - 2016 - Ant Homing Ability Is Not Diminished When Traveling Backwards.pdf:pdf},
issn = {1662-5153},
journal = {Frontiers in Behavioral Neuroscience},
keywords = {biology,experiment,scene familiarity,visual homing},
mendeley-tags = {biology,experiment,scene familiarity,visual homing},
month = {apr},
title = {{Ant Homing Ability Is Not Diminished When Traveling Backwards}},
url = {http://journal.frontiersin.org/Article/10.3389/fnbeh.2016.00069/abstract},
volume = {10},
year = {2016}
}
@inproceedings{Hornung2012,
author = {Hornung, Armin and Phillips, Mike and Jones, E Gil and Bennewitz, Maren and Likhachev, Maxim and Chitta, Sachin},
booktitle = {Robotics and Automation (ICRA), 2012 IEEE International Conference on},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Hornung et al. - 2012 - Navigation in Three-Dimensional Cluttered Environments for Mobile Manipulation.pdf:pdf},
pages = {423--429},
publisher = {IEEE},
title = {{Navigation in Three-Dimensional Cluttered Environments for Mobile Manipulation}},
year = {2012}
}
@book{Szeliski2010,
abstract = {As humans, we perceive the three-dimensional structure of the world around us with apparent ease. However, despite all of the recent advances in computer vision research, the dream of having a computer interpret an image at the same level as a two-year old remains elusive. Why is computer vision such a challenging problem, and what is the current state of the art?Computer Vision: Algorithms and Applications explores the variety of techniques commonly used to analyze and interpret images. It also describes challenging real-world applications where vision is being successfully used, both for specialized applications such as medical imaging and fun consumer-level tasks such as image editing and stitching, which students can apply to their own personal photos and videos.More than just a source of "recipes", this text/reference also takes a scientific approach to basic vision problems, formulating physical models of the imaging process before inverting this process to produce the best possible descriptions of a scene. Exercises are presented throughout the book, with a heavy emphasis on testing algorithms.Suitable for either an undergraduate or a graduate-level course in computer vision, this textbook focuses on basic techniques that work under real-world conditions and encourages students to push their creative boundaries.Dr. Richard Szeliski has over twenty years' experience in computer vision research, most notably at Digital Equipment Corporation and Microsoft.},
address = {London},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Szeliski, Richard},
doi = {10.1007/978-1-84882-935-0},
eprint = {arXiv:1011.1669v3},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Szeliski - 2011 - Computer Vision.pdf:pdf},
isbn = {978-1-84882-934-3},
issn = {10636919},
keywords = {highlight,survey},
mendeley-tags = {highlight,survey},
pmid = {16259003},
publisher = {Springer London},
series = {Texts in Computer Science},
title = {{Computer Vision}},
url = {http://szeliski.org/Book/},
year = {2011}
}
@inproceedings{Denuelle2016,
author = {Denuelle, Aymeric and Srinivasan, Mandyam V},
booktitle = {2016 IEEE International Conference on Robotics and Automation (ICRA)},
doi = {10.1109/ICRA.2016.7487524},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Denuelle, Srinivasan - 2016 - A sparse snapshot-based navigation strategy for UAS guidance in natural environments.pdf:pdf},
isbn = {978-1-4673-8026-3},
issn = {10504729},
keywords = {biology,highlight,snapshot model},
mendeley-tags = {biology,highlight,snapshot model},
month = {may},
pages = {3455--3462},
publisher = {IEEE},
title = {{A sparse snapshot-based navigation strategy for UAS guidance in natural environments}},
url = {http://ieeexplore.ieee.org/document/7487524/},
year = {2016}
}
@article{Dryanovski2010,
abstract = {Advancing research into autonomous micro aerial vehicle navigation requires data structures capable of representing indoor and outdoor 3D environments. The vehicle must be able to update the map structure in real time using readings from range-finding sensors when mapping unknown areas; it must also be able to look up occupancy information from the map for the purposes of localization and path-planning. Mapping models that have been used for these tasks include voxel grids, multi-level surface maps, and octrees. In this paper, we suggest a new approach to 3D mapping using a multi-volume occupancy grid, or MVOG. MVOGs explicitly store information about both obstacles and free space. This allows us to correct previous potentially erroneous sensor readings by incrementally fusing in new positive or negative sensor information. In turn, this enables extracting more reliable probabilistic information about the occupancy of 3D space. MVOGs outperform existing probabilistic 3D mapping methods in terms of memory usage, due to the fact that observations are grouped together into continuous vertical volumes to save space. We describe the techniques required for mapping using MVOGs, and analyze their performance using indoor and outdoor experimental data.},
author = {Dryanovski, Ivan and Morris, William and Xiao, Jizhong},
doi = {10.1109/IROS.2010.5652494},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Dryanovski, Morris, Xiao - 2010 - Multi-volume occupancy grids An efficient probabilistic 3D mapping model for micro aerial vehicles.pdf:pdf},
isbn = {9781424466757},
issn = {2153-0858},
journal = {IEEE/RSJ 2010 International Conference on Intelligent Robots and Systems, IROS 2010 - Conference Proceedings},
keywords = {Aerial Robotics,Mapping,Range Sensing},
number = {1},
pages = {1553--1559},
title = {{Multi-volume occupancy grids: An efficient probabilistic 3D mapping model for micro aerial vehicles}},
year = {2010}
}
@article{Fu2017,
author = {Fu, Yu and Hsiang, Tien-Ruey and Chung, Sheng-Luen},
doi = {10.1017/S0263574712000434},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Fu, Hsiang, Chung - 2017 - Multi-waypoint visual homing in piecewise linear trajectory.pdf:pdf},
journal = {Robotica},
keywords = {multi-waypoint visual homing,piecewise},
number = {2013},
pages = {479--491},
title = {{Multi-waypoint visual homing in piecewise linear trajectory}},
volume = {31},
year = {2017}
}
@phdthesis{Nous2016a,
author = {Nous, C.W.M.},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Nous - 2016 - Performance in Obstacle Avoidance.pdf:pdf},
school = {Delft University of Technology},
title = {{Performance in Obstacle Avoidance}},
type = {MSc},
year = {2016}
}
@misc{Sturzl,
author = {St{\"{u}}rzl, Wolfgang and Suppa, Michael and Burschka, Darius},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/St{\"{u}}rzl, Suppa, Burschka - Unknown - Light-Weight Panoramic Mirror Design for Visual Navigation.pdf:pdf},
title = {{Light-Weight Panoramic Mirror Design for Visual Navigation}}
}
@article{Kendoul2009a,
abstract = {The problem considered in this paper involves the design of a vision-based autopilot for small and micro Unmanned Aerial Vehicles (UAVs). The proposed autopilot is based on an optic flow-based vision system for autonomous localization and scene mapping, and a nonlinear control system for flight control and guidance. This paper focusses on the development of a real-time 3D vision algorithm for estimating optic flow, aircraft self-motion and depth map, using a low-resolution onboard camera and a low-cost Inertial Measurement Unit (IMU). Our implementation is based on 3 Nested Kalman Filters (3NKF) and results in an efficient and robust estimation process. The vision and control algorithms have been implemented on a quadrotor UAV, and demonstrated in real-time flight tests. Experimental results show that the proposed vision-based autopilot enabled a small rotorcraft to achieve fully-autonomous flight using information extracted from optic flow. {\textcopyright} 2009 Elsevier B.V. All rights reserved.},
author = {Kendoul, Farid and Fantoni, Isabelle and Nonami, Kenzo},
doi = {10.1016/j.robot.2009.02.001},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Kendoul, Fantoni, Nonami - 2009 - Optic flow-based vision system for autonomous 3D localization and control of small aerial vehicles.pdf:pdf},
isbn = {9781848211278},
issn = {09218890},
journal = {Robotics and Autonomous Systems},
keywords = {Autonomous localization,Flight guidance and control,Optic flow,Structure-From-Motion (SFM),UAV,Visual SLAM},
number = {6-7},
pages = {591--602},
title = {{Optic flow-based vision system for autonomous 3D localization and control of small aerial vehicles}},
volume = {57},
year = {2009}
}
@article{Mikolajczyk2005,
abstract = {In this paper, we compare the performance of descriptors computed for local interest regions, as, for example, extracted by the Harris-Affine detector [Mikolajczyk, K and Schmid, C, 2004]. Many different descriptors have been proposed in the literature. It is unclear which descriptors are more appropriate and how their performance depends on the interest region detector. The descriptors should be distinctive and at the same time robust to changes in viewing conditions as well as to errors of the detector. Our evaluation uses as criterion recall with respect to precision and is carried out for different image transformations. We compare shape context [Belongie, S, et al., April 2002], steerable filters [Freeman, W and Adelson, E, Setp. 1991], PCA-SIFT [Ke, Y and Sukthankar, R, 2004], differential invariants [Koenderink, J and van Doorn, A, 1987], spin images [Lazebnik, S, et al., 2003], SIFT [Lowe, D. G., 1999], complex filters [Schaffalitzky, F and Zisserman, A, 2002], moment invariants [Van Gool, L, et al., 1996], and cross-correlation for different types of interest regions. We also propose an extension of the SIFT descriptor and show that it outperforms the original method. Furthermore, we observe that the ranking of the descriptors is mostly independent of the interest region detector and that the SIFT-based descriptors perform best. Moments and steerable filters show the best performance among the low dimensional descriptors.},
author = {Mikolajczyk, K. and Schmid, C.},
doi = {10.1109/TPAMI.2005.188},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Mikolajczyk, Schmid - 2005 - A performance evaluation of local descriptors.pdf:pdf},
isbn = {0162-8828},
issn = {0162-8828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {salient features},
mendeley-tags = {salient features},
number = {10},
pages = {1615--1630},
pmid = {16237996},
title = {{A performance evaluation of local descriptors}},
url = {http://doi.ieeecomputersociety.org/10.110910.1109/TPAMI.2005.188},
volume = {27},
year = {2005}
}
@incollection{Lamon2003,
abstract = {In this paper a perception approach allowing for high distinctiveness is presented. The method works in accordance to the fingerprint concept. Such representation allows using a very flexible matching approach based on the minimum energy algorithm. The whole extraction and matching approach is presented in details and viewed in a topological optic, where the matching result can directly be used as observation function for a topological localization approach. The experimentation section will validate the fingerprint approach and present different set of experiments in order to explain practically the choice of different types of features.},
author = {Lamon, P and Tapus, A and Glauser, E and Tomatis, N and Siegwart, R},
booktitle = {Intelligent Robots and Systems, 2003. (IROS 2003). Proceedings. 2003 IEEE/RSJ International Conference on},
doi = {10.1109/IROS.2003.1249743},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Lamon et al. - 2003 - Environmental modeling with fingerprint sequences for topological global localization.pdf:pdf},
isbn = {0780378601},
keywords = {fingerprint},
mendeley-tags = {fingerprint},
pages = {3781--3786},
publisher = {IEEE},
title = {{Environmental modeling with fingerprint sequences for topological global localization}},
volume = {4},
year = {2003}
}
@article{Moller2009,
author = {M{\"{o}}ller, Ralf},
doi = {10.1016/j.robot.2008.02.001},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/M{\"{o}}ller - 2009 - Local visual homing by warping of two-dimensional images.pdf:pdf},
journal = {Robotics and Autonomous Systems},
keywords = {image warping,navigation,visual homing},
number = {1},
pages = {87--101},
title = {{Local visual homing by warping of two-dimensional images}},
volume = {57},
year = {2009}
}
@phdthesis{Tijmons2012,
annote = {semi global block matching stereo. Verschillende avoidance technieken incl (voorloper van?) droplet.},
author = {Tijmons, S},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Tijmons - 2012 - Stereo Vision for Flapping Wing MAVs.pdf:pdf},
keywords = {TU Delft,low complexity,obstacle avoidance,reactive obstacle avoidance,safe region,scenario: room,stereo vision},
mendeley-tags = {TU Delft,low complexity,obstacle avoidance,reactive obstacle avoidance,safe region,scenario: room,stereo vision},
pages = {96},
title = {{Stereo Vision for Flapping Wing MAVs}},
year = {2012}
}
@article{Bradley2005,
abstract = {This paper presents a real-time implementation of a topological localization method based on matching image features. This work is supported by a unique sensor pod design that provides stand-alone sensing and computing for localizing a vehicle on a previously traveled road. We report extensive field test results from outdoor environments, with the sensor pod mounted on both a small and a large all-terrain vehicle. Off-line analysis of the approach is also presented to evaluate the robustness of the various image features tested against different weather and lighting conditions.},
author = {Bradley, David M. and Patel, Rashmi and Vandapel, Nicolas and Thayer, Scott M.},
doi = {10.1109/IROS.2005.1545442},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Bradley et al. - 2005 - Real-time image-based topological localization in large outdoor environments.pdf:pdf},
isbn = {0780389123},
journal = {2005 IEEE/RSJ International Conference on Intelligent Robots and Systems, IROS},
keywords = {histogram - gradient orientation},
mendeley-tags = {histogram - gradient orientation},
pages = {3062--3069},
title = {{Real-time image-based topological localization in large outdoor environments}},
year = {2005}
}
@article{Sturzl2006,
abstract = {We present a fast and efficient homing algorithm based on Fourier transformed panoramic images. By continuously comparing Fourier coefficients calculated from the current view with coefficients representing the goal location, a mobile robot is able to find its way back to known locations. No prior knowledge about the orientation with respect to the goal location is required, since the Fourier phase is used for a fast sub-pixel orientation estimation. We present homing runs performed by an autonomous mobile robot in an office environment. In a more comprehensive investigation the algorithm is tested on an image data base recorded by a small mobile robot in a toy house arena. Catchment areas for the proposed algorithm are calculated and compared to results of a homing scheme described in [M. Franz, B. Sch??lkopf, H. Mallot, H. B??lthoff, Where did I take that snapshot? Scene based homing by image matching, Biological Cybernetics 79 (1998) 191-202] and a simple homing strategy using neighbouring views. The results show that a small number of coefficients is sufficient to achieve a good homing performance. Also, a coarse-to-fine homing strategy is proposed in order to achieve both a large catchment area and a high homing accuracy: the number of Fourier coefficients used is increased during the homing run. ?? 2005 Elsevier Ltd. All rights reserved.},
author = {St{\"{u}}rzl, W. and Mallot, H. A.},
doi = {10.1016/j.robot.2005.12.001},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/St{\"{u}}rzl, Mallot - 2006 - Efficient visual homing based on Fourier transformed panoramic images.pdf:pdf},
isbn = {09218890},
issn = {09218890},
journal = {Robotics and Autonomous Systems},
keywords = {Fourier transformation,Orientation estimation,Panoramic images,Visual homing},
number = {4},
pages = {300--313},
title = {{Efficient visual homing based on Fourier transformed panoramic images}},
volume = {54},
year = {2006}
}
@article{Surzl2007,
author = {S{\"{u}}rzl, Wolfgang and Zeil, Jochen},
doi = {10.1007/s00422-007-0147-3},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/S{\"{u}}rzl, Zeil - 2007 - Depth, contrast and view-based homing in outdoor scenes.pdf:pdf},
journal = {Biological Cybernetics},
number = {96},
pages = {519--531},
title = {{Depth, contrast and view-based homing in outdoor scenes}},
year = {2007}
}
@article{Mej??as2006,
abstract = {The use of UAVs in civilian and domestic applications is highly demanding, requiring a high-level of capability from the vehicles. This work addresses the task in which a UAV is performing an inspection on a set of power lines and an emergency situation occurs requiring the UAV to avoid the lines and then find a safe landing area (a forced landing). This problem is approached using vision, where the vision system acts as the overall controller sending velocity commands to a low-level controller. The use of vision here allows the 2D position of the UAV to be updated by an image-based signal where the error to minimize is the location of a feature or set of features in the image. The system has been tested in a air vehicle simulator (AVS) - a cable array robot which allows to simulate and control three DOF (translation) of a UAV. Results obtained from tests in a scale scenario show the feasibility of this approach},
author = {Mej??as, Luis and Campoy, Pascual and Usher, Kane and Roberts, Jonathan and Corke, Peter},
doi = {10.1109/IROS.2006.281638},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Mejas et al. - 2006 - Two seconds to touchdown - Vision-based controlled forced landing.pdf:pdf},
isbn = {142440259X},
journal = {IEEE International Conference on Intelligent Robots and Systems},
pages = {3527--3532},
title = {{Two seconds to touchdown - Vision-based controlled forced landing}},
year = {2006}
}
@article{Rastegari2016,
abstract = {We propose two efficient approximations to standard convolutional neural networks: Binary-Weight-Networks and XNOR-Networks. In Binary-Weight-Networks, the filters are approximated with binary values resulting in 32x memory saving. In XNOR-Networks, both the filters and the input to convolutional layers are binary. XNOR-Networks approximate convolutions using primarily binary operations. This results in 58x faster convolutional operations and 32x memory savings. XNOR-Nets offer the possibility of running state-of-the-art networks on CPUs (rather than GPUs) in real-time. Our binary networks are simple, accurate, efficient, and work on challenging visual tasks. We evaluate our approach on the ImageNet classification task. The classification accuracy with a Binary-Weight-Network version of AlexNet is only 2.9{\%} less than the full-precision AlexNet (in top-1 measure). We compare our method with recent network binarization methods, BinaryConnect and BinaryNets, and outperform these methods by large margins on ImageNet, more than 16{\%} in top-1 accuracy.},
archivePrefix = {arXiv},
arxivId = {1603.05279},
author = {Rastegari, Mohammad and Ordonez, Vicente and Redmon, Joseph and Farhadi, Ali},
doi = {10.1007/978-3-319-46493-0},
eprint = {1603.05279},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Rastegari et al. - 2016 - XNOR-Net ImageNet Classification Using Binary Convolutional Neural Networks.pdf:pdf},
isbn = {9783319464930},
issn = {0302-9743},
journal = {arXiv preprint},
keywords = {binary convolution,binary deep learning,binary neural networks,convolutional neural network,deep learning},
pages = {1--17},
title = {{XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks}},
url = {http://arxiv.org/abs/1603.05279},
year = {2016}
}
@article{Labrosse2006,
author = {Labrosse, Fr{\'{e}}d{\'{e}}ric},
doi = {10.1002/rob.20159},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Labrosse - 2006 - The visual compass performance and limitations of an appearance-based method.pdf:pdf},
journal = {Journal of Field Robotics},
number = {10},
pages = {913--941},
title = {{The visual compass: performance and limitations of an appearance-based method}},
volume = {23},
year = {2006}
}
@article{Mori2013,
abstract = {Obstacle avoidance is desirable for lightweight micro aerial vehicles and is a challenging problem since the payload constraints only permit monocular cameras and obstacles cannot be directly observed. Depth can however be inferred based on various cues in the image. Prior work has examined optical flow, and perspective cues, however these methods cannot handle frontal obstacles well. In this paper we examine the problem of detecting obstacles right in front of the vehicle. We developed a method to detect relative size changes of image patches that is able to detect size changes in the absence of optical flow. The method uses SURF feature matches in combination with template matching to compare relative obstacle sizes with different image spacing. We present results from our algorithm in autonomous flight tests on a small quadrotor. We are able to detect obstacles with a frame- to-frame enlargement of 120{\%} with a high confidence and confirmed our algorithm in 20 successful flight experiments. In future work, we will improve the control algorithms to avoid more complicated obstacle configurations},
annote = {SIFT feature scale verandering, apparent size om obstakels te herkennen. Mooi overzicht van obstacle detection technieken, daarom survey tag.},
author = {Mori, Tomoyuki and Scherer, Sebastian},
doi = {10.1109/ICRA.2013.6630807},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Mori, Scherer - 2013 - First results in detecting and avoiding frontal obstacles from a monocular camera for micro unmanned aerial vehic.pdf:pdf},
isbn = {9781467356411},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
keywords = {expansion rate,experiment,highlight,low complexity,monocular,nieuwe referenties,obstacle avoidance,obstacle detection,reactive obstacle avoidance,scenario: forest,simulation,survey},
mendeley-tags = {expansion rate,experiment,highlight,low complexity,monocular,nieuwe referenties,obstacle avoidance,obstacle detection,reactive obstacle avoidance,scenario: forest,simulation,survey},
pages = {1750--1757},
title = {{First results in detecting and avoiding frontal obstacles from a monocular camera for micro unmanned aerial vehicles}},
year = {2013}
}
@inproceedings{Einhorn2011,
author = {Einhorn, Erik and Gross, Horst-michael},
booktitle = {Robotics and Automation (ICRA), 2011 IEEE International Conference on},
doi = {10.1109/ICRA.2011.5980084},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Einhorn, Gross - 2011 - Finding the Adequate Resolution for Grid Mapping - Cell Sizes Locally Adapting On-the-Fly.pdf:pdf},
isbn = {9781612843858},
pages = {1843--1848},
publisher = {IEEE},
title = {{Finding the Adequate Resolution for Grid Mapping - Cell Sizes Locally Adapting On-the-Fly}},
year = {2011}
}
@article{Vijayanarasimhan2017,
archivePrefix = {arXiv},
arxivId = {1704.07804v1},
author = {Vijayanarasimhan, Sudheendra and Ricco, Susanna and Schmid, Cordelia and Sukthankar, Rahul and Fragkiadaki, Katerina},
eprint = {1704.07804v1},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Vijayanarasimhan et al. - 2017 - SfM-Net Learning of Structure and Motion from Video.pdf:pdf},
journal = {arXiv preprint arXiv:1704.07804},
title = {{SfM-Net: Learning of Structure and Motion from Video}},
year = {2017}
}
@phdthesis{TomasCardosoRezioMartins2017,
author = {{Tom{\'{a}}s Cardoso R{\'{e}}zio Martins}, Diogo},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Tom{\'{a}}s Cardoso R{\'{e}}zio Martins - 2017 - Fusion of stereo and monocular depth estimates in a self-supervised learning context.pdf:pdf},
school = {Delft University of Technology},
title = {{Fusion of stereo and monocular depth estimates in a self-supervised learning context}},
type = {MSc thesis},
year = {2017}
}
@article{Murray1987,
abstract = {This paper presents results from computer experiments with an algorithm to perform scene disposition and motion segmentation from visual motion or optic flow. The maximum a posteriori (MAP) criterion is used to formulate what the best segmentation or interpretation of the scene should be, where the scene is assumed to be made up of some fixed number of moving planar surface patches. The Bayesian approach requires, first, specification of prior expectations for the optic flow field, which here is modeled as spatial and temporal Markov random fields; and, secondly, a way of measuring how well the segmentation predicts the measured flow field. The Markov random fields incorporate the physical constraints that objects and their images are probably spatially continuous, and that their images are likely to move quite smoothly across the image plane. To compute the flow predicted by the segmentation, a recent method for reconstructing the motion and orientation of planar surface facets is used. The search for the globally optimal segmentation is performed using simulated annealing.},
author = {Murray, David W. and Buxton, Bernard F.},
doi = {10.1109/TPAMI.1987.4767896},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Murray, Buxton - 1987 - Scene Segmentation from Visual Motion Using Global Optimization.pdf:pdf},
issn = {0162-8828},
journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
keywords = {obstacle detection,optical flow},
mendeley-tags = {obstacle detection,optical flow},
number = {2},
pages = {220--228},
title = {{Scene Segmentation from Visual Motion Using Global Optimization}},
volume = {PAMI-9},
year = {1987}
}
@inproceedings{Courbon2009,
abstract = {See, stats, and : https : / / www . researchgate . net / publication / 224090919 Visual Conference DOI : 10 . 1109 / IROS . 2009 . 5354494 : IEEE CITATIONS 25 READS 85 4 , including : Y . Mezouar SIGMA - Clermont 141 , 293 SEE Philippe Ecole 293 , 570 SEE All - text , letting . Available : Philippe Retrieved : 12 Abstract—This paper presents a vision - based navigation strat - egy for a Vertical Take - off and Landing (VTOL) Unmanned Aerial Vehicle (UAV) using a single embedded camera observing natural landmarks . In the proposed approach , images of the environment are first sampled and stored as a set of ordered key images (visual path) and organized providing a visual memory of the environment . The robot navigation task is then defined as a concatenation of visual path subsets (called visual route) linking the current observed image and a target image belonging to the visual memory . The UAV is controlled to reach each image of the visual route using a vision - based control law adapted to its dynamic model and without explicitly planning any trajectory . This framework is largely substantiated by experiments with a X4 - flyer equipped with a fisheye camera .},
author = {Courbon, Jonathan and Mezouar, Youcef and Guenard, Nicolas and Martinet, Philippe},
booktitle = {2009 IEEE/RSJ International Conference on Intelligent Robots and Systems},
doi = {10.1109/IROS.2009.5354494},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Courbon et al. - 2009 - Visual navigation of a quadrotor Aerial Vehicle.pdf:pdf},
isbn = {978-1-4244-3803-7},
keywords = {biology,homing,snapshot model},
mendeley-tags = {biology,homing,snapshot model},
month = {oct},
pages = {5315--5320},
publisher = {IEEE},
title = {{Visual navigation of a quadrotor Aerial Vehicle}},
url = {http://ieeexplore.ieee.org/document/5354494/},
year = {2009}
}
@article{Shen2013,
abstract = {In this paper, we consider the development of a rotorcraft micro aerial vehicle (MAV) system capable of vision-based state estimation in complex environments. We pursue a systems solution for the hardware and software to enable autonomous flight with a small rotorcraft in complex indoor and outdoor environments using only onboard vision and inertial sensors. As rotorcrafts frequently operate in hover or nearhover conditions, we propose a vision-based state estimation approach that does not drift when the vehicle remains stationary. The vision-based estimation approach combines the advantages of monocular vision (range, faster processing) with that of stereo vision (availability of scale and depth information), while overcoming several disadvantages of both. Specifically, our system relies on fisheye camera images at 25 Hz and imagery from a second camera at a much lower frequency for metric scale initialization and failure recovery. This estimate is fused with IMU information to yield state estimates at 100 Hz for feedback control. We show indoor experimental results with performance benchmarking and illustrate the autonomous operation of the system in challenging indoor and outdoor environments.},
author = {Shen, Shaojie and Mulgaonkar, Yash and Michael, Nathan and Kumar, Vijay},
doi = {10.1109/ICRA.2013.6630808},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Shen et al. - 2013 - Vision-based state estimation for autonomous rotorcraft MAVs in complex environments.pdf:pdf},
isbn = {9781467356411},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
pages = {1758--1764},
title = {{Vision-based state estimation for autonomous rotorcraft MAVs in complex environments}},
year = {2013}
}
@inproceedings{Matthies2014,
abstract = {We address obstacle avoidance for outdoor flight of micro air vehicles. The highly textured nature of outdoor scenes enables camera-based perception, which will scale to very small size, weight, and power with very wide, two-axis field of regard. In this paper, we use forward-looking stereo cameras for obstacle detection and a downward-looking camera as an input to state estimation. For obstacle representation, we use image space with the stereo disparity map itself. We show that a C-space-like obstacle expansion can be done with this representation and that collision checking can be done by projecting candidate 3-D trajectories into image space and performing a z-buffer-like operation with the disparity map. This approach is very efficient in memory and computing time. We do motion planning and trajectory generation with an adaptation of a closed-loop RRT planner to quadrotor dynamics and full 3D search. We validate the performance of the system with Monte Carlo simulations in virtual worlds and flight tests of a real quadrotor through a grove of trees. The approach is designed to support scalability to high speed flight and has numerous possible generalizations to use other polar or hybrid polar/Cartesian representations and to fuse data from additional sensors, such as peripheral optical flow or radar.},
annote = {From Duplicate 1 (Stereo vision-based obstacle avoidance for micro air vehicles using disparity space - Matthies, Larry; Brockers, Roland; Kuwata, Yoshiaki; Weiss, Stephan)

Obstacle avoidance met RRT, collision checking in image space met inverted range (disparity).

Dual-core CPU.},
author = {Matthies, Larry and Brockers, Roland and Kuwata, Yoshiaki and Weiss, Stephan},
booktitle = {2014 IEEE International Conference on Robotics and Automation (ICRA)},
doi = {10.1109/ICRA.2014.6907325},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Matthies et al. - 2014 - Stereo vision-based obstacle avoidance for micro air vehicles using disparity space(2).pdf:pdf},
isbn = {978-1-4799-3685-4},
issn = {1050-4729},
keywords = {3-D trajectories,3D search,C-space-like obstacle expansion,Cameras,Monte Carlo simulations,Optical imaging,Optical sensors,Planning,Three-dimensional displays,Trajectory,Vehicles,autonomous aerial vehicles,camera-based perception,closed-loop RRT planner,collision avoidance,collision checking,deliberate obstacle avoidance,disparity space,downward-looking camera,egocylinder,experiment,flight tests,forward-looking stereo cameras,helicopters,high speed flight,highlight,image space,low complexity,micro air vehicles,motion planning,nieuwe referenties,obstacle avoidance,obstacle detection,obstacle representation,outdoor flight,quadrotor dynamics,robot vision,scenario: forest,scenario: window,simulation,state estimation,stereo disparity map,stereo image processing,stereo vision,stereo vision-based obstacle avoidance,trajectory generation,tree grove,virtual worlds,vision,z-buffer-like operation},
mendeley-tags = {deliberate obstacle avoidance,experiment,highlight,image space,low complexity,nieuwe referenties,obstacle avoidance,scenario: forest,scenario: window,simulation,stereo vision},
month = {may},
pages = {3242--3249},
publisher = {IEEE},
shorttitle = {2014 IEEE International Conference on Robotics and},
title = {{Stereo vision-based obstacle avoidance for micro air vehicles using disparity space}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6907325},
volume = {9836},
year = {2014}
}
@article{Vedula2005,
abstract = {Just as optical flow is the two-dimensional motion of points in an image, scene flow is the three-dimensional motion of points in the world. The fundamental difficulty with optical flow is that only the normal flow can be computed directly from the image measurements, without some form of smoothing or regularization. In this paper, we begin by showing that the same fundamental limitation applies to scene flow; however, many cameras are used to image the scene. There are then two choices when computing scene flow: 1) perform the regularization in the images or 2) perform the regularization on the surface of the object in the scene. In this paper, we choose to compute scene flow using regularization in the images. We describe three algorithms, the first two for computing scene flow from optical flows and the third for constraining scene structure from the inconsistencies in multiple optical flows.},
author = {Vedula, S and Rander, P and Collins, R and Kanade, T},
doi = {10.1109/TPAMI.2005.63},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Vedula et al. - 2005 - Three-dimensional scene flow.pdf:pdf},
issn = {0162-8828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Algorithms,Artificial Intelligence,Automated,Cameras,Cluster Analysis,Computer Graphics,Computer vision,Computer-Assisted,Deformable models,Fluid flow measurement,Image Enhancement,Image Interpretation,Image motion analysis,Imaging,Index Terms- Scene flow,Information Storage and Retrieval,Layout,Motion estimation,Movement,Numerical Analysis,Optical computing,Pattern Recognition,Photogrammetry,Reflectivity,Reproducibility of Results,Sensitivity and Specificity,Signal Processing,Smoothing methods,Three-Dimensional,Video Recording,cameras,computational geometry,image measurements,image motion analysis,image regularization,image sequences,multiple optical flows,normal flow,optical flow,scene structure constraint,the brightness constancy constraint,three dimensional motion,three dimensional scene flow computing,three-dimensional dense nonrigid motion,three-dimensional normal flow.},
number = {3},
pages = {475--480},
title = {{Three-dimensional scene flow}},
volume = {27},
year = {2005}
}
@article{Goldberg2011,
abstract = {Small robots require very compact, low-power, yet high performance processors for vision-based navigation algorithms like stereo vision and visual odometry. Research on real-time implementations of these algorithms has focused on FPGAs, GPUs, ASICs, and general purpose processors, which are either too big, too hot, or too hard to program. System-on-a-chip (SoC) processors for smart phones have not been exploited yet for these functions. Here we present a real-time stereo vision system with IMU assisted visual odometry implemented on a single Texas Instruments 720Mhz/520Mhz OMAP3530 SoC. We achieve frame rates of 46 fps at QVGA or 8 fps at VGA resolutions while simultaneously tracking up to 200 features, taking full advantage of the OMAP3530's integer DSP and floating point ARM processors. This is a substantial advancement over previous work as the stereo implementation produces 146Mde/s in 2.5W, yielding a stereo energy efficiency of 58.8Mde/J, which is 3.75x better than prior DSP stereo while providing more functionality.},
author = {Goldberg, Steven B. and Matthies, Larry},
doi = {10.1109/CVPRW.2011.5981842},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Goldberg, Matthies - 2011 - Stereo and IMU assisted visual odometry on an OMAP3530 for small robots.pdf:pdf},
isbn = {9781457705298},
issn = {21607508},
journal = {IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops},
title = {{Stereo and IMU assisted visual odometry on an OMAP3530 for small robots}},
year = {2011}
}
@article{McLoughlin2017,
abstract = {The automatic detection and recognition of sound events by computers is a requirement for a number of emerging sensing and human computer interaction technologies. Recent advances in this field have been achieved by machine learning classifiers working in conjunction with time-frequency feature representations. This combination has achieved excellent accuracy for classification of discrete sounds. The ability to recognise sounds under real-world noisy conditions, called robust sound event classification, is an especially challenging task that has attracted recent research attention. Another aspect of real-word conditions is the classification of continuous, occluded or overlapping sounds, rather than classification of short isolated sound recordings. This paper addresses the classification of noise-corrupted, occluded, overlapped, continuous sound recordings. It first proposes a standard evaluation task for such sounds based upon a common existing method for evaluating isolated sound classification. It then benchmarks several high performing isolated sound classifiers to operate with continuous sound data by incorporating an energy-based event detection front end. Results are reported for each tested system using the new task, to provide the first analysis of their performance for continuous sound event detection. In addition it proposes and evaluates a novel Bayesian-inspired front end for the segmentation and detection of continuous sound recordings prior to classification.},
annote = {Goede punten van paper:
- Duidelijk over network configuration en training, frame sizes etc.

Can summarize/sketch raw data to frames to classification to ROC?

Wat zijn de belangrijke conclusies voor onderzoek?
- CNN/SVM betere classifier dan MFCC-HMM wanneer noise aanwezig [p.14]
- Detection process is less robust than classification [p.15]. Other researchers may obtain good performance by separating the detection and classification tasks [p.17]},
author = {McLoughlin, Ian and Zhang, Haomin and Xie, Zhipeng and Song, Yan and Xiao, Wei and Phan, Huy},
doi = {10.1371/journal.pone.0182309},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/McLoughlin et al. - 2017 - Continuous robust sound event classification using time-frequency features and deep learning.pdf:pdf},
isbn = {1111111111},
issn = {19326203},
journal = {PLoS ONE},
number = {9},
pages = {1--19},
title = {{Continuous robust sound event classification using time-frequency features and deep learning}},
volume = {12},
year = {2017}
}
@article{Kundu2018,
abstract = {Supervised deep learning methods have shown promising results for the task of monocular depth estimation; but acquiring ground truth is costly, and prone to noise as well as inaccuracies. While synthetic datasets have been used to circumvent above problems, the resultant models do not generalize well to natural scenes due to the inherent domain shift. Recent adversarial approaches for domain adaption have performed well in mitigating the differences between the source and target domains. But these methods are mostly limited to a classification setup and do not scale well for fully-convolutional architectures. In this work, we propose AdaDepth - an unsupervised domain adaptation strategy for the pixel-wise regression task of monocular depth estimation. The proposed approach is devoid of above limitations through a) adversarial learning and b) explicit imposition of content consistency on the adapted target representation. Our unsupervised approach performs competitively with other established approaches on depth estimation tasks and achieves state-of-the-art results in a semi-supervised setting.},
archivePrefix = {arXiv},
arxivId = {1803.01599},
author = {Kundu, Jogendra Nath and Uppala, Phani Krishna and Pahuja, Anuj and Babu, R. Venkatesh},
doi = {10.1109/CVPR.2018.00281},
eprint = {1803.01599},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Kundu et al. - 2018 - AdaDepth Unsupervised Content Congruent Adaptation for Depth Estimation.pdf:pdf},
title = {{AdaDepth: Unsupervised Content Congruent Adaptation for Depth Estimation}},
url = {http://arxiv.org/abs/1803.01599},
year = {2018}
}
@incollection{Baddeley2011a,
abstract = {In this paper we propose a model of visually guided route navigation in ants that captures the known properties of real behaviour whilst retaining mechanistic simplicity and thus biological plausibility. For an ant, the coupling of movement and viewing direction means that a familiar view specifies a familiar direction of movement. Since the views experienced along a habitual route will be more familiar, route navigation can be re-cast as a search for familiar views. This search can be performed with a simple scanning routine, a behaviour that ants have been observed to perform. We test this proposed route navigation strategy in simulation, by learning a series of routes through visually cluttered environments consisting of objects that are only distinguishable as silhouettes against the sky. In the first instance we determine view familiarity by exhaustive comparison with the set of views experienced during training. In further experiments we train an artificial neural network to perform familiarity discrimination using the training views. Our results indicate that, not only is the approach successful, but also that the routes that are learnt show many of the characteristics of the routes of desert ants. As such, we believe the model represents the only detailed and complete model of insect route guidance to date. What is more, the model provides a general demonstration that visually guided routes can be produced with parsimonious mechanisms that do not specify when or what to learn, nor separate routes into sequences of waypoints.},
author = {Baddeley, Bart and Graham, Paul and Philippides, Andrew and Husbands, Philip},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-25489-5_8},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Baddeley et al. - 2011 - Models of Visually Guided Routes in Ants Embodiment Simplifies Route Acquisition.pdf:pdf},
isbn = {9783642254888},
issn = {03029743},
keywords = {Autonomous Robotics,Generative Models,Insect Navigation,Restricted Boltzmann Machine,Route Learning,View-Based Homing},
number = {PART 2},
pages = {75--84},
title = {{Models of Visually Guided Routes in Ants: Embodiment Simplifies Route Acquisition}},
url = {http://link.springer.com/10.1007/978-3-642-25489-5{\_}8},
volume = {7102 LNAI},
year = {2011}
}
@article{Jones2011a,
abstract = {We present amodel to estimatemotion frommonocular visual and inertialmeasurements. We analyze the model and characterize the conditions under which its state is observable, and its parameters are identifiable. These include the unknown gravity vector, and the unknown transformation between the camera coordinate frame and the inertial unit. We show that it is possible to estimate both state and parameters as part of an on-line procedure, but only provided that the motion sequence is “rich enough,” a condition that we characterize explicitly. We then describe an efficient implementation of a filter to estimate the state and parameters of this model, including gravity and camera-to-inertial calibration. It runs in real-time on an embedded platform, and its performance has been tested extensively. We report experiments of continuous operation, without failures, re-initialization, or re-calibration, on paths of length up to 30Km. We also describe an integrated approach to “loop-closure,” that is the recognition of previously-seen locations and the topological re-adjustment of the traveled path. It represents visual features relative to the global orientation reference provided by the gravity vector estimated by the filter, and relative to the scale provided by their known position within the map; these features are organized into “locations” defined by visibility constraints, represented in a topological graph, where loop closure can be performed without the need to re-compute past trajectories or perform bundle adjustment. The software infrastructure as well as the embedded platform is described in detail in a technical report (Jones and Soatto (2009).)},
author = {Jones, Eagle S. and Soatto, Stefano},
doi = {10.1177/0278364910388963},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Jones, Soatto - 2011 - Visual-inertial navigation, mapping and localization A scalable real-time causal approach.pdf:pdf},
isbn = {0278-3649},
issn = {0278-3649},
journal = {The International Journal of Robotics Research},
keywords = {assisted driving,autonomous robotics,localization,location recognition,loop closure,simultaneous localization and mapping,slam,structure from motion,tion,vision-aided naviga-,vision-based navigation,visual-inertial navigation},
number = {4},
pages = {407--430},
pmid = {5423178},
title = {{Visual-inertial navigation, mapping and localization: A scalable real-time causal approach}},
url = {http://journals.sagepub.com/doi/10.1177/0278364910388963},
volume = {30},
year = {2011}
}
@article{Jaegle2016,
abstract = {We propose robust methods for estimating camera egomotion in noisy, real-world monocular image sequences in the general case of unknown observer rotation and translation with two views and a small baseline. This is a difficult problem because of the nonconvex cost function of the perspective camera motion equation and because of non-Gaussian noise arising from noisy optical flow estimates and scene non-rigidity. To address this problem, we introduce the expected residual likelihood method (ERL), which estimates confidence weights for noisy optical flow data using likelihood distributions of the residuals of the flow field under a range of counterfactual model parameters. We show that ERL is effective at identifying outliers and recovering appropriate confidence weights in many settings. We compare ERL to a novel formulation of the perspective camera motion equation using a lifted kernel, a recently proposed optimization framework for joint parameter and confidence weight estimation with good empirical properties. We incorporate these strategies into a motion estimation pipeline that avoids falling into local minima. We find that ERL outperforms the lifted kernel method and baseline monocular egomotion estimation strategies on the challenging KITTI dataset, while adding almost no runtime cost over baseline egomotion methods.},
archivePrefix = {arXiv},
arxivId = {1602.04886},
author = {Jaegle, Andrew and Phillips, Stephen and Daniilidis, Kostas},
doi = {10.1109/ICRA.2016.7487206},
eprint = {1602.04886},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Jaegle, Phillips, Daniilidis - 2016 - Fast, robust, continuous monocular egomotion computation.pdf:pdf},
isbn = {9781467380263},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
pages = {773--780},
title = {{Fast, robust, continuous monocular egomotion computation}},
volume = {2016-June},
year = {2016}
}
@article{Ho2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1509.01423v1},
author = {Ho, H W and Wagter, C De and Remes, B D W and Croon, G C H E De},
eprint = {arXiv:1509.01423v1},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Ho et al. - 2015 - Optical-Flow based Self-Supervised Learning of Obstacle Appearance applied to MAV Landing.pdf:pdf},
keywords = {TU Delft,obstacle detection},
mendeley-tags = {TU Delft,obstacle detection},
number = {Iros 15},
pages = {1--10},
title = {{Optical-Flow based Self-Supervised Learning of Obstacle Appearance applied to MAV Landing}},
year = {2015}
}
@article{Nayar1997,
abstract = {Conventional video cameras have limited fields of view that make them restrictive in a variety of vision ap-plications. There are several ways to enhance the field of view of an imaging system. However, the entire imaging system must have a single effective viewpoint to enable the generation of pure perspective images from a sensed image. A new camera with a hemispherical field of view is presented. Two such cameras can be placed back-to-back, without violating the single viewpoint constraint, to arrive at a truly omnidirectional sensor. Results are presented on the software generation of pure perspective images from an omnidirectional image, given any user-selected viewing direction and magnification. The paper concludes with a discussion on the spatial resolution of the proposed camera.},
author = {Nayar, S.K.},
doi = {10.1109/CVPR.1997.609369},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Nayar - 1997 - Catadioptric omnidirectional camera.pdf:pdf},
isbn = {0-8186-7822-4},
issn = {1063-6919},
journal = {Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {482--488},
title = {{Catadioptric omnidirectional camera}},
url = {http://ieeexplore.ieee.org/document/609369/},
year = {1997}
}
@inproceedings{Newcombe2010,
abstract = {We present a method which enables rapid and dense reconstruction of scenes browsed by a single live camera. We take point-based real-time structure from motion (SFM) as our starting point, generating accurate 3D camera pose estimates and a sparse point cloud. Our main novel contribution is to use an approximate but smooth base mesh generated from the SFM to predict the view at a bundle of poses around automatically selected reference frames spanning the scene, and then warp the base mesh into highly accurate depth maps based on view-predictive optical flow and a constrained scene flow update. The quality of the resulting depth maps means that a convincing global scene model can be obtained simply by placing them side by side and removing overlapping regions. We show that a cluttered indoor environment can be reconstructed from a live hand-held camera in a few seconds, with all processing performed by current desktop hardware. Real-time monocular dense reconstruction opens up many application areas, and we demonstrate both real-time novel view synthesis and advanced augmented reality where augmentations interact physically with the 3D scene and are correctly clipped by occlusions.},
author = {Newcombe, R A and Davison, A J},
booktitle = {2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2010.5539794},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Newcombe, Davison - 2010 - Live dense reconstruction with a single moving camera.pdf:pdf},
issn = {1063-6919},
keywords = {3D camera pose estimates,3D scene,Augmented reality,Cameras,Clouds,Image motion analysis,Image reconstruction,Layout,Machine vision,Robot vision systems,Simultaneous localization and mapping,Surface reconstruction,accurate depth maps,advanced augmented reality,cameras,computer graphics,constrained scene flow update,desktop hardware,global scene model,image reconstruction,live dense reconstruction,live hand-held camera,occlusion,point-based real-time structure from motion,real-time monocular dense reconstruction,scenes,single moving camera,sparse point cloud,view-predictive optical flow},
pages = {1498--1505},
title = {{Live dense reconstruction with a single moving camera}},
year = {2010}
}
@article{Denuelle2015,
author = {Denuelle, Aymeric and Srinivasan, Mandyam V},
doi = {10.1109/ROBIO.2015.7418788},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Denuelle, Srinivasan - 2015 - Bio-inspired visual guidance From insect homing to UAS navigation.pdf:pdf},
isbn = {978-1-4673-9675-2},
keywords = {Biologically Inspired Robotics,Flying Robots,Robot Vision,biology,homing,survey},
mendeley-tags = {biology,homing,survey},
pages = {326--332},
title = {{Bio-inspired visual guidance: From insect homing to UAS navigation}},
year = {2015}
}
@article{Churchill2008,
abstract = {Local visual homing is the process of determining the direction of movement required to return an agent to a goal location by comparing the current image with an image taken at the goal, known as the snapshot image. One way of accomplishing visual homing is by computing the correspondences between features and then analyzing the resulting flow field to determine the correct direction of motion. Typically, some strong assumptions need to be posited in order to compute the home direction from the flow field. For example, it is difficult to locally distinguish translation from rotation, so many authors assume rotation to be computable by other means (e.g. magnetic compass). In this paper we present a novel approach to visual homing using scale change information from Scale Invariant Feature Transforms (SIFT) which we use to compute landmark correspondences. The method described here is able to determine the direction of the goal in the robotpsilas frame of reference, irrespective of the relative 3D orientation with the goal.},
author = {Churchill, David and Vardy, Andrew},
doi = {10.1109/IROS.2008.4651166},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Churchill, Vardy - 2008 - Homing in scale space.pdf:pdf},
isbn = {9781424420582},
journal = {2008 IEEE/RSJ International Conference on Intelligent Robots and Systems, IROS},
keywords = {Autonomous Agents,Navigation},
pages = {1307--1312},
title = {{Homing in scale space}},
year = {2008}
}
@article{Chahl1996,
author = {Chahl, J S and Srinivasan, M V},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Chahl, Srinivasan - 1996 - Biological Cybernetics.pdf:pdf},
journal = {Journal of Experimental Biology},
pages = {405--411},
title = {{Biological Cybernetics}},
volume = {411},
year = {1996}
}
@article{Luo2018,
abstract = {Previous monocular depth estimation methods take a single view and directly regress the expected results. Though recent advances are made by applying geometrically inspired loss functions during training, the inference procedure does not explicitly impose any geometrical constraint. Therefore these models purely rely on the quality of data and the effectiveness of learning to generalize. This either leads to suboptimal results or the demand of huge amount of expensive ground truth labelled data to generate reasonable results. In this paper, we show for the first time that the monocular depth estimation problem can be reformulated as two sub-problems, a view synthesis procedure followed by stereo matching, with two intriguing properties, namely i) geometrical constraints can be explicitly imposed during inference; ii) demand on labelled depth data can be greatly alleviated. We show that the whole pipeline can still be trained in an end-to-end fashion and this new formulation plays a critical role in advancing the performance. The resulting model outperforms all the previous monocular depth estimation methods as well as the stereo block matching method in the challenging KITTI dataset by only using a small number of real training data. The model also generalizes well to other monocular depth estimation benchmarks. We also discuss the implications and the advantages of solving monocular depth estimation using stereo methods.},
archivePrefix = {arXiv},
arxivId = {1803.02612},
author = {Luo, Yue and Ren, Jimmy and Lin, Mude and Pang, Jiahao and Sun, Wenxiu and Li, Hongsheng and Lin, Liang},
eprint = {1803.02612},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Luo et al. - 2018 - Single View Stereo Matching.pdf:pdf},
title = {{Single View Stereo Matching}},
url = {http://arxiv.org/abs/1803.02612},
year = {2018}
}
@inproceedings{Eigen2015,
author = {Eigen, David and Fergus, Rob},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Eigen, Fergus - 2015 - Predicting Depth, Surface Normals and Semantic Labels with a Common Multi-Scale Convolutional Architecture.pdf:pdf},
pages = {2650--2658},
title = {{Predicting Depth, Surface Normals and Semantic Labels with a Common Multi-Scale Convolutional Architecture}},
year = {2015}
}
@article{Nieuwenhuisen2014,
abstract = {Obstacle detection and real-time planning of collision-free trajectories are key for the fully autonomous operation of micro Aerial vehicles in restricted environments. In this paper, we propose a complete system with a multimodal sensor setup for omnidirectional obstacle perception consisting of a 3D laser scanner, two stereo camera pairs, and ultrasonic distance sensors. Detected obstacles are aggregated in egocentric local multiresolution grid maps. We generate trajectories in a multi-layered approach: from mission planning to global and local trajectory planning, to reactive obstacle avoidance. We evaluate our approach in simulation and with the real autonomous micro aerial vehicle.},
author = {Nieuwenhuisen, Matthias and Droeschel, David and Beul, Marius and Behnke, Sven},
doi = {10.1109/ICUAS.2014.6842355},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Nieuwenhuisen et al. - 2014 - Obstacle detection and navigation planning for autonomous micro aerial vehicles.pdf:pdf},
isbn = {978-1-4799-2376-2},
journal = {2014 Int. Conf. Unmanned Aircr. Syst.},
number = {May},
pages = {1040--1047},
title = {{Obstacle detection and navigation planning for autonomous micro aerial vehicles}},
url = {http://www.ais.uni-bonn.de/papers/ICUAS{\_}2014{\_}Nieuwenhuisen.pdf{\%}5Cnhttp://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6842355},
year = {2014}
}
@inproceedings{Junell,
author = {Junell, J and van Kampen, E},
booktitle = {IMAV},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Junell, van Kampen - Unknown - Adaptive Path Planning for a Vision-Based quadrotor in an Obstacle Field.pdf:pdf},
title = {{Adaptive Path Planning for a Vision-Based quadrotor in an Obstacle Field}}
}
@article{Huang2015,
abstract = {Dense depth cues are important and have wide applications in various computer vision tasks. In autonomous driving, LIDAR sensors are adopted to acquire depth measurements around the vehicle to perceive the surrounding environments. However, depth maps obtained by LIDAR are generally sparse because of its hardware limitation. The task of depth completion attracts increasing attention, which aims at generating a dense depth map from an input sparse depth map. To effectively utilize multi-scale features, we propose three novel sparsity-invariant operations, based on which, a sparsity-invariant multi-scale encoder-decoder network (HMS-Net) for handling sparse inputs and sparse feature maps is also proposed. Additional RGB features could be incorporated to further improve the depth completion performance. Our extensive experiments and component analysis on two public benchmarks, KITTI depth completion benchmark and NYU-depth-v2 dataset, demonstrate the effectiveness of the proposed approach. As of Aug. 12th, 2018, on KITTI depth completion leaderboard, our proposed model without RGB guidance ranks 1st among all peer-reviewed methods without using RGB information, and our model with RGB guidance ranks 2nd among all RGB-guided methods. Index Terms-depth completion, convolutional neural network, sparsity-invariant operations.},
archivePrefix = {arXiv},
arxivId = {arXiv:1808.08685v1},
author = {Huang, Zixuan and Fan, Junming and Yi, Shuai and Wang, Xiaogang and Li, Hongsheng},
eprint = {arXiv:1808.08685v1},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Huang et al. - 2015 - HMS-Net Hierarchical Multi-scale Sparsity-invariant Network for Sparse Depth Completion.pdf:pdf},
number = {8},
title = {{HMS-Net: Hierarchical Multi-scale Sparsity-invariant Network for Sparse Depth Completion}},
url = {https://arxiv.org/pdf/1808.08685.pdf},
volume = {14},
year = {2015}
}
@article{Yamaguchi2006,
abstract = {This paper proposes a method for detecting moving obstacles on roads, by using a vehicle mounted monocular camera. To detect various moving obstacles, such as vehicles and pedestrians, the ego-motion of the vehicle is initially estimated from images captured by the camera. There are two problems in ego-motion estimation. Firstly, a typical road scene contains moving obstacles. This causes false estimation of the ego-motion. Secondly, roads possess fewer features, when compared to the number associated with background structures. This reduces the accuracy of ego-motion estimation. In our approach, the ego-motion is estimated from the correspondences of dispersed feature points extracted from various regions other than those that contain moving obstacles. After estimating the ego-motion, any moving obstacles are detected by tracking the feature points over consecutive frames. In our experiments, it has been shown that the proposed method is able to detect moving obstacles},
author = {Yamaguchi, K. and Kato, T. and Ninomiya, Y.},
doi = {10.1109/IVS.2006.1689643},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Yamaguchi, Kato, Ninomiya - 2006 - Moving Obstacle Detection using Monocular Vision.pdf:pdf},
isbn = {4-901122-86-X},
journal = {IV - Proceedings of IEEE Intelligent Vehicles Symposium},
pages = {288--293},
title = {{Moving Obstacle Detection using Monocular Vision}},
url = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=1689643},
year = {2006}
}
@incollection{Tapus2004,
author = {Tapus, A and Tomatis, N and Siegwart, R},
booktitle = {Experimental Robotics IX},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Tapus, Tomatis, Siegwart - 2006 - Topological Global Localization and Mapping with Fingerprint and Uncertainty.pdf:pdf},
pages = {99--111},
publisher = {Springer},
title = {{Topological Global Localization and Mapping with Fingerprint and Uncertainty}},
year = {2006}
}
@article{Yang2000,
abstract = {In this paper, a biologically inspired neural network approach to real-time collision-free motion planning of mobile robots or robot manipulators in a nonstationary environment is proposed. Each neuron in the topologically organized neural network has only local connections, whose neural dynamics is characterized by a shunting equation. Thus the computational complexity linearly depends on the neural network size. The real-time robot motion is planned through the dynamic activity landscape of the neural network without any prior knowledge of the dynamic environment, without explicitly searching over the free workspace or the collision paths, and without any learning procedures. Therefore it is computationally efficient. The global stability of the neural network is guaranteed by qualitative analysis and the Lyapunov stability theory. The effectiveness and efficiency of the proposed approach are demonstrated through simulation studies.},
annote = {Path planning met neural grid. Deed erg aan potential fields denken, maar gaat mogelijk iets beter om met obstakels. Ondersteunt bewegende obstakels en targets, maar vanwege grid erg hoge computational complexity, helemaal voor MAVs.},
author = {Yang, S X and Meng, M},
doi = {10.1016/S0893-6080(99)00103-3},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Yang, Meng - 2000 - An efficient neural network approach to dynamic robot motion planning.pdf:pdf},
issn = {0893-6080},
journal = {Neural networks : the official journal of the International Neural Network Society},
keywords = {cartesian map,collision avoidance,deliberate obstacle avoidance,motion planning,moving obstacles,neural networks,obstacle avoidance,real-time algorithm,robot navigation,simulation,voxel map},
mendeley-tags = {cartesian map,deliberate obstacle avoidance,motion planning,moving obstacles,obstacle avoidance,simulation,voxel map},
number = {2},
pages = {143--148},
pmid = {10935758},
title = {{An efficient neural network approach to dynamic robot motion planning.}},
volume = {13},
year = {2000}
}
@article{Werner2009a,
author = {Werner, Felix and Maire, Frederic and Sitte, Joaquin},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Werner, Maire, Sitte - 2009 - Topological SLAM Using Fast Vision Techniques.pdf:pdf},
keywords = {Histogram - color,autonomous mobile robots,colour histograms,correspondence prob-,lem,panoramic vision,slam,topological navigation},
mendeley-tags = {Histogram - color},
pages = {187--196},
title = {{Topological SLAM Using Fast Vision Techniques}},
year = {2009}
}
@article{Fu2015,
abstract = {Visual odometry and mapping methods can provide accurate navigation and comprehensive environment (obstacle) information for autonomous flights of Unmanned Aerial Vehicle (UAV) in GPS-denied cluttered environments. This work presents a new light small-scale low-cost ARM-based stereo vision pre-processing system, which not only is used as onboard sensor to continuously estimate 6-DOF UAV pose, but also as onboard assistant computer to pre-process visual information, thereby saving more computational capability for the onboard host computer of the UAV to conduct other tasks. The visual odometry is done by one plugin specifically developed for this new system with a fixed baseline (12cm). In addition, the pre-processed infromation from this new system are sent via a Gigabit Ethernet cable to the onboard host computer of UAV for real-time environment reconstruction and obstacle detection with a octree-based 3D occupancy grid mapping approach, i.e. OctoMap. The visual algorithm is evaluated with the stereo video datasets from EuRoC Challenge III in terms of efficiency, accuracy and robustness. Finally, the new system is mounted and tested on a real quadrotor UAV to carry out the visual odometry and mapping task.},
author = {Fu, Changhong and Carrio, Adrian and Campoy, Pascual},
doi = {10.1109/ICUAS.2015.7152384},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Fu, Carrio, Campoy - 2015 - Efficient visual odometry and mapping for Unmanned Aerial Vehicle using ARM-based stereo vision pre-processi.pdf:pdf},
isbn = {9781479960101},
journal = {2015 International Conference on Unmanned Aircraft Systems, ICUAS 2015},
pages = {957--962},
title = {{Efficient visual odometry and mapping for Unmanned Aerial Vehicle using ARM-based stereo vision pre-processing system}},
year = {2015}
}
@article{Bian2005,
abstract = {The relative effectiveness of the ground surface and other environmental surfaces (the ceiling and sidewalls) in determining perceived layout was investigated in five experiments and a real-world demonstration. In the first three experiments, two vertical or horizontal posts were positioned between two surfaces (ground and ceiling in all three experiments, left wall and right wall in Experiment 1), and optical contact was manipulated so that the two surfaces provided contradictory information about the relative distances of the posts. Observers judged which of the two posts appeared to be closer. In Experiment 4, to control the height on the posts at which the distance judgments were made, a blue dot was attached to both vertical posts at varying heights and observers judged which dot appeared closer. In Experiment 5, the posts were replaced by two gray ellipses to eliminate the effects of the regular shape and texture. Our findings were that (1) among all four surfaces tested, observers showed a preference to respond according to the optical contact information provided by the ground surface-a ground dominance effect, (2) this effect did not depend on the height of the posts in the image, (3) as the scene was tilted away from a ground/ceiling orientation, the ground dominance effect decreased, and (4) this effect was not due to the location of the judgment.},
author = {Bian, Zheng and Braunstein, Myron L. and Andersen, George J.},
doi = {10.3758/BF03193534},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Bian, Braunstein, Andersen - 2005 - The ground dominance effect in the perception of 3-D layout.pdf:pdf},
isbn = {0031-5117 (Print)},
issn = {00315117},
journal = {Perception and Psychophysics},
number = {5},
pages = {802--815},
pmid = {16334053},
title = {{The ground dominance effect in the perception of 3-D layout}},
volume = {67},
year = {2005}
}
@inproceedings{Silberman2012,
address = {Berlin, Heidelberg},
author = {Silberman, Nathan and Hoiem, Derek and Kohli, Pushmeet and Fergus, Rob},
booktitle = {Computer Vision -- ECCV 2012},
editor = {Fitzgibbon, Andrew and Lazebnik, Svetlana and Perona, Pietro and Sato, Yoichi and Schmid, Cordelia},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Silberman et al. - 2012 - Indoor Segmentation and Support Inference from RGBD Images.pdf:pdf},
pages = {746--760},
publisher = {Springer Berlin Heidelberg},
title = {{Indoor Segmentation and Support Inference from RGBD Images}},
year = {2012}
}
@article{Mondragon2010,
author = {Mondrag{\'{o}}n, Iv{\'{a}}n F and Campoy, Pascual and Martinez, Carol and Olivares, Miguel},
doi = {10.1016/j.robot.2010.02.012},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Mondrag{\'{o}}n et al. - 2010 - Omnidirectional vision applied to Unmanned Aerial Vehicles (UAVs) attitude and heading estimation.pdf:pdf},
issn = {0921-8890},
journal = {Robotics and Autonomous Systems},
keywords = {catadioptric systems,omnidirectional images,uav,unmanned aerial vehicles},
number = {6},
pages = {809--819},
publisher = {Elsevier B.V.},
title = {{Omnidirectional vision applied to Unmanned Aerial Vehicles (UAVs) attitude and heading estimation}},
url = {http://dx.doi.org/10.1016/j.robot.2010.02.012},
volume = {58},
year = {2010}
}
@article{Denuelle2016a,
abstract = {With the emergence of rotorcraft unmanned aerial systems (UAS) in civilian applications, the capability of accurate visual hovering is required in near-ground, GPS-denied, flying operations. Optic flow is commonly used for vision-based guidance and control of UAS, enabling autonomous obstacle avoidance, speed regulation, odometry, etc. This paper presents an optic flow-based method that uses a bio-inspired concept of image matching for the control of drift-free hover in natural environments. Our approach uses a reference snapshot (panoramic image) taken at the desired hover location and, through optic flow measurements, it estimates the rotorcraft's 3D position and velocity relative to that location by matching the current and reference views at each time step. These position and velocity signals are fed to a hover controller. Sensing and control are performed in real-time, at camera frame rate (25Hz) onboard a small-size, custom-built quadrotor, and without additional sensor fusion. Results from outdoor closed-loop flight tests demonstrate robustness against long-term drift, as well as improved hover accuracy when compared to techniques that use frame-to-frame integration of egomotion vectors derived from optic flow.},
author = {Denuelle, Aymeric and Strydom, Reuben and Srinivasan, Mandyam V.},
doi = {10.1109/ROBIO.2015.7418947},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Denuelle, Strydom, Srinivasan - 2016 - Snapshot-based control of UAS hover in outdoor environments.pdf:pdf},
isbn = {9781467396745},
journal = {2015 IEEE International Conference on Robotics and Biomimetics, IEEE-ROBIO 2015},
keywords = {Flying Robots,Robot Vision,Field Robotics},
pages = {1278--1284},
title = {{Snapshot-based control of UAS hover in outdoor environments}},
year = {2016}
}
@article{Liu2012a,
abstract = {Topological mapping and scene recognition problems are still challenging, especially for online realtime vision-based applications. We develop a hierarchical probabilistic model to tackle them using color information. This work is stimulated by our previous work [1] which defined a lightweight descriptor using color and geometry information from segmented panoramic images. Our novel model uses a Dirichlet Process Mixture Model to combine color and geometry features which are extracted from omnidirectional images. The inference of the model is based on an approximation of conditional probabilities of observations given estimated models. It allows online inference of the mixture model in real-time (at 50Hz), which outperforms other existing approaches. A real experiment is carried out on a mobile robot equipped with an omnidirectional camera. The results show the competence against the state-of-art.},
author = {Liu, Ming and Siegwart, Roland},
doi = {10.1109/ICRA.2012.6225040},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Liu, Siegwart - 2012 - DP-FACT Towards topological mapping and scene recognition with color for omnidirectional camera.pdf:pdf},
isbn = {9781467314039},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
pages = {3503--3508},
title = {{DP-FACT: Towards topological mapping and scene recognition with color for omnidirectional camera}},
year = {2012}
}
@phdthesis{Deans2005,
abstract = {In many applications, mobile robots must be able to localize themselves with respect to environments which are not known a priori in order to navigate and accomplish tasks. This means that the robot must be able to build a map of an unknown environment while simultaneously localizing itself within that map. The so-called Simultaneous Localization and Mapping or SLAM problem is a formulation of this requirement, and has been the subject of a considerable amount of robotics research in the last decade. This thesis looks at the problem of localization and mapping when the only information available to the robot is measurements of relative motion and bear- ings to features. The relative motion sensor measures displacement from one time to the next through some means such as inertial measurement or odome- try, as opposed to externally referenced position measurements like compass or GPS. The bearing sensor measures the direction toward features from the robot through a sensor such as an omnidirectional camera, as opposed to bearing and range sensors such as laser rangefinders, sonar, or millimeter wave radar. A full solution to the bearing-only SLAM problem must take into consid- eration detecting and identifying features and estimating the location of the features as well as the motion of the robot using the measurements. This thesis focuses on the estimation problem given that feature detection and data as- sociation are available. Estimation requires a solution that is fast, accurate, consistent, and robust. In an applied sense, this dissertation puts forth a methodology for build- ing maps and localizing a mobile robot using odometry and monocular vision. This sensor suite is chosen for its simplicity and generality, and in some sense represents a minimal configuration for localization and mapping. In a broader sense, the dissertation describes a novel method for state estima- tion applicable to problems which exhibit particular nonlinearity and sparseness properties. The method relies on deterministic sampling in order to compute sufficient statistics at each time step in a recursive filter. The relationship of the new algorithm to bundle adjustment and Kalman filtering (including some of its variants) is discussed. i},
author = {Deans, Matthew Charles},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Deans - 2005 - Bearings-Only Localization and Mapping.pdf:pdf},
keywords = {gaussian quadrature,kalman filter,localization,mapping,mobile robots,non-linear state estimation,omnidirectional vision,unscented filter},
title = {{Bearings-Only Localization and Mapping}},
year = {2005}
}
@incollection{Philippides2016,
abstract = {This paper discusses the implementation of insect-inspired visual navigation strategies in flying robots, in particular focusing on the impact of changing height. We start by assessing the information available at different heights for visual homing in natural environments, comparing results from an open environment against one where trees and bushes are closer to the camera. We then test a route following algorithm using a gantry robot and show that a robot would be able to successfully navigate a route at a variety of heights using images saved at a different height.},
author = {Philippides, Andrew and Steadman, Nathan and Dewar, Alex and Walker, Christopher and Graham, Paul},
doi = {10.1007/978-3-319-42417-0_24},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Philippides et al. - 2016 - Insect-Inspired Visual Navigation for Flying Robots.pdf:pdf},
keywords = {experiment,visual homing},
mendeley-tags = {experiment,visual homing},
pages = {263--274},
title = {{Insect-Inspired Visual Navigation for Flying Robots}},
url = {http://link.springer.com/10.1007/978-3-319-42417-0{\_}24},
year = {2016}
}
@inproceedings{Forster,
author = {Forster, Christian and Carlone, Luca and Dellaert, Frank and Scaramuzza, Davide},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Forster et al. - 2015 - IMU Preintegration on Manifold for Efficient Visual-Inertial Maximum-a-Posteriori Estimation.pdf:pdf},
publisher = {Georgia Institute of Technology},
title = {{IMU Preintegration on Manifold for Efficient Visual-Inertial Maximum-a-Posteriori Estimation}},
year = {2015}
}
@article{Sousa2012,
abstract = {Does size matter in explaining firms' environmental responsiveness? Are large corporations more likely to engage with green issues for fear of losing stakeholder support? Are bigger companies greener because they have more resources to devote to environmental problems? This thesis argues that explaining the ambiguous relationship between organization size and environmental responsiveness depends on disaggregation. Researchers should examine alternative explanations for the size-responsiveness relationship, different levels of analysis, and distinct types of environmental responsiveness.},
author = {Sousa, Rita and Smeets, Jeroen B.J. and Brenner, Eli},
doi = {10.1068/p7324},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Sousa, Smeets, Brenner - 2012 - Does size matter.pdf:pdf},
journal = {Perception},
keywords = {Blind walking,Depth,Size},
number = {12},
pages = {1532--1534},
title = {{Does size matter?}},
volume = {41},
year = {2012}
}
@article{Howard2006,
abstract = {We describe the design and experimental validation of a large heterogeneous mobile robot team built for the DARPA Software for Distributed Robotics (SDR) program. The core challenge for the SDR program was to develop a multi-robot system capable of carrying out a specific mission: to deploy a large number of robots into an unexplored building, map the building interior, detect and track intruders, and transmit all of the above information to a remote operator. To satisfy these requirements, we developed a heterogeneous robot team consisting of approximately 80 robots. We sketch the key technical elements of this team, focusing on the novel aspects, and present selected results from supervised experiments conducted in a 600 m 2 indoor environment.},
author = {Howard, a.},
doi = {10.1177/0278364906065378},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Howard - 2006 - Experiments with a Large Heterogeneous Mobile Robot Team Exploration, Mapping, Deployment and Detection.pdf:pdf},
isbn = {0278364906065},
issn = {0278-3649},
journal = {The International Journal of Robotics Research},
keywords = {exploration,slam},
mendeley-tags = {exploration,slam},
number = {5-6},
pages = {431--447},
title = {{Experiments with a Large Heterogeneous Mobile Robot Team: Exploration, Mapping, Deployment and Detection}},
volume = {25},
year = {2006}
}
@inproceedings{Kortenkamp1992,
author = {Kortenkamp, D. and Baker, L.D. and Weymouth, Terry},
booktitle = {Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems},
doi = {10.1109/IROS.1992.602138},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Kortenkamp, Baker, Weymouth - 1992 - Using Gateways To Build A Route Map.pdf:pdf},
isbn = {0-7803-0738-0},
pages = {2209--2214},
publisher = {IEEE},
title = {{Using Gateways To Build A Route Map}},
url = {http://ieeexplore.ieee.org/document/602138/},
volume = {3},
year = {1992}
}
@article{Kelly2008,
abstract = {We describe an UAV navigation system which combines stereo visual odometry with inertial measurements from an IMU. Our approach fuses the motion estimates from both sensors in an extended Kalman filter to determine vehicle position and attitude. We present results using data from a robotic helicopter, in which the visual and inertial system produced a final position estimate within 1{\%} of the measured GPS position, over a flight distance of more than 400 meters. Our results show that the combination of visual and inertial sensing reduced overall positioning error by nearly an order of magnitude compared to visual odometry alone.},
author = {Kelly, Jonathan and Saripalli, Srikanth and Sukhatme, Gaurav S.},
doi = {10.1007/978-3-540-75404-6_24},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Kelly, Saripalli, Sukhatme - 2008 - Combined visual and inertial navigation for an unmanned aerial vehicle.pdf:pdf},
isbn = {9783540754039},
issn = {16107438},
journal = {Springer Tracts in Advanced Robotics},
pages = {255--264},
title = {{Combined visual and inertial navigation for an unmanned aerial vehicle}},
volume = {42},
year = {2008}
}
@article{VanBreugel2012,
abstract = {Landing behavior is one of the most critical, yet least studied, aspects of insect flight. In order to land safely, an insect must recognize a visual feature, navigate towards it, decelerate, and extend its legs in preparation for touchdown. Although previous studies have focused on the visual stimuli that trigger these different components, the complete sequence has not been systematically studied in a free-flying animal. Using a real-time 3D tracking system in conjunction with high speed digital imaging, we were able to capture the landing sequences of fruit flies (Drosophila melanogaster) from the moment they first steered toward a visual target, to the point of touchdown. This analysis was made possible by a custom-built feedback system that actively maintained the fly in the focus of the high speed camera. The results suggest that landing is composed of three distinct behavioral modules. First, a fly actively turns towards a stationary target via a directed body saccade. Next, it begins to decelerate at a point determined by both the size of the visual target and its rate of expansion on the retina. Finally, the fly extends its legs when the visual target reaches a threshold retinal size of approximately 60 deg. Our data also let us compare landing sequences with flight trajectories that, although initially directed toward a visual target, did not result in landing. In these 'fly-by' trajectories, flies steer toward the target but then exhibit a targeted aversive saccade when the target subtends a retinal size of approximately 33 deg. Collectively, the results provide insight into the organization of sensorimotor modules that underlie the landing and search behaviors of insects.},
author = {van Breugel, Floris and Dickinson, Michael H.},
doi = {10.1242/jeb.066498},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/van Breugel, Dickinson - 2012 - The visual control of landing and obstacle avoidance in the fruit fly Drosophila melanogaster.pdf:pdf},
isbn = {0022-0949},
issn = {0022-0949},
journal = {Journal of Experimental Biology},
keywords = {biology,drosophila,expansion rate,experiment,flight control,insect,landing behavior,monocular,nieuwe referenties,obstacle avoidance,obstacle detection,reactive obstacle avoidance,saccade,target orientation},
mendeley-tags = {biology,expansion rate,experiment,monocular,nieuwe referenties,obstacle avoidance,obstacle detection,reactive obstacle avoidance},
number = {11},
pages = {1783--1798},
pmid = {22573757},
title = {{The visual control of landing and obstacle avoidance in the fruit fly Drosophila melanogaster}},
url = {http://eutils.ncbi.nlm.nih.gov/entrez/eutils/elink.fcgi?dbfrom=pubmed{\&}id=22573757{\&}retmode=ref{\&}cmd=prlinks},
volume = {215},
year = {2012}
}
@article{Durrant-Whyte2006,
abstract = {This tutorial provides an introduction to Simultaneous Localisation and Mapping (SLAM) and the extensive research on SLAM that has been undertaken over the past decade. SLAM is the process by which a mobile robot can build a map of an environment and at the same time use this map to compute it's own location. The past decade has seen rapid and exciting progress in solving the SLAM problem together with many compelling implementations of SLAM methods. Part I of this tutorial (this paper), describes the probabilistic form of the SLAM problem, essential solution methods and significant implementations. Part II of this tutorial will be concerned with recent advances in computational methods and new formulations of the SLAM problem for large scale and complex environments.},
archivePrefix = {arXiv},
arxivId = {there is not},
author = {Durrant-Whyte, Hugh and Bailey, Tim},
doi = {10.1109/MRA.2006.1638022},
eprint = {there is not},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Durrant-Whyte, Bailey - 2006 - Simultaneous localization and mapping (SLAM) part I The Essential Algorithms.pdf:pdf},
isbn = {1070-9932},
issn = {10709932},
journal = {Robotics {\&} Automation Magazine},
keywords = {SLAM,SLAM problem,mobile robots,simultaneous localization and mapping problem},
mendeley-tags = {SLAM},
pages = {99--110},
pmid = {8460702},
title = {{Simultaneous localization and mapping (SLAM): part I The Essential Algorithms}},
volume = {2},
year = {2006}
}
@article{Tippetts2011,
author = {Tippetts, Beau J and Lee, Dah-jye and Archibald, James K and Lillywhite, Kirt D},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Tippetts et al. - 2011 - Dense Disparity Real-Time Stereo Vision Algorithm for Resource-Limited Systems.pdf:pdf},
number = {10},
pages = {1547--1555},
title = {{Dense Disparity Real-Time Stereo Vision Algorithm for Resource-Limited Systems}},
volume = {21},
year = {2011}
}
@incollection{Kosecka2003,
abstract = {Man made indoor environments possess regularities, which can be efficiently exploited in automated model acquisition by means of visual sensing. In this context we propose an approach for inferring a topological model of an environment from images or the video stream captured by a mobile robot during exploration. The proposed model consists of a set of locations and neighborhood relationships between them. Initially each location in the model is represented by a collection of similar, temporally adjacent views, with the similarity defined according to a simple appearance based distance measure. The sparser representation is obtained in a subsequent learning stage by means of learning vector quantization (LVQ). The quality of the model is tested in the context of qualitative localization scheme by means of location recognition: given a new view, the most likely location where that view came from is determined.},
author = {Ko{\v{s}}eck{\'{a}}, J and Zhou, Liang and Barber, P. and Duric, Z.},
booktitle = {Computer Vision and Pattern Recognition, 2003. Proceedings. 2003 IEEE Computer Society Conference on},
doi = {10.1109/CVPR.2003.1211445},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Ko{\v{s}}eck{\'{a}} et al. - 2003 - Qualitative image based localization in indoors environments.pdf:pdf},
isbn = {0-7695-1900-8},
issn = {1063-6919},
keywords = {histogram - edge,histogram - gradient orientation},
mendeley-tags = {histogram - edge,histogram - gradient orientation},
pages = {II--3},
publisher = {IEEE},
title = {{Qualitative image based localization in indoors environments}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-0041941139{\&}partnerID=40{\&}md5=c99beef6d5d7c2a56c700b89af036a5a},
volume = {2},
year = {2003}
}
@inbook{Zach2007,
abstract = {Variational methods are among the most successful approaches to calculate the optical flow between two image frames. A particularly appealing formulation is based on total variation (TV) regularization and the robust L 1 norm in the data fidelity term. This formulation can preserve discontinuities in the flow field and offers an increased robustness against illumination changes, occlusions and noise. In this work we present a novel approach to solve the TV-L 1 formulation. Our method results in a very efficient numerical scheme, which is based on a dual formulation of the TV energy and employs an efficient point-wise thresholding step. Additionally, our approach can be accelerated by modern graphics processing units. We demonstrate the real-time performance (30 fps) of our approach for video inputs at a resolution of 320{\{}$\backslash$texttimes{\}}240 pixels.},
address = {Berlin, Heidelberg},
author = {Zach, C and Pock, T and Bischof, H},
booktitle = {Pattern Recognition: 29th DAGM Symposium, Heidelberg, Germany, September 12-14, 2007. Proceedings},
doi = {10.1007/978-3-540-74936-3_22},
editor = {Hamprecht, Fred A and Schn{\"{o}}rr, Christoph and J{\"{a}}hne, Bernd},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Zach, Pock, Bischof - 2007 - A Duality Based Approach for Realtime TV-L 1 Optical Flow.pdf:pdf},
isbn = {978-3-540-74936-3},
pages = {214--223},
publisher = {Springer Berlin Heidelberg},
title = {{A Duality Based Approach for Realtime TV-L 1 Optical Flow}},
url = {https://doi.org/10.1007/978-3-540-74936-3{\_}22},
year = {2007}
}
@inproceedings{Allen2016,
address = {Reston, Virginia},
annote = {Motion planning, weinig detection of avoidance.},
author = {Allen, Ross and Pavone, Marco},
booktitle = {AIAA Guidance, Navigation, and Control Conference},
doi = {10.2514/6.2016-1374},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Allen, Pavone - 2016 - A Real-Time Framework for Kinodynamic Planning with Application to Quadrotor Obstacle Avoidance.pdf:pdf},
isbn = {978-1-62410-389-6},
keywords = {delete},
mendeley-tags = {delete},
month = {jan},
publisher = {American Institute of Aeronautics and Astronautics},
title = {{A Real-Time Framework for Kinodynamic Planning with Application to Quadrotor Obstacle Avoidance}},
url = {http://arc.aiaa.org/doi/10.2514/6.2016-1374},
year = {2016}
}
@inproceedings{Tully2009,
author = {Tully, Stephen and Kantor, George and Choset, Howie and Werner, Felix},
booktitle = {2009 IEEE/RSJ International Conference on Intelligent Robots and Systems},
doi = {10.1109/IROS.2009.5354255},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Tully et al. - 2009 - A multi-hypothesis topological SLAM approach for loop closing on edge-ordered graphs.pdf:pdf},
isbn = {978-1-4244-3803-7},
keywords = {loop closure,topological slam},
mendeley-tags = {loop closure,topological slam},
month = {oct},
pages = {4943--4948},
publisher = {IEEE},
title = {{A multi-hypothesis topological SLAM approach for loop closing on edge-ordered graphs}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5354255},
year = {2009}
}
@incollection{Veit2016,
abstract = {In this work we propose a novel interpretation of residual networks showing that they can be seen as a collection of many paths of differing length. Moreover, residual networks seem to enable very deep networks by leveraging only the short paths during training. To support this observation, we rewrite residual networks as an explicit collection of paths. Unlike traditional models, paths through residual networks vary in length. Further, a lesion study reveals that these paths show ensemble-like behavior in the sense that they do not strongly depend on each other. Finally, and most surprising, most paths are shorter than one might expect, and only the short paths are needed during training, as longer paths do not contribute any gradient. For example, most of the gradient in a residual network with 110 layers comes from paths that are only 10-34 layers deep. Our results reveal one of the key characteristics that seem to enable the training of very deep networks: Residual networks avoid the vanishing gradient problem by introducing short paths which can carry gradient throughout the extent of very deep networks.},
archivePrefix = {arXiv},
arxivId = {1605.06431},
author = {Veit, Andreas and Wilber, Michael and Belongie, Serge},
booktitle = {Advances in Neural Information Processing Systems 29},
eprint = {1605.06431},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Veit, Wilber, Belongie - 2016 - Residual Networks Behave Like Ensembles of Relatively Shallow Networks.pdf:pdf},
issn = {10495258},
pages = {550--558},
publisher = {Curran Associates, Inc.},
title = {{Residual Networks Behave Like Ensembles of Relatively Shallow Networks}},
url = {http://papers.nips.cc/paper/6556-residual-networks-behave-like-ensembles-of-relatively-shallow-networks.pdf},
year = {2016}
}
@article{Kendoul2012,
author = {Kendoul, Farid},
doi = {10.1002/rob},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Kendoul - 2012 - Survey of Advances in Guidance, Navigation, and Control of Unmanned Rotorcraft Systems.pdf:pdf},
journal = {Journal of Field Robotics},
number = {2},
pages = {315--378},
title = {{Survey of Advances in Guidance, Navigation, and Control of Unmanned Rotorcraft Systems}},
volume = {29},
year = {2012}
}
@article{Schmid2000,
abstract = {Many different low-level feature detectors exist and it is widely agreed that the evaluation of detectors is important. In this paper we introduce two evaluation criteria for interest points' repeatability rate and information content. Repeatability rate evaluates the geometric stability under different transformations. Information content measures the distinctiveness of features. Different interest point detectors are compared using these two criteria. We determine which detector gives the best results and show that it satisfies the criteria well.},
author = {Schmid, Cordelia and Mohr, Roger and Bauckhage, Christian},
doi = {10.1023/A:1008199403446},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Schmid, Mohr, Bauckhage - 2000 - Evaluation of interest point detectors.pdf:pdf},
isbn = {0920-5691},
issn = {09205691},
journal = {International Journal of Computer Vision},
keywords = {comparison of detectors,information content,interest points,quantitative evaluation,repeatability},
number = {2},
pages = {151--172},
title = {{Evaluation of interest point detectors}},
volume = {37},
year = {2000}
}
@article{Denuelle2015b,
abstract = {This paper presents the development and flight testing of a novel and efficient view-based method for the navigation and control of rotorcraft unmanned aerial vehicles (UAVs) in unknown, GPS-denied, outdoor environments. At the core of our system is the Image Coordinates Extrapolation (ICE) algorithm which estimates the UAV 3D position and velocity in real-time by computing the pixel-wise difference between the current view (panoramic image) and a snapshot taken at a reference location (e.g., the hovering position). When combined with a PID flight controller, this simple, but effective algorithm allows a rotorcraft UAV to achieve stable and drift-free hover using image differences only, without the need to track features or to compute optic flow. The performance of our approach is evaluated in closed-loop flight tests on a custom-built quadrotor equipped with an onboard panoramic vision system and flight computer.},
author = {Denuelle, Aymeric and Thurrowgood, Saul and Strydom, Reuben and Kendoul, Farid and Srinivasan, Mandyam V.},
doi = {10.1109/ICUAS.2015.7152400},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Denuelle et al. - 2015 - Biologically-inspired visual stabilization of a rotorcraft UAV in unknown outdoor environments.pdf:pdf},
isbn = {9781479960101},
journal = {2015 International Conference on Unmanned Aircraft Systems, ICUAS 2015},
pages = {1084--1093},
title = {{Biologically-inspired visual stabilization of a rotorcraft UAV in unknown outdoor environments}},
year = {2015}
}
@incollection{Franz2008,
author = {Franz, Matthias O. and St{\"{u}}rzl, Wolfgang and H{\"{u}}bner, Wolfgang and Mallot, Hanspeter A.},
booktitle = {Robotics and Cognitive Approaches to Spatial Mapping},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Franz et al. - 2008 - A Robot System for Biomimetic Navigation – From Snapshots to Metric Embeddings of View Graphs.pdf:pdf},
pages = {297--314},
title = {{A Robot System for Biomimetic Navigation – From Snapshots to Metric Embeddings of View Graphs}},
year = {2008}
}
@article{Trautman2010,
abstract = {In this paper, we study the safe navigation of a mobile robot through crowds of dynamic agents with uncertain trajectories. Existing algorithms suffer from the {\&}{\#}x201C;freezing robot{\&}{\#}x201D; problem: once the environment surpasses a certain level of complexity, the planner decides that all forward paths are unsafe, and the robot freezes in place (or performs unnecessary maneuvers) to avoid collisions. Since a feasible path typically exists, this behavior is suboptimal. Existing approaches have focused on reducing the predictive uncertainty for individual agents by employing more informed models or heuristically limiting the predictive covariance to prevent this overcautious behavior. In this work, we demonstrate that both the individual prediction and the predictive uncertainty have little to do with the frozen robot problem. Our key insight is that dynamic agents solve the frozen robot problem by engaging in {\&}{\#}x201C;joint collision avoidance{\&}{\#}x201D;: They cooperatively make room to create feasible trajectories. We develop IGP, a nonparametric statistical model based on dependent output Gaussian processes that can estimate crowd interaction from data. Our model naturally captures the non-Markov nature of agent trajectories, as well as their goal-driven navigation. We then show how planning in this model can be efficiently implemented using particle based inference. Lastly, we evaluate our model on a dataset of pedestrians entering and leaving a building, first comparing the model with actual pedestrians, and find that the algorithm either outperforms human pedestrians or performs very similarly to the pedestrians. We also present an experiment where a covariance reduction method results in highly overcautious behavior, while our model performs desirably.},
author = {Trautman, Peter and Krause, Andreas},
doi = {10.1109/IROS.2010.5654369},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Trautman, Krause - 2010 - Unfreezing the robot Navigation in dense, interacting crowds.pdf:pdf},
isbn = {9781424466757},
issn = {2153-0858},
journal = {IEEE/RSJ 2010 International Conference on Intelligent Robots and Systems, IROS 2010 - Conference Proceedings},
keywords = {Learning and Adaptive Systems,Path Planning for Multiple Mobile Robot Systems,Social Human-Robot Interaction},
pages = {797--803},
title = {{Unfreezing the robot: Navigation in dense, interacting crowds}},
year = {2010}
}
@article{Lulham2011,
abstract = {Psychological experiments have shown that the capacity of the brain for discriminating visual stimuli as novel or familiar is almost limitless. Neurobiological studies have established that the perirhinal cortex is critically involved in both familiarity discrimination and feature extraction. However, opinion is divided as to whether these two processes are performed by the same neurons. Previously proposed models have been unable to simultaneously extract features and discriminate familiarity for large numbers of stimuli. We show that a well-known model of visual feature extraction, Infomax, can simultaneously perform familiarity discrimination and feature extraction efficiently. This model has a significantly larger capacity than previously proposed models combining these two processes, particularly when correlation exists between inputs, as is the case in the perirhinal cortex. Furthermore, we show that once the model fully extracts features, its ability to perform familiarity discrimination increases markedly.},
author = {Lulham, Andrew and Bogacz, Rafal and Vogt, Simon and Brown, Malcolm W},
doi = {10.1162/NECO_a_00097},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Lulham et al. - 2011 - An Infomax algorithm can perform both familiarity discrimination and feature extraction in a single network.pdf:pdf},
isbn = {0899-7667},
issn = {0899-7667},
journal = {Neural computation},
keywords = {Infomax},
mendeley-tags = {Infomax},
number = {4},
pages = {909--926},
pmid = {21222523},
title = {{An Infomax algorithm can perform both familiarity discrimination and feature extraction in a single network.}},
volume = {23},
year = {2011}
}
@inproceedings{Shan2016,
author = {Shan, Mo and Bi, Yingcai and Qin, Hailong and Li, Jiaxin and Gao, Zhi and Lin, Feng and Chen, Ben M},
booktitle = {Industrial Electronics Society, IECON 2016-42nd Annual Conference of the IEEE},
doi = {10.1109/IECON.2016.7793198},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Shan et al. - 2016 - A Brief Survey of Visual Odometry for Micro Aerial Vehicles.pdf:pdf},
isbn = {9781509034741},
keywords = {abstract,and compre-,applications,for a range of,has experienced a,rapid growth,recently,this survey paper attempts,to provide a timely,visual odometry,vo,which makes it viable},
pages = {6049--6054},
publisher = {IEEE},
title = {{A Brief Survey of Visual Odometry for Micro Aerial Vehicles}},
year = {2016}
}
@article{Cherabier2017,
abstract = {Point cloud is an important type of geometric data structure. Due to its irregular format, most researchers transform such data to regular 3D voxel grids or collections of images. This, however, renders data unnecessarily voluminous and causes issues. In this paper, we design a novel type of neural network that directly consumes point clouds and well respects the permutation invariance of points in the input. Our network, named PointNet, provides a unified architecture for applications ranging from object classification, part segmentation, to scene semantic parsing. Though simple, PointNet is highly efficient and effective. Empirically, it shows strong performance on par or even better than state of the art. Theoretically, we provide analysis towards understanding of what the network has learnt and why the network is robust with respect to input perturbation and corruption.},
archivePrefix = {arXiv},
arxivId = {1612.00593},
author = {Qi, Charles R. and Su, Hao and Mo, Kaichun and Guibas, Leonidas J.},
doi = {10.1109/3DV.2016.68},
eprint = {1612.00593},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Qi et al. - 2016 - PointNet Deep Learning on Point Sets for 3D Classification and Segmentation.pdf:pdf},
isbn = {9781509054077},
issn = {1063-6919},
journal = {Cvpr},
keywords = {dense 3D reconstruction,semantic segmentation},
month = {dec},
pages = {601--610},
title = {{PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation}},
url = {http://arxiv.org/abs/1612.00593},
year = {2016}
}
@article{Gauthier2016,
abstract = {How do we recognize objects despite changes in their appearance? The past three decades have been witness to intense debates regarding both whether objects are encoded invariantly with respect to viewing conditions and whether specialized, separable mechanisms are used for the recognition of different object categories. We argue that such dichotomous debates ask the wrong question. Much more important is the nature of object representations: What are features that enable invariance or differential processing between categories? Although the nature of object features is still an unanswered question, new methods for connecting data to models show significant potential for helping us to better understand neural codes for objects. Most prominently, new approaches to analyzing data from functional magnetic resonance imaging, including neural decoding and representational similarity analysis, and new computational models of vision, including convolutional neural networks, have enabled a much more nuanced unders...},
author = {Gauthier, Isabel and Tarr, Michael J.},
doi = {10.1146/annurev-vision-111815-114621},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Gauthier, Tarr - 2016 - Visual Object Recognition Do We (Finally) Know More Now Than We Did.pdf:pdf},
isbn = {978-0-8243-5102-1},
issn = {2374-4642},
journal = {Annual Review of Vision Science},
keywords = {category selectivity,decoding,deep neural networks,face recognition,invariance,object recognition},
number = {1},
pages = {377--396},
pmid = {16903801},
title = {{Visual Object Recognition: Do We (Finally) Know More Now Than We Did?}},
url = {http://www.annualreviews.org/doi/10.1146/annurev-vision-111815-114621},
volume = {2},
year = {2016}
}
@incollection{Ulrich2000,
abstract = {This paper presents a new appearance-based place recognition system for topological localization. The method uses a panoramic vision system to sense the environment. Color images are classified in real-time based on nearest-neighbor learning, image histogram matching, and a simple voting scheme. The system has been evaluated with eight cross-sequence tests in four unmodified environments, three indoors and one outdoors. In all eight cases, the system successfully tracked the mobile robot's position. The system correctly classified between 87{\%} and 98{\%} of the input color images. For the remaining images, the system was either momentarily confused or uncertain, but never classified an image incorrectly},
annote = {todo add to place recognition},
author = {Ulrich, I and Nourbakhsh, I},
booktitle = {Robotics and Automation, 2000. Proceedings. ICRA'00. IEEE International Conference on},
doi = {10.1109/ROBOT.2000.844734},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Ulrich, Nourbakhsh - 2000 - Appearance-Based Place Recognition for Topological Localization.pdf:pdf},
isbn = {0780358864},
issn = {1050-4729},
keywords = {place recognition,todo},
mendeley-tags = {place recognition,todo},
pages = {1023--1029},
publisher = {IEEE},
title = {{Appearance-Based Place Recognition for Topological Localization}},
volume = {2},
year = {2000}
}
@article{Alcantarilla2012a,
author = {Alcantarilla, Pablo F and Bergasa, Luis M},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Alcantarilla et al. - 2012 - On combining visual SLAM and dense scene flow to increase the robustness of localization and mapping in dyn.pdf:pdf},
isbn = {9781467314053},
pages = {1290--1297},
title = {{On Combining Visual SLAM and Dense Scene Flow to Increase the Robustness of Localization and Mapping in Dynamic Environments}},
year = {2012}
}
@article{McGuire2018a,
abstract = {This paper presents a literature survey and a comparative study of Bug Algorithms, with the goal of investigating their potential for robotic navigation. At first sight, these methods seem to provide an efficient navigation paradigm, ideal for implementations on tiny robots with limited resources. Closer inspection, however, shows that many of these Bug Algorithms assume perfect global position estimate of the robot which in GPS-denied environments implies considerable expenses of computation and memory -- relying on accurate Simultaneous Localization And Mapping (SLAM) or Visual Odometry (VO) methods. We compare a selection of Bug Algorithms in a simulated robot and environment where they endure different types noise and failure-cases of their on-board sensors. From the simulation results, we conclude that the implemented Bug Algorithms' performances are sensitive to many types of sensor-noise, which was most noticeable for odometry-drift. This raises the question if Bug Algorithms are suitable for real-world, on-board, robotic navigation as is. Variations that use multiple sensors to keep track of their progress towards the goal, were more adept in completing their task in the presence of sensor-failures. This shows that Bug Algorithms must spread their risk, by relying on the readings of multiple sensors, to be suitable for real-world deployment.},
archivePrefix = {arXiv},
arxivId = {1808.05050},
author = {McGuire, Kimberly and de Croon, Guido and Tuyls, Karl},
eprint = {1808.05050},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/McGuire, de Croon, Tuyls - 2018 - A Comparative Study of Bug Algorithms for Robot Navigation.pdf:pdf},
keywords = {bug algorithms,comparative study,indoor navigation,limited sensing,robotic navigation},
title = {{A Comparative Study of Bug Algorithms for Robot Navigation}},
url = {http://arxiv.org/abs/1808.05050},
year = {2018}
}
@article{Scherer2008,
author = {Scherer, Sebastian and Singh, Sanjiv and Chamberlain, Lyle and Elgersma, Mike},
doi = {10.1177/0278364908090949},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Scherer et al. - 2008 - Flying Fast and Low Among Obstacles Methodology and Experiments.pdf:pdf},
isbn = {0278364908090},
journal = {The International Journal of Robotics Research},
keywords = {aerial robotics,learning},
number = {5},
pages = {549--574},
title = {{Flying Fast and Low Among Obstacles: Methodology and Experiments}},
volume = {27},
year = {2008}
}
@article{Corke1993,
abstract = {This paper attempts to present a comprehensive summary of research results in the use of visual information to control robot manipulators and related mechanisms. An extensive bibliography is provided which also includes important papers from the elemental disciplines upon which visual servoing is based. The research results are discussed in terms of historical context, commonality of function, algorithmic approach and method of implementation. 1 Introduction This paper presents the history, and reviews current research into the use of visual information for the control of robot manipulators and mechanisms. Visual control of manipulators promises substantial advantages when working with targets whose position is unknown, or with manipulators which may be flexible or inaccurate. The reported use of visual information to guide robots, or more generally mechanisms, is quite extensive and encompasses manufacturing applications, teleoperation, missile tracking cameras, fruit picking as well...},
author = {Corke, P},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Corke - 1993 - Visual control of robot manipulators - a review.pdf:pdf},
journal = {Visual Servoing},
pages = {1--31},
title = {{Visual control of robot manipulators - a review}},
volume = {7},
year = {1993}
}
@phdthesis{Jensfelt2001,
abstract = {This thesis deals with all aspects of mobile robot localization for indoor applications. The problems span from tracking the position given an initial estimate, over finding it without any prior position knowledge, to automatically building a representation of the environment while performing localization. The theme is the use of minimalistic models which capture the large scale structures of the environment, such as the dominant walls, to provide scalable and low-complexity solutions.},
author = {Jensfelt, Petric},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Jensfelt - 2001 - Approches to Mobile robot localization in indoor navigation.pdf:pdf},
title = {{Approches to Mobile robot localization in indoor navigation}},
url = {http://nyx-www.informatik.uni-bremen.de/347/1/jensfelt{\_}thesis{\_}01.pdf},
year = {2001}
}
@inproceedings{DeWagter2005,
author = {de Wagter, C. and Mulder, J.A.},
booktitle = {AIAA Guidance, Navigation, and Control Conference and Exhibit},
doi = {10.2514/6.2005-5872},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/de Wagter, Mulder - 2005 - Towards Vision-Based UAV Situation Awareness.pdf:pdf},
number = {August},
pages = {1--16},
title = {{Towards Vision-Based UAV Situation Awareness}},
year = {2005}
}
@article{Scaramuzza2011b,
abstract = {This paper presents a new method to estimate the relative motion of a vehicle from images of a single cam-era. The computational cost of the algorithm is limited only by the feature extraction and matching process, as the out-lier removal and the motion estimation steps take less than a fraction of millisecond with a normal laptop computer. The biggest problem in visual motion estimation is data asso-ciation; matched points contain many outliers that must be detected and removed for the motion to be accurately esti-mated. In the last few years, a very established method for removing outliers has been the " 5-point RANSAC " algo-rithm which needs a minimum of 5 point correspondences to estimate the model hypotheses. Because of this, however, it can require up to several hundreds of iterations to find a set of points free of outliers. In this paper, we show that by ex-ploiting the nonholonomic constraints of wheeled vehicles it is possible to use a restrictive motion model which allows us to parameterize the motion with only 1 point correspon-dence. Using a single feature correspondence for motion es-timation is the lowest model parameterization possible and results in the two most efficient algorithms for removing out-liers: 1-point RANSAC and histogram voting. To support our method we run many experiments on both synthetic and real data and compare the performance with a state-of-the-art approach. Finally, we show an application of our method to visual odometry by recovering a 3 Km trajectory in a clut-tered urban environment and in real-time.},
author = {Scaramuzza, Davide},
doi = {10.1007/s11263-011-0441-3},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Scaramuzza - 2011 - 1-Point-RANSAC Structure from Motion for Vehicle-Mounted Cameras by Exploiting Non-holonomic Constraints.pdf:pdf},
issn = {0920-5691},
journal = {International Journal of Computer Vision},
keywords = {low complexity,state estimation},
mendeley-tags = {low complexity,state estimation},
month = {oct},
number = {1},
pages = {74--85},
title = {{1-Point-RANSAC Structure from Motion for Vehicle-Mounted Cameras by Exploiting Non-holonomic Constraints}},
url = {http://link.springer.com/10.1007/s11263-011-0441-3},
volume = {95},
year = {2011}
}
@article{Siagian2007,
abstract = {Abstract We describe and validate a simple context-based scene recognition algorithm for mobile robotics applications. The system can differentiate outdoor scenes from various sites on a college campus using a multiscale set of},
author = {Siagian, Christian and Itti, Laurent},
doi = {10.1109/TPAMI.2007.40},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Siagian, Itti - 2007 - Rapid Biologically-Inspired Scene Classification Using Features Shared with Visual Attention.pdf:pdf},
issn = {0162-8828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
month = {feb},
number = {2},
pages = {300--312},
title = {{Rapid Biologically-Inspired Scene Classification Using Features Shared with Visual Attention}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=4042704{\%}0Apapers://27662af5-e24b-497a-8780-4dcec04ea037/Paper/p1790 http://ieeexplore.ieee.org/document/4042704/},
volume = {29},
year = {2007}
}
@inproceedings{Merrell2004,
author = {Merrell, Paul Clark and Lee, Dah-jye and Beard, Randal W},
booktitle = {Proc. SPIE 5609, Mobile Robots XVII},
doi = {10.1117/12.571554},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Merrell, Lee, Beard - 2004 - Obstacle Avoidance for Unmanned Air Vehicles Using Optical Flow Probability Distributions.pdf:pdf},
keywords = {motion parallax,obstacle avoidance,optical flow,structure from motion,unmanned air vehicles},
title = {{Obstacle Avoidance for Unmanned Air Vehicles Using Optical Flow Probability Distributions}},
year = {2004}
}
@article{Holz2013,
abstract = {Limiting factors for increasing autonomy and complexity of truly autonomous systems (without external sensing and control) are onboard sensing and onboard processing power. In this paper, we propose a hardware setup and processing pipeline that allows a fully autonomous UAV to perceive obstacles in (almost) all directions in its surroundings. Different sensor modalities are applied in order take into account the different characteristics of obstacles that can commonly be found in typical UAV applications. We provide a complete overview on the implemented system and present experimental results as a proof of concept.},
author = {Holz, D. and Nieuwenhuisen, M. and Droeschel, D. and Schreiber, M. and Behnke, S.},
doi = {10.5194/isprsarchives-XL-1-W2-201-2013},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Holz et al. - 2013 - Towards Multimodal Omnidirectional Obstacle Detection for Autonomous Unmanned Aerial Vehicles.pdf:pdf},
issn = {1682-1777},
journal = {ISPRS - International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
keywords = {3d laser scanner,autonomous uavs,multimodal sensor setup},
number = {September},
pages = {201--206},
title = {{Towards Multimodal Omnidirectional Obstacle Detection for Autonomous Unmanned Aerial Vehicles}},
url = {http://www.int-arch-photogramm-remote-sens-spatial-inf-sci.net/XL-1-W2/201/2013/},
volume = {XL-1/W2},
year = {2013}
}
@inproceedings{Milford2012,
abstract = { Abstract— Learning and then recognizing a route, whether travelled during the day or at night, in clear or inclement weather, and in summer or winter is a challenging task for state of the art algorithms in computer vision and robotics. In this paper, we present a new approach to visual navigation under changing conditions dubbed SeqSLAM. Instead of calculating the single location most likely given a current image, our approach calculates the best candidate matching location within every local navigation sequence. Localization is then achieved by recognizing coherent sequences of these " local best matches " . This approach removes the need for global matching performance by the vision front-end – instead it must only pick the best match within any short sequence of images. The approach is applicable over environment changes that render traditional feature-based techniques ineffective. Using two car-mounted camera datasets we demonstrate the effectiveness of the algorithm and compare it to one of the most successful feature-based SLAM algorithms, FAB-MAP. The perceptual change in the datasets is extreme; repeated traverses through environments during the day and then in the middle of the night, at times separated by months or years and in opposite seasons, and in clear weather and extremely heavy rain. While the feature-based method fails, the sequence-based algorithm is able to match trajectory segments at 100{\%} precision with recall rates of up to 60{\%}.},
annote = {Recognize scenes in varying conditions while avoiding aliasing},
author = {Milford, Michael J. and Wyeth, Gordon F.},
booktitle = {2012 IEEE International Conference on Robotics and Automation},
doi = {10.1109/ICRA.2012.6224623},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Milford, Wyeth - 2012 - SeqSLAM Visual route-based navigation for sunny summer days and stormy winter nights.pdf:pdf},
isbn = {978-1-4673-1405-3},
month = {may},
pages = {1643--1649},
publisher = {IEEE},
title = {{SeqSLAM: Visual route-based navigation for sunny summer days and stormy winter nights}},
url = {http://ieeexplore.ieee.org/document/6224623/},
year = {2012}
}
@article{Jacobs2003,
abstract = {In the parallel map theory, the hippocampus encodes space with 2 mapping systems. The bearing map is constructed primarily in the dentate gyrus from directional cues such as stimulus gradients. The sketch map is constructed within the hippocampus proper from positional cues. The integrated map emerges when data from the bearing and sketch maps are combined. Because the component maps work in parallel, the impairment of one can reveal residual learning by the other. Such parallel function may explain paradoxes of spatial learning, such as learning after partial hippocampal lesions, taxonomic and sex differences in spatial learning, and the function of hippocampal neurogenesis. By integrating evidence from physiology to phylogeny, the parallel map theory offers a unified explanation for hippocampal function.},
annote = {Hippocampus.
Parallel map theory.
Bearing map.},
author = {Jacobs, Lucia F and Schenk, Fran{\c{c}}oise},
doi = {10.1037/0033-295X.110.2.285},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Jacobs, Schenk - 2003 - Unpacking the cognitive map the parallel map theory of hippocampal function.pdf:pdf},
isbn = {1939-1471$\backslash$n0033-295X},
issn = {0033-295X},
journal = {Psychological review},
keywords = {biology},
mendeley-tags = {biology},
number = {2},
pages = {285--315},
pmid = {12747525},
title = {{Unpacking the cognitive map: the parallel map theory of hippocampal function.}},
volume = {110},
year = {2003}
}
@inproceedings{Gomez-Ojeda2016,
author = {Gomez-Ojeda, Ruben and Briales, Jesus and Gonzalez-Jimenez, Javier},
booktitle = {Intelligent Robots and Systems (IROS), 2016 IEEE/RSJ International Conference on},
doi = {10.1109/IROS.2016.7759620},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Gomez-Ojeda, Briales, Gonzalez-Jimenez - 2016 - PL-SVO Semi-Direct Monocular Visual Odometry by Combining Points and Line Segments.pdf:pdf},
isbn = {9781509037629},
pages = {4211--4216},
publisher = {IEEE},
title = {{PL-SVO: Semi-Direct Monocular Visual Odometry by Combining Points and Line Segments}},
year = {2016}
}
@article{Lee2011,
author = {Lee, Jeong-Oog and Lee, Keun-Hwan and Park, Sang-Heon and Im, Sung-Gyu and Park, Jungkeun},
doi = {10.1108/00022661111173270},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Lee et al. - 2011 - Obstacle avoidance for small UAVs using monocular vision.pdf:pdf},
isbn = {0002266111117},
journal = {Aircraft Engineering and Aerospace Technology: An International Journal},
keywords = {collisions,image processing,monocular vision,mops,obstacle avoidance,paper type research paper,sift algorithm,small uavs},
number = {6},
pages = {397--406},
title = {{Obstacle avoidance for small UAVs using monocular vision}},
volume = {83},
year = {2011}
}
@article{Weerasekera2018,
abstract = {We present ``just-in-time reconstruction" as real-time image-guided inpainting of a map with arbitrary scale and sparsity to generate a fully dense depth map for the image. In particular, our goal is to inpaint a sparse map --- obtained from either a monocular visual SLAM system or a sparse sensor --- using a single-view depth prediction network as a virtual depth sensor. We adopt a fairly standard approach to data fusion, to produce a fused depth map by performing inference over a novel fully-connected Conditional Random Field (CRF) which is parameterized by the input depth maps and their pixel-wise confidence weights. Crucially, we obtain the confidence weights that parameterize the CRF model in a data-dependent manner via Convolutional Neural Networks (CNNs) which are trained to model the conditional depth error distributions given each source of input depth map and the associated RGB image. Our CRF model penalises absolute depth error in its nodes and pairwise scale-invariant depth error in its edges, and the confidence-based fusion minimizes the impact of outlier input depth values on the fused result. We demonstrate the flexibility of our method by real-time inpainting of ORB-SLAM, Kinect, and LIDAR depth maps acquired both indoors and outdoors at arbitrary scale and varied amount of irregular sparsity.},
archivePrefix = {arXiv},
arxivId = {1805.04239},
author = {Weerasekera, Chamara Saroj and Dharmasiri, Thanuja and Garg, Ravi and Drummond, Tom and Reid, Ian},
eprint = {1805.04239},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Weerasekera et al. - 2018 - Just-in-Time Reconstruction Inpainting Sparse Maps using Single View Depth Predictors as Priors.pdf:pdf},
journal = {arXiv preprint arXiv:1805.04239},
title = {{Just-in-Time Reconstruction: Inpainting Sparse Maps using Single View Depth Predictors as Priors}},
year = {2018}
}
@book{LaValle2006,
author = {LaValle, Steven Michael},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/LaValle - 2006 - Planning algorithms.pdf:pdf},
isbn = {0521862051},
publisher = {Cambridge University Press},
title = {{Planning algorithms}},
url = {http://planning.cs.uiuc.edu/},
year = {2006}
}
@article{Brooks1986,
annote = {Layered control.
Subsumption architecture.},
author = {Brooks, R.},
doi = {10.1109/JRA.1986.1087032},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Brooks - 1986 - A robust layered control system for a mobile robot.pdf:pdf},
issn = {0882-4967},
journal = {IEEE Journal on Robotics and Automation},
number = {1},
pages = {14--23},
title = {{A robust layered control system for a mobile robot}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1087032},
volume = {2},
year = {1986}
}
@article{Celik2013,
abstract = {This paper presents a novel indoor navigation and ranging strategy via monocular camera. By exploiting the architectural orthogonality of the indoor environments, we introduce a new method to estimate range and vehicle states from a monocular camera for vision-based SLAM. The navigation strategy assumes an indoor or indoor-like manmade environment whose layout is previously unknown, GPS-denied, representable via energy based feature points, and straight architectural lines. We experimentally validate the proposed algorithms on a fully self-contained microaerial vehicle (MAV) with sophisticated on-board image processing and SLAM capabilities. Building and enabling such a small aerial vehicle to fly in tight corridors is a significant technological challenge, especially in the absence of GPS signals and with limited sensing options. Experimental results show that the system is only limited by the capabilities of the camera and environmental entropy.},
author = {{\c{C}}elik, Koray and Somani, Arun K.},
doi = {10.1155/2013/374165},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/{\c{C}}elik, Somani - 2013 - Monocular Vision SLAM for Indoor Aerial Vehicles.pdf:pdf},
issn = {2090-0147},
journal = {Journal of Electrical and Computer Engineering},
pages = {1--15},
title = {{Monocular Vision SLAM for Indoor Aerial Vehicles}},
url = {http://www.hindawi.com/journals/jece/2013/374165/},
volume = {2013},
year = {2013}
}
@article{Pilzer2018a,
abstract = {While recent deep monocular depth estimation approaches based on supervised regression have achieved remarkable performance, costly ground truth annotations are required during training. To cope with this issue, in this paper we present a novel unsupervised deep learning approach for predicting depth maps and show that the depth estimation task can be effectively tackled within an adversarial learning framework. Specifically, we propose a deep generative network that learns to predict the correspondence field i.e. the disparity map between two image views in a calibrated stereo camera setting. The proposed architecture consists of two generative sub-networks jointly trained with adversarial learning for reconstructing the disparity map and organized in a cycle such as to provide mutual constraints and supervision to each other. Extensive experiments on the publicly available datasets KITTI and Cityscapes demonstrate the effectiveness of the proposed model and competitive results with state of the art methods. The code and trained model are available on https://github.com/andrea-pilzer/unsup-stereo-depthGAN.},
archivePrefix = {arXiv},
arxivId = {1807.10915},
author = {Pilzer, Andrea and Xu, Dan and Puscas, Mihai Marian and Ricci, Elisa and Sebe, Nicu},
eprint = {1807.10915},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Pilzer et al. - 2018 - Unsupervised Adversarial Depth Estimation using Cycled Generative Networks.pdf:pdf},
journal = {arXiv preprint arXiv:1807.10915},
title = {{Unsupervised Adversarial Depth Estimation using Cycled Generative Networks}},
year = {2018}
}
@article{Lambrinos1998,
abstract = {Some insects, such as desert ants, employ visual homing strategies for returning to important places in their environment. In this study, two models which reproduce aspects of the insects{\&}{\#}039; navigation behavior were implemented and tested on two mobile robots. The rst model, the $\backslash$snapshot model", is based on the assumption that insects store a snapshot of the surroundings of the target location, and derive a home direction by comparing the current image with this snapshot. The snapshot model was adapted for robot navigation, extended with image processing routines, and tested on the mobile robot Sahabot 2 in the habitat of desert ants. The second model is the $\backslash$average landmark vector model", an extremely parsimonious model related to the snapshot model. In this model, only a single vector has to be stored to characterize the target location, and the home direction is obtained from the difference of two such vectors. The average landmark vector model was implemented in analog hardware and tested on a robot in a laboratory environment. The precision of the visual homing achieved in the experiments with the Sahabot 2 is comparable to that of desert ants, which conrms that the snapshot model is an appropriate model of insect navigation also under real-world conditions. The successful operation of the analog implementation of the average landmark vector model allows to de-ne a lower limit of complexity for landmark navigation methods, and at the same time provides indications about the structure and complexity of neural circuits underlying landmark navigation abilities. Both series of experiments demonstrate that the parsimonious navigation strategies used by insects might be used as a guideline for the design of algorithms for robot navigation.},
author = {Lambrinos, Dimitrios and Roggendorf, Thorsten and Pfeifer, Rolf and Dimitrios, Lambrinos and Thorsten, Roggendorf and Rolf, Pfeifer},
doi = {citeulike-article-id:4211894},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Lambrinos et al. - 1998 - Insect Strategies of Visual Homing in Mobile Robots.pdf:pdf},
journal = {Biorobotics - Methods and Applications. AAAI Press},
keywords = {alv,biology,homing,robot},
mendeley-tags = {biology,homing},
pages = {37--66},
title = {{Insect Strategies of Visual Homing in Mobile Robots}},
url = {http://europepmc.org/abstract/CIT/235693{\%}5Cnhttp://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.30.5189},
year = {1998}
}
@article{Wystrach2013,
abstract = {Ants can use visual information to guide long idiosyncratic routes and accurately pinpoint locations in complex natural environments. It has often been assumed that the world knowledge of these foragers consists of multiple discrete views that are retrieved sequentially for breaking routes into sections controlling approaches to a goal. Here we challenge this idea using a model of visual navigation that does not store and use discrete views to replicate the results from paradigmatic experiments that have been taken as evidence that ants navigate using such discrete snapshots. Instead of sequentially retrieving views, the proposed architecture gathers information from all experienced views into a single memory network, and uses this network all along the route to determine the most familiar heading at a given location. This algorithm is consistent with the navigation of ants in both laboratory and natural environments, and provides a parsimonious solution to deal with visual information from multiple locations.},
author = {Wystrach, Antoine and Mangan, Michael and Philippides, Andrew and Graham, Paul},
doi = {10.1242/jeb.082941},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Wystrach et al. - 2013 - Snapshots in ants New interpretations of paradigmatic experiments.pdf:pdf},
isbn = {0022-0949},
issn = {0022-0949},
journal = {The Journal of experimental biology},
keywords = {Animals,Ants,Ants: physiology,Biological,Memory,Memory: physiology,Models,Reproducibility of Results,Visual Perception,Visual Perception: physiology},
number = {January},
pages = {1766--70},
pmid = {23348949},
title = {{Snapshots in ants? New interpretations of paradigmatic experiments}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/23348949{\%}5Cnhttp://jeb.biologists.org/cgi/doi/10.1242/jeb.082941},
volume = {216},
year = {2013}
}
@incollection{Brenner2018,
abstract = {Depth perception is the ability to see the three-dimensional volume of objects and the spatial layout of objects relative to one another and the viewer. Humans accomplish depth perception using a variety of cues, including some based on how the eyes and brain function in concert and others that rely on lawful regularities in the environment. The topics emphasized in this article are consistent with a current imbalance in scientific understanding, which is that much more is known about the specific cues involved than about how the brain uses these cues to achieve depth perception.},
address = {New York},
author = {Brenner, Eli and Smeets, Jeroen B.J.},
booktitle = {Stevens' Handbook of Experimental Psychology and Cognitive Neuroscience},
chapter = {Depth Perc},
doi = {10.1016/B978-0-12-375000-6.00130-0},
edition = {4},
editor = {Wixted, J.T.},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Brenner, Smeets - 2018 - Depth Perception.pdf:pdf},
isbn = {9780123750006},
issn = {00166928},
keywords = {Figure-ground perception,Shape perception,Space perception,Spatial vision,Vision,Visual perception},
pages = {385--414},
pmid = {20842969},
publisher = {John Wiley {\&} Sons},
title = {{Depth Perception}},
year = {2018}
}
@article{Zufferey2006,
abstract = {This paper uses optic flow measurements to mimic the saccade movements$\backslash$nof flys. {\%} A 30-gram indoor flyer is used with two 1-D optic flow$\backslash$ncameras and a single rate gyro used to measure yaw rate. {\%} The 1-D$\backslash$ncamera arrays are pointed 45 degrees from the nose. {\%} The paper gives$\backslash$na simple optic flow computation algorithm for the 1-D array. {\%} The$\backslash$ncontrol strategy is to fly straight until the optic flow sensors$\backslash$nindicate an emminant collision, and then to perform a sensor-less$\backslash$nturn for a certain period of time, and then to resume straight flight.$\backslash$nStraight flight is maintained by wrapping proportional control around$\backslash$nthe yaw rate gyro. Manual input is used to maintain speed and altitude.$\backslash$nThe control strategy is extremely simple but avoids collisions.},
author = {Zufferey, Jean Christophe and Floreano, Dario},
doi = {10.1109/TRO.2005.858857},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Zufferey, Floreano - 2006 - Fly-inspired visual steering of an ultralight indoor aircraft.pdf:pdf},
isbn = {1552-3098},
issn = {15523098},
journal = {IEEE Transactions on Robotics},
keywords = {Collision avoidance,Indoor flying robot,Optic flow (OF),Steering control},
number = {1},
pages = {137--146},
title = {{Fly-inspired visual steering of an ultralight indoor aircraft}},
volume = {22},
year = {2006}
}
@article{Argamon-Engelson1998,
abstract = {This paper describes and evaluates an image-based place recognition method for mobile robots based on matching image signatures. The method represents an image as a set of arrays of coarse-scale measurements over subimages. We describe several different measurement functions we devised, and show how they can be used conjunctively to improve recognition accuracy. Experimental results show that this method enables accurate place recognition, comparable to recent results for localization using occupancy grids, and also to recent results for image-based fingerprint recognition.},
author = {Argamon-Engelson, Shlomo},
doi = {10.1016/S0167-8655(98)00074-9},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Argamon-Engelson - 1998 - Using image signatures for place recognition.pdf:pdf},
issn = {01678655},
journal = {Pattern Recognition Letters},
keywords = {Feature extraction,Image matching,Mobile robotics,Place recognition,Signatures},
number = {10},
pages = {941--951},
title = {{Using image signatures for place recognition}},
url = {http://www.sciencedirect.com/science/article/pii/S0167865598000749},
volume = {19},
year = {1998}
}
@article{Hu2018,
abstract = {We revisit the problem of estimating depth of a scene from its single RGB image. Despite the recent success of deep learning based methods, we show that there is still room for improvement in two aspects by training a deep network consisting of two sub-networks; a base network for providing an initial depth estimate, and a refinement network for refining it. First, spatial resolution of the estimated depth maps can be improved using skip connections among the sub-networks which are trained in a sequential fashion. Second, we can improve estimation accuracy of boundaries of objects in scenes by employing the proposed loss functions using depth gradients. Experimental results show that the proposed network and methods improve depth estimation performance of baseline networks, particularly for reconstruction of small objects and refinement of distortion of edges, and outperform the state-of-the-art methods on benchmark datasets.},
archivePrefix = {arXiv},
arxivId = {1803.08673},
author = {Hu, Junjie and Ozay, Mete and Zhang, Yan and Okatani, Takayuki},
eprint = {1803.08673},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Hu et al. - 2018 - Revisiting Single Image Depth Estimation Toward Higher Resolution Maps with Accurate Object Boundaries.pdf:pdf},
journal = {arXiv preprint arXiv:1803.08673},
keywords = {fully convolutional network,gradient-based loss,natural range image statistics,nection,single image depth estimation,skip con-},
title = {{Revisiting Single Image Depth Estimation: Toward Higher Resolution Maps with Accurate Object Boundaries}},
url = {http://arxiv.org/abs/1803.08673},
year = {2018}
}
@article{McGuire2016,
archivePrefix = {arXiv},
arxivId = {1603.07644},
author = {McGuire, K N and de Croon, G.C.H.E. and {De Wagter}, Christophe. and {Remes B.D.W. Tuyls}, K and Kappen, H J},
eprint = {1603.07644},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/McGuire et al. - 2016 - Local Histogram Matching for Efficient Optical Flow Computation Applied to Velocity Estimation on Pocket Drones.pdf:pdf},
isbn = {9781467380256},
journal = {arXiv preprint arXiv:1603.07644},
keywords = {TU Delft,experiment,low complexity,optical flow},
mendeley-tags = {TU Delft,experiment,low complexity,optical flow},
pages = {1--15},
title = {{Local Histogram Matching for Efficient Optical Flow Computation Applied to Velocity Estimation on Pocket Drones}},
year = {2016}
}
@article{Beyeler2007,
abstract = {Fully autonomous control of ultra-light indoor airplanes has not yet been achieved because of the strong limitations on the kind of sensors that can be embedded making it difficult to obtain good estimations of altitude. We propose to revisit altitude control by considering it as an obstacle avoidance problem and introduce a novel control scheme where the ground and ceiling is avoided based on translatory optic flow, in a way similar to existing vision-based wall avoidance strategies. We show that this strategy is successful at controlling a simulated microflyer without any explicit altitude estimation and using only simple sensors and processing that have already been embedded in an existing 10-gram microflyer. This result is thus a significant step toward autonomous control of indoor flying robots.},
annote = {Pitch control van een fixed-wing micro flyer, door de vloer en plafond te ontwijken op basis van optical flow.},
author = {Beyeler, Antoine and Zufferey, Jean Christophe and Floreano, Dario},
doi = {10.1109/ROBOT.2007.363170},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Beyeler, Zufferey, Floreano - 2007 - 3D vision-based navigation for indoor microflyers.pdf:pdf},
isbn = {1424406021},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
keywords = {low complexity,nieuwe referenties,obstacle avoidance,optical flow,reactive obstacle avoidance,scenario: room,simulation,turn away from flow},
mendeley-tags = {low complexity,nieuwe referenties,obstacle avoidance,optical flow,reactive obstacle avoidance,scenario: room,simulation,turn away from flow},
number = {April},
pages = {1336--1341},
title = {{3D vision-based navigation for indoor microflyers}},
year = {2007}
}
@article{Hrabar2005,
abstract = {We present a novel vision-based technique for navigating an unmanned aerial vehicle (UAV) through urban canyons. Our technique relies on both optic flow and stereo vision information. We show that the combination of stereo and optic flow (stereo flow) is more effective at navigating urban canyons than either technique alone. Optic flow from a pair of sideways looking cameras is used to stay centered in a canyon and initiate turns at junctions, while stereo vision from a forward facing stereo head is used to avoid obstacles to the front. The technique was tested in full on an autonomous tractor at CSIRO and in part on the USC autonomous helicopter. Experimental results are presented from these two robotic platforms operating in outdoor environments. We show that the autonomous tractor can navigate urban canyons using stereo-flow, and that the autonomous helicopter can turn away from obstacles to the side using optic flow. In addition, preliminary results show that a single pair of forward facing fisheye cameras can be used for both stereo and optic flow. The center portions of the fisheye images are used for stereo, while flow is measured in the periphery of the images.},
annote = {Combinatie tussen stereo en optical flow is erg simplistisch: stereo krijgt voorrang. Optical flow is erg beperkt (geen laterale beweging). Verder weinig nieuws.},
author = {Hrabar, Stefan and Sukhatme, Gaurav S. and Corke, Peter and Usher, Kane and Roberts, Jonathan},
doi = {10.1109/IROS.2005.1544998},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Hrabar et al. - 2005 - Combined optic-flow and stereo-based navigation of urban canyons for a UAV.pdf:pdf},
isbn = {0780389123},
journal = {2005 IEEE/RSJ International Conference on Intelligent Robots and Systems, IROS},
keywords = {Optic flow,Stereo vision,Uav,Urban canyon navigation,nieuwe referenties,turn away from flow},
mendeley-tags = {nieuwe referenties,turn away from flow},
pages = {302--309},
title = {{Combined optic-flow and stereo-based navigation of urban canyons for a UAV}},
year = {2005}
}
@article{Pham2015,
abstract = {Collision avoidance is a key factor in enabling the integration of unmanned aerial vehicle into real life use, whether it is in military or civil application. For a long time there have been a large number of works to address this problem; therefore a comparative summary of them would be desirable. This paper presents a survey on the major collision avoidance systems developed in up to date publications. Each collision avoidance system contains two main parts: sensing and detection, and collision avoidance. Based on their characteristics each part is divided into different categories; and those categories are explained, compared and discussed about advantages and disadvantages in this paper.},
archivePrefix = {arXiv},
arxivId = {1508.07723},
author = {Pham, Hung and Smolka, Scott A and Stoller, Scott D and Phan, Dung and Yang, Junxing},
eprint = {1508.07723},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Pham et al. - 2015 - A survey on unmanned aerial vehicle collision avoidance systems.pdf:pdf},
journal = {arXiv preprint},
month = {aug},
number = {arXiv:1508.07723},
title = {{A survey on unmanned aerial vehicle collision avoidance systems}},
year = {2015}
}
@article{Heng2011,
abstract = {We present a novel stereo-based obstacle avoidance system on a vision-guided micro air vehicle (MAV) that is capable of fully autonomous maneuvers in unknown and dynamic environments. All algorithms run exclusively on the vehicle's on-board computer, and at high frequencies that allow the MAV to react quickly to obstacles appearing in its flight trajectory. Our MAV platform is a quadrotor aircraft equipped with an inertial measurement unit and two stereo rigs. An obstacle mapping algorithm processes stereo images, producing a 3D map representation of the environment; at the same time, a dynamic anytime path planner plans a collision-free path to a goal point.},
author = {Heng, Lionel and Meier, Lorenz and Tanskanen, Petri and Fraundorfer, Friedrich and Pollefeys, Marc},
doi = {10.1109/ICRA.2011.5980095},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Heng et al. - 2011 - Autonomous obstacle avoidance and maneuvering on a vision-guided MAV using on-board processing.pdf:pdf},
isbn = {9781612843865},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
keywords = {cartesian map,deliberate obstacle avoidance,low complexity,obstacle avoidance},
mendeley-tags = {cartesian map,deliberate obstacle avoidance,low complexity,obstacle avoidance},
pages = {2472--2477},
title = {{Autonomous obstacle avoidance and maneuvering on a vision-guided MAV using on-board processing}},
year = {2011}
}
@article{Scaramuzza2011,
abstract = {Visual odometry (VO) is the process of estimating the egomotion of an agent (e.g., vehicle, human, and robot) using only the input of a single or If multiple cameras attached to it. Application domains include robotics, wearable computing, augmented reality, and automotive. The term VO was coined in 2004 by Nister in his landmark paper. The term was chosen for its similarity to wheel odometry, which incrementally estimates the motion of a vehicle by integrating the number of turns of its wheels over time. Likewise, VO operates by incrementally estimating the pose of the vehicle through examination of the changes that motion induces on the images of its onboard cameras. For VO to work effectively, there should be sufficient illumination in the environment and a static scene with enough texture to allow apparent motion to be extracted. Furthermore, consecutive frames should be captured by ensuring that they have sufficient scene overlap.},
author = {Scaramuzza, Davide and Fraundorfer, Friedrich},
doi = {10.1109/MRA.2011.943233},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Scaramuzza, Fraundorfer - 2011 - Visual Odometry Part II.pdf:pdf},
isbn = {1070-9932},
issn = {1070-9932},
journal = {IEEE Robotics {\&} Automation Magazine},
number = {4},
pages = {80--92},
pmid = {6153423},
title = {{Visual Odometry Part II}},
volume = {18},
year = {2011}
}
@inproceedings{Kamon2003,
abstract = {Presents a globally convergent range-sensor based navigation algorithm in three-dimensions, called 3D Bug. The 3D Bug algorithm navigates a point robot in a three-dimensional unknown environment using position and range sensors. The algorithm strives to process the sensory data in the most reactive way possible, without sacrificing the global convergence guarantee. Moreover, unlike previous reactive-like algorithms, 3D Bug uses three-dimensional range data and plans three-dimensional motion throughout the navigation process. The algorithm alternates between two modes of motion. During motion towards the target, which is the first motion mode of the algorithm, the robot follows the locally shortest path in a purely reactive fashion. During traversal of an obstacle surface, which is the second mode of motion, the robot incrementally constructs a reduced data structure of an obstacle, while performing local shortcuts based on range data. We resent preliminary simulation results of the algorithm, which show that 3D Bug generates paths that resemble the globally shortest path in simple scenarios. Moreover, the algorithm generates reasonably short paths even in concave, room-like environments},
author = {Kamon, I. and Rimon, E. and Rivlin, E.},
booktitle = {Proceedings 1999 IEEE International Conference on Robotics and Automation (Cat. No.99CH36288C)},
doi = {10.1109/ROBOT.1999.769955},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Kamon, Rimon, Rivlin - 2003 - Range-sensor based navigation in three dimensions.pdf:pdf},
isbn = {0-7803-5180-0},
number = {May},
pages = {163--169},
publisher = {IEEE},
title = {{Range-sensor based navigation in three dimensions}},
url = {http://ieeexplore.ieee.org/document/769955/},
volume = {1},
year = {2003}
}
@article{Mariottini2008,
abstract = {This paper presents an image-based visual servoing strategy for the autonomous navigation of a mobile holonomic robot from a current towards a desired pose, specified only through a current and a desired image acquired by the on-board central catadioptric camera. This kind of vision sensor combines lenses and mirrors to enlarge the field of view. The proposed visual servoing does not require any metrical information about the three-dimensional viewed scene and is mainly based on a novel geometrical property, the auto-epipolar condition, which occurs when two catadioptric views (current and desired) undergo a pure translation. This condition can be detected in real time in the image domain by observing when a set of so-called disparity conics have a common intersection. The auto-epipolar condition and the pixel distances between the current and target image features are used to design the image-based control law. Lyapunov-based stability analysis and simulation results demonstrate the parametric robustness of the proposed method. Experimental results are presented to show the applicability of our visual servoing in a real context.},
author = {Mariottini, G. L. and Prattichizzo, D.},
doi = {10.1177/0278364907084320},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Mariottini, Prattichizzo - 2008 - Image-based Visual Servoing with Central Catadioptric Cameras.pdf:pdf},
isbn = {0278-3649},
issn = {0278-3649},
journal = {The International Journal of Robotics Research},
keywords = {computer vision,sensing and,visual servoing},
number = {1},
pages = {41--56},
title = {{Image-based Visual Servoing with Central Catadioptric Cameras}},
url = {http://ijr.sagepub.com/content/27/1/41.short{\%}5Cnhttp://ijr.sagepub.com/cgi/doi/10.1177/0278364907084320},
volume = {27},
year = {2008}
}
@article{Chen2009,
abstract = {We present a simple approach for vision-based path following for a mobile robot. Based upon a novel concept called the funnel lane, the coordinates of feature points during the replay phase are compared with those obtained during the teaching phase in order to determine the turning direction. Increased robustness is achieved by coupling the feature coordinates with odometry information. The system requires a single off-the-shelf, forward-looking camera with no calibration (either external or internal, including lens distortion). Implicit calibration of the system is needed only in the form of a single controller gain. The algorithm is qualitative in nature, requiring no map of the environment, no image Jacobian, no homography, no fundamental matrix, and no assumption about a flat ground plane. Experimental results demonstrate the capability of real-time autonomous navigation in both indoor and outdoor environments and on flat, slanted, and rough terrain with dynamic occluding objects for distances of hundreds of meters. We also demonstrate that the same approach works with wide-angle and omnidirectional cameras with only slight modification.},
author = {Chen, Zhichao and Birchfield, Stanley T.},
doi = {10.1109/TRO.2009.2017140},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Chen, Birchfield - 2009 - Qualitative vision-based path following.pdf:pdf},
isbn = {1552-3098},
issn = {15523098},
journal = {IEEE Transactions on Robotics},
keywords = {Control,Feature tracking,Mobile robot navigation,Vision-based navigation},
number = {3},
pages = {749--754},
title = {{Qualitative vision-based path following}},
volume = {25},
year = {2009}
}
@inproceedings{Newcombe2011,
author = {Newcombe, Richard A and Lovegrove, Steven J and Davison, Andrew J},
booktitle = {2011 International Conference on Computer Vision},
doi = {10.1109/ICCV.2011.6126513},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Newcombe, Lovegrove, Davison - 2011 - DTAM Dense tracking and mapping in real-time.pdf:pdf},
isbn = {978-1-4577-1102-2},
month = {nov},
pages = {2320--2327},
publisher = {IEEE},
title = {{DTAM: Dense tracking and mapping in real-time}},
url = {http://ieeexplore.ieee.org/document/6126513/},
year = {2011}
}
@misc{Erhan2009,
abstract = {Deep architectures have demonstrated state-of-the-art results in a variety of settings, especially with vision datasets. Beyond the model definitions and the quantitative analyses, there is a need for qualitative comparisons of the solutions learned by various deep architectures. The goal of this paper is to find good qualitative interpretations of high level features represented by such models. To this end, we contrast and compare several techniques applied on Stacked Denoising Autoencoders and Deep Belief Networks, trained on several vision datasets. We show that, perhaps counter-intuitively, such interpretation is possible at the unit level, that it is simple to accomplish and that the results are consistent across various techniques. We hope that such techniques will allow researchers in deep architectures to understand more of how and why deep architectures work},
author = {Erhan, Dumitru and Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
booktitle = {Universit{\'{e}} de Montr{\'{e}}al},
doi = {10.2464/jilm.23.425},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Erhan et al. - 2009 - Visualizing higher-layer features of a deep network.pdf:pdf},
isbn = {0149-5992 (Print)$\backslash$r0149-5992 (Linking)},
issn = {04515994},
number = {1341},
pmid = {8718432},
title = {{Visualizing higher-layer features of a deep network}},
url = {http://igva2012.wikispaces.asu.edu/file/view/Erhan+2009+Visualizing+higher+layer+features+of+a+deep+network.pdf},
year = {2009}
}
@article{Zheng2018,
archivePrefix = {arXiv},
arxivId = {arXiv:1808.01454v1},
author = {Zheng, Chuanxia and Cham, Tat-jen and Cai, Jianfei},
eprint = {arXiv:1808.01454v1},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Zheng, Cham, Cai - 2018 - T²Net Synthetic-to-Realistic Translation for Solving Single-Image Depth Estimation Tasks.pdf:pdf},
journal = {arXiv preprint arXiv:1808.01454},
keywords = {data,domain adaptation,single-image depth estimation,synthetic,unpaired images},
title = {{T²Net: Synthetic-to-Realistic Translation for Solving Single-Image Depth Estimation Tasks}},
year = {2018}
}
@article{Mallot,
abstract = {This paper describes a purely visual navigation scheme based on two elementary mecha{\~{}}fisms (piloting and guidance) and a graph structure combining individual navigation steps controlled by these mech-anisms. In robot experiments in real environments, both mechanisms have been tested, piloting in an open environment and guidance in a maze with restricted movement opportunities. The results indicate that navigation and path planning can be brought about with these simple mechanisms. We argue that the graph of local views (snapshots) is a general and biologically plausible means of representing space and inte-grating the various mechanisms of map behaviour.},
author = {Mallot, Hanspeter A and Franz, Matthias and Sch{\"{o}}lkopf, Bernhard and B{\"{u}}lthoff, Heinrich H},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Mallot et al. - Unknown - The View-Graph Approach to Visual Navigation and Spatial Memory.pdf:pdf},
keywords = {appearance-based navigation,slam},
mendeley-tags = {appearance-based navigation,slam},
title = {{The View-Graph Approach to Visual Navigation and Spatial Memory}}
}
@article{Stricker1994,
abstract = {Color histogram matching has been shown to be a promising way of quickly indexing into a large image database. Yet, few experiments have been done to test the method on truly large databases, and even if they were performed, they would give little guidance to a user wondering if the technique would be useful with his or her database. In this paper we define and analyze a measure relevant to extending color histogram indexing to large databases: capacity (how many distinguishable histograms can be stored)},
author = {Stricker, M and Swain, M},
doi = {10.1109/CVPR.1994.323774},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Stricker, Swain - 1994 - The capacity of color histogram indexing.pdf:pdf},
isbn = {0-8186-5825-8},
issn = {1063-6919},
journal = {Computer Vision and Pattern Recognition 1994 Proceedings CVPR 94 1994 IEEE Computer Society Conference on},
pages = {704--708},
title = {{The capacity of color histogram indexing}},
url = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp={\&}arnumber=323774{\&}contentType=Conference+Publications{\&}searchField=Search{\_}All{\&}queryText=The+Capacity+of+Color+Histogram+Indexing},
volume = {2420},
year = {1994}
}
@article{Baker2001,
abstract = {Conventional video cameras have limited elds of view which make them restrictive for certain applications in computational vision. A catadioptric camera uses a combination of lenses and mirrors placed in a carefully arranged con guration to capture a much wider eld of view. One important design goal for catadioptric cameras is choosing the shapes of the mirrors in a way that ensures that the complete catadioptric system has a single e ective viewpoint. The reason a single viewpoint is so desirable is that it is a requirement for the generation of pure perspective images from the sensed images. In this chapter, we derive the complete class of single-lens single-mirror catadioptric cameras that have a single viewpoint. We describe all of the solutions in detail, including the degenerate ones, with reference to many of the catadioptric systems that have been proposed in the literature. In addition, we derive a simple expression for the spatial resolution of a catadioptric camera in terms of the resolution of the cameras used to construct it, and present an analysis of the defocus blur caused by the use of a curved mirror in a catadioptric camera. We end with a case study describing several implementations of one of the possible designs.},
author = {Baker, Simon and Nayar, Shree K.},
doi = {10.1007/s11263-005-3220-1},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Baker, Nayar - 2001 - Single Viewpoint Catadioptric Cameras.pdf:pdf},
isbn = {0-387-95111-3},
issn = {0920-5691},
journal = {Monographs in Computer Science},
pages = {39--72},
title = {{Single Viewpoint Catadioptric Cameras}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.35.2792},
year = {2001}
}
@article{Fuller2014,
abstract = {Scaling a flying robot down to the size of a fly or bee requires advances in manufacturing, sensing and control, and will provide insights into mechanisms used by their biological counterparts. Controlled flight at this scale has previously required external cameras to provide the feedback to regulate the continuous corrective manoeuvres necessary to keep the unstable robot from tumbling. One stabilization mechanism used by flying insects may be to sense the horizon or Sun using the ocelli, a set of three light sensors distinct from the compound eyes. Here, we present an ocelli-inspired visual sensor and use it to stabilize a fly-sized robot. We propose a feedback controller that applies torque in proportion to the angular velocity of the source of light estimated by the ocelli. We demonstrate theoretically and empirically that this is sufficient to stabilize the robot's upright orientation. This constitutes the first known use of onboard sensors at this scale. Dipteran flies use halteres to provide gyroscopic velocity feedback, but it is unknown how other insects such as honeybees stabilize flight without these sensory organs. Our results, using a vehicle of similar size and dynamics to the honeybee, suggest how the ocelli could serve this role.},
author = {Fuller, Sawyer B and Karpelson, Michael and Censi, Andrea and Ma, Kevin Y and Wood, Robert J},
doi = {10.1098/rsif.2014.0281},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Fuller et al. - 2014 - Controlling free flight of a robotic fly using an onboard vision sensor inspired by insect ocelli.pdf:pdf},
isbn = {0000000167},
issn = {1742-5662},
journal = {Journal of the Royal Society, Interface / the Royal Society},
keywords = {Aircraft,Aircraft: instrumentation,Animals,Biomimetics,Biomimetics: instrumentation,Compound Eye, Arthropod,Compound Eye, Arthropod: physiology,Diptera,Diptera: physiology,Equipment Design,Equipment Failure Analysis,Feedback,Feedback, Sensory,Feedback, Sensory: physiology,Flight, Animal,Flight, Animal: physiology,Miniaturization,Orientation,Orientation: physiology,Robotics,Robotics: instrumentation,Torque,Transducers,Wings, Animal,Wings, Animal: physiology,biology,low complexity},
mendeley-tags = {biology,low complexity},
number = {97},
pages = {20140281},
pmid = {24942846},
title = {{Controlling free flight of a robotic fly using an onboard vision sensor inspired by insect ocelli.}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-84903649934{\&}partnerID=tZOtx3y1},
volume = {11},
year = {2014}
}
@article{Hamel2002,
author = {Hamel, Tarek and Mahony, Robert},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Hamel, Mahony - 2002 - Visual Servoing of an Under-Actuated Dynamic Rigid-Body System An Image-Based Approach.pdf:pdf},
journal = {IEEE Transactions on Robotics and Automation},
number = {2},
pages = {187--198},
title = {{Visual Servoing of an Under-Actuated Dynamic Rigid-Body System: An Image-Based Approach}},
volume = {18},
year = {2002}
}
@inproceedings{Vanneste2014,
abstract = {Recently, researchers have tried to solve the computational intensive three-dimensional obstacle avoidance by creating a 2D map from a 3D map or by creating a 2D map with multiple altitude levels. When a robot can move in a three-dimensional space, these techniques are no longer sufficient. This paper proposes a new algorithm for real-time three-dimensional obstacle avoidance. This algorithm is based on the 2D VFH+ obstacle avoidance algorithm and uses the octomap frame-work to represent the three-dimensional environment. The algorithm will generate a 2D Polar Histogram from this octomap which will be used to generate a robot motion. The results show that the robot is able to avoid 3D obstacles in real-time. The algorithm is able to calculate a new robot motion with an average time of 300 µs.},
author = {Vanneste, Simon and Bellekens, Ben and Weyn, Maarten},
booktitle = {MORSE 2014 Model-Driven Robot Software Engineering: proceedings of the 1st International Workshop on Model-Driven Robot Software Engineering co-located with International Conference on Software Technologies: Applications and Foundations (STAF 2014), York,},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Vanneste, Bellekens, Weyn - 2014 - 3DVFH Real-Time Three-Dimensional Obstacle Avoidance Using an Octomap.pdf:pdf},
keywords = {naviga-,octomap,robotics,ros,three-dimensional obstacle avoidance,tion and planning},
pages = {91--102},
title = {{3DVFH+: Real-Time Three-Dimensional Obstacle Avoidance Using an Octomap}},
year = {2014}
}
@article{Mahjourian2018,
abstract = {We present a novel approach for unsupervised learning of depth and ego-motion from monocular video. Unsupervised learning removes the need for separate supervisory signals (depth or ego-motion ground truth, or multi-view video). Prior work in unsupervised depth learning uses pixel-wise or gradient-based losses, which only consider pixels in small local neighborhoods. Our main contribution is to explicitly consider the inferred 3D geometry of the scene, enforcing consistency of the estimated 3D point clouds and ego-motion across consecutive frames. This is a challenging task and is solved by a novel (approximate) backpropagation algorithm for aligning 3D structures. We combine this novel 3D-based loss with 2D losses based on photometric quality of frame reconstructions using estimated depth and ego-motion from adjacent frames. We also incorporate validity masks to avoid penalizing areas in which no useful information exists. We test our algorithm on the KITTI dataset and on a video dataset captured on an uncalibrated mobile phone camera. Our proposed approach consistently improves depth estimates on both datasets, and outperforms the state-of-the-art for both depth and ego-motion. Because we only require a simple video, learning depth and ego-motion on large and varied datasets becomes possible. We demonstrate this by training on the low quality uncalibrated video dataset and evaluating on KITTI, ranking among top performing prior methods which are trained on KITTI itself.},
archivePrefix = {arXiv},
arxivId = {1802.05522},
author = {Mahjourian, Reza and Wicke, Martin and Angelova, Anelia},
eprint = {1802.05522},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Mahjourian, Wicke, Angelova - 2018 - Unsupervised Learning of Depth and Ego-Motion from Monocular Video Using 3D Geometric Constraints.pdf:pdf},
title = {{Unsupervised Learning of Depth and Ego-Motion from Monocular Video Using 3D Geometric Constraints}},
url = {http://arxiv.org/abs/1802.05522},
year = {2018}
}
@inproceedings{Smith2006,
author = {Smith, Paul and Reid, Ian and Davison, Andrew},
booktitle = {Proceedings of the British Machine Vision Conference 2006},
doi = {10.5244/C.20.3},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Smith, Reid, Davison - 2006 - Real-Time Monocular SLAM with Straight Lines.pdf:pdf},
title = {{Real-Time Monocular SLAM with Straight Lines}},
year = {2006}
}
@article{Black1996,
author = {Black, Michael J. and Anandan, P.},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Black, Anandan - 1996 - The Robust Estimation of Multiple Motions Parametric and Piecewise-Smooth Flow Fields.pdf:pdf},
journal = {Computer Vision and Image Understanding},
keywords = {optical flow},
mendeley-tags = {optical flow},
number = {1},
pages = {75--104},
title = {{The Robust Estimation of Multiple Motions: Parametric and Piecewise-Smooth Flow Fields}},
volume = {63},
year = {1996}
}
@article{Steinbrucker2014,
abstract = {In this paper we propose a novel volumetric multi-resolution mapping system for RGB-D images that runs on a standard CPU in real-time. Our approach generates a textured triangle mesh from a signed distance function that it continuously updates as new RGB-D images arrive. We propose to use an octree as the primary data structure which allows us to represent the scene at multiple scales. Furthermore, it allows us to grow the reconstruction volume dynamically. As most space is either free or unknown, we allocate and update only those voxels that are located in a narrow band around the observed surface. In contrast to a regular grid, this approach saves enormous amounts of memory and computation time. The major challenge is to generate and maintain a consistent triangle mesh, as neighboring cells in the octree are more difficult to find and may have different resolutions. To remedy this, we present in this paper a novel algorithm that keeps track of these dependencies, and efficiently updates corresponding parts of the triangle mesh. In our experiments, we demonstrate the real-time capability on a large set of RGB-D sequences. As our approach does not require a GPU, it is well suited for applications on mobile or flying robots with limited computational resources. I.},
author = {Steinbr{\"{u}}cker, Frank and Sturm, J{\"{u}}rgen and Cremers, Daniel},
doi = {10.1109/ICRA.2014.6907127},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Steinbr{\"{u}}cker, Sturm, Cremers - 2014 - Volumetric 3D mapping in real-time on a CPU.pdf:pdf},
isbn = {9781479936847},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
pages = {2021--2028},
pmid = {8190083},
title = {{Volumetric 3D mapping in real-time on a CPU}},
year = {2014}
}
@article{Alahi2012,
author = {Alahi, Alexandre and Ortiz, Raphael and Vandergheynst, Pierre},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Alahi, Ortiz, Vandergheynst - 2012 - FREAK Fast Retina Keypoint.pdf:pdf},
isbn = {9781467312288},
pages = {510--517},
title = {{FREAK : Fast Retina Keypoint}},
year = {2012}
}
@inproceedings{Lamon2001,
author = {Lamon, Pierre and Nourbakhsh, Illah and Jensen, B. and Siegwart, Roland},
booktitle = {Proceedings 2001 ICRA. IEEE International Conference on Robotics and Automation (Cat. No.01CH37164)},
doi = {10.1109/ROBOT.2001.932841},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Lamon et al. - 2001 - Deriving and matching image fingerprint sequences for mobile robot localization.pdf:pdf},
isbn = {0-7803-6576-3},
pages = {1609--1614},
publisher = {IEEE},
title = {{Deriving and matching image fingerprint sequences for mobile robot localization}},
url = {http://ieeexplore.ieee.org/document/932841/},
volume = {2},
year = {2001}
}
@inproceedings{Lee2016,
author = {Lee, Changmin and Kim, Daeeun},
booktitle = {International Conference on Simulation of Adaptive Behavior},
doi = {10.1007/978-3-319-43488-9_13},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Lee, Kim - 2016 - A Landmark Vector Approach Using Gray-Colored Information.pdf:pdf},
isbn = {9783319434889},
keywords = {landmark arrangement matching,landmark vector,localization,vision-based homing navigation},
pages = {138--144},
title = {{A Landmark Vector Approach Using Gray-Colored Information}},
year = {2016}
}
@article{Oniga2015,
author = {Oniga, Florin and Sarkozi, Ervin and Nedevschi, Sergiu},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Oniga, Sarkozi, Nedevschi - 2015 - Fast Obstacle Detection Using U-Disparity Maps with Stereo Vision.pdf:pdf},
isbn = {9781467382007},
keywords = {driving,image space,low complexity,obstacle detection,stereo vision,stereovision,u-disparity,uv disparity},
mendeley-tags = {image space,low complexity,obstacle detection,stereo vision,uv disparity},
pages = {203--207},
title = {{Fast Obstacle Detection Using U-Disparity Maps with Stereo Vision}},
year = {2015}
}
@incollection{Sprunk2015,
author = {Sprunk, Christoph and Parent, Gershon and Spinello, Luciano and Tipaldi, Gian Diego and Burgard, Wolfram and Jalobeanu, Mihai},
booktitle = {Experimental Robotics},
doi = {10.1007/978-3-319-23778-7_32},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Sprunk et al. - 2015 - An Experimental Protocol for Benchmarking Robotic Indoor Navigation.pdf:pdf},
isbn = {978-3-319-23778-7},
keywords = {autonomous navigation,benchmark,dynamic,indoor robots},
pages = {487--504},
publisher = {Springer International Publishing},
title = {{An Experimental Protocol for Benchmarking Robotic Indoor Navigation}},
year = {2015}
}
@article{Mur-Artal2017,
author = {Mur-Artal, R. and Tard{\'{o}}s, J.D.},
doi = {10.1109/TRO.2017.2705103},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Mur-Artal, Tard{\'{o}}s - 2017 - ORB-SLAM2 An Open-Source SLAM System for Monocular, Stereo, and RGB-D Cameras.pdf:pdf},
journal = {IEEE Transactions on Robotics},
number = {5},
pages = {1255--1262},
title = {{ORB-SLAM2: An Open-Source SLAM System for Monocular, Stereo, and RGB-D Cameras}},
volume = {33},
year = {2017}
}
@article{Lalonde2018,
archivePrefix = {arXiv},
arxivId = {1804.04241},
author = {LaLonde, Rodney and Bagci, Ulas},
eprint = {1804.04241},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/LaLonde, Bagci - 2018 - Capsules for Object Segmentation.pdf:pdf},
number = {Midl},
pages = {1--9},
title = {{Capsules for Object Segmentation}},
year = {2018}
}
@book{Brockers2014,
author = {Brockers, Roland and Humenberger, Martin and Kuwata, Yoshi and Matthies, Larry and Weiss, Stephan},
doi = {10.1007/978-3-319-09387-1},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Brockers et al. - 2014 - Advances in Embedded Computer Vision - Computer Vision for Micro Air Vehicles.pdf:pdf},
isbn = {978-3-319-09386-4},
keywords = {deliberate obstacle avoidance,image space,obstacle avoidance,obstacle detection,scenario: forest,stereo vision},
mendeley-tags = {deliberate obstacle avoidance,image space,obstacle avoidance,obstacle detection,scenario: forest,stereo vision},
pages = {239--248},
title = {{Advances in Embedded Computer Vision - Computer Vision for Micro Air Vehicles}},
url = {http://link.springer.com/10.1007/978-3-319-09387-1},
year = {2014}
}
@article{Konam,
abstract = {With the success of deep learning, recent efforts have been focused on analyzing how learned networks make their classi-fications. We are interested in analyzing the network output based on the network structure and information flow through the network layers. We contribute an algorithm for 1) analyz-ing a deep network to find neurons that are " important" in terms of the network classification outcome, and 2) automati-cally labeling the patches of the input image that activate these important neurons. We propose several measures of impor-tance for neurons and demonstrate that our technique can be used to gain insight into, and explain how a network decom-poses an image to make its final classification.},
archivePrefix = {arXiv},
arxivId = {1802.03675},
author = {Konam, Sandeep and Quah, Ian and Rosenthal, Stephanie and Veloso, Manuela},
eprint = {1802.03675},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Konam et al. - Unknown - Understanding Convolutional Networks with APPLE Automatic Patch Pattern Labeling for Explanation.pdf:pdf},
title = {{Understanding Convolutional Networks with APPLE : Automatic Patch Pattern Labeling for Explanation}},
url = {http://www.aies-conference.com/wp-content/papers/main/AIES{\_}2018{\_}paper{\_}91.pdf}
}
@inproceedings{Zhou2003,
abstract = {The paper presents a novel method for mobile robot localization using visual appearance features. A multidimensional histogram is used to describe the global appearance features of an image such as colors, edge density, gradient magnitude, textures and so on. The matching of histograms determines the location of the robot. The method has been evaluated in an indoor environment, and the system correctly determines the location of 82.9{\%} of the input scene images.},
author = {Zhou, Chao and Wei, Yucheng and Tan, Tieniu},
booktitle = {Robotics and Automation, 2003. Proceedings. ICRA'03. IEEE International Conference on},
doi = {10.1109/ROBOT.2003.1241767},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Zhou, Wei, Tan - 2003 - Mobile robot self-localization based on global visual appearance features.pdf:pdf},
isbn = {0-7803-7736-2},
issn = {1050-4729},
pages = {1271--1276},
publisher = {IEEE},
title = {{Mobile robot self-localization based on global visual appearance features}},
url = {http://ieeexplore.ieee.org/document/1241767/},
volume = {1},
year = {2003}
}
@article{Cheng2018a,
abstract = {Depth estimation from a single image is a fundamental problem in computer vision. In this paper, we propose a simple yet effective convolutional spatial propagation network (CSPN) to learn the affinity matrix for depth prediction. Specifically, we adopt an efficient linear propagation model, where the propagation is performed with a manner of recurrent convolutional operation, and the affinity among neighboring pixels is learned through a deep convolutional neural network (CNN). We apply the designed CSPN to two depth estimation tasks given a single image: (1) To refine the depth output from state-of-the-art (SOTA) existing methods; and (2) to convert sparse depth samples to a dense depth map by embedding the depth samples within the propagation procedure. The second task is inspired by the availability of LIDARs that provides sparse but accurate depth measurements. We experimented the proposed CSPN over two popular benchmarks for depth estimation, i.e. NYU v2 and KITTI, where we show that our proposed approach improves in not only quality (e.g., 30{\%} more reduction in depth error), but also speed (e.g., 2 to 5 times faster) than prior SOTA methods.},
archivePrefix = {arXiv},
arxivId = {1808.00150},
author = {Cheng, Xinjing and Wang, Peng and Yang, Ruigang},
eprint = {1808.00150},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Cheng, Wang, Yang - 2018 - Depth Estimation via Affinity Learned with Convolutional Spatial Propagation Network.pdf:pdf},
journal = {arXiv preprint arXiv:1808.00150},
keywords = {convolutional spatial propagation,depth estimation},
title = {{Depth Estimation via Affinity Learned with Convolutional Spatial Propagation Network}},
url = {http://arxiv.org/abs/1808.00150},
year = {2018}
}
@article{Petillot2001,
abstract = {This paper describes a new framework for segmentation of sonar images,$\backslash$ntracking of underwater objects and motion estimation. This framework$\backslash$nis applied to the design of an obstacle avoidance and path planning$\backslash$nsystem for underwater vehicles based on a multi- beam forward looking$\backslash$nsonar sensor. The real-time data flow (acoustic images) at the input$\backslash$nof the system is first segmented and relevant features are extracted.$\backslash$nWe also take advantage of the real-time data stream to track the$\backslash$nobstacles in following frames to obtain their dynamic characteristics.$\backslash$nThis allows us to optimize the preprocessing phases in segmenting$\backslash$nonly the relevant part of the images. Once the static (size and shape)$\backslash$nas well as dynamic characteristics (velocity, acceleration, ...)$\backslash$nof the obstacles have been computed, we create a representation of$\backslash$nthe vehicle's workspace based on these features. This representation$\backslash$nuses constructive solid geometry (CSG) to create a convex set of$\backslash$nobstacles defining the workspace. The tracking takes also into account$\backslash$nobstacles which are no longer in the field of view of the sonar in$\backslash$nthe path planning phase. A well- proven nonlinear search (sequential$\backslash$nquadratic programming) is then employed, where obstacles are expressed$\backslash$nas constraints in the search space. This approach is less affected$\backslash$nby local minima than classical methods using potential fields. The$\backslash$nproposed system is not only capable of obstacle avoidance but also$\backslash$nof path planning in complex environments which include fast moving$\backslash$nobstacles. Results obtained on real sonar data are shown and discussed.$\backslash$nPossible applications to sonar servoing and real-time motion estimation$\backslash$nare also discussed},
author = {Petillot, Yvan and Ruiz, Ioseba Tena and Lane, David M.},
doi = {10.1109/48.922790},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Petillot, Ruiz, Lane - 2001 - Underwater vehicle obstacle avoidance and path planning using a multi-beam forward looking sonar.pdf:pdf},
isbn = {0-7803-5045-6},
issn = {03649059},
journal = {IEEE Journal of Oceanic Engineering},
keywords = {Obstacle avoidance,Path planning,Segmentation,Sonar,Tracking,Underwater robotics},
number = {2},
pages = {240--251},
title = {{Underwater vehicle obstacle avoidance and path planning using a multi-beam forward looking sonar}},
volume = {26},
year = {2001}
}
@article{Goedeme2007,
abstract = {In this work we present a novel system for autonomous$\backslash$r$\backslash$nmobile robot navigation. With only$\backslash$r$\backslash$nan omnidirectional camera as sensor, this system$\backslash$r$\backslash$nis able to build automatically and robustly accurate$\backslash$r$\backslash$ntopologically organised environment maps of a$\backslash$r$\backslash$ncomplex, natural environment. It can localise itself$\backslash$r$\backslash$nusing such a map at each moment, including both$\backslash$r$\backslash$nat startup (kidnapped robot) or using knowledge of$\backslash$r$\backslash$nformer localisations. The topological nature of the$\backslash$r$\backslash$nmap is similar to the intuitive maps humans use, is$\backslash$r$\backslash$nmemory-efficient and enables fast and simple path$\backslash$r$\backslash$nplanning towards a specified goal. We developed$\backslash$r$\backslash$na real-time visual servoing technique to steer the$\backslash$r$\backslash$nsystem along the computed path.$\backslash$r$\backslash$nA key technology making this all possible is the$\backslash$r$\backslash$nnovel fast wide baseline feature matching, which$\backslash$r$\backslash$nyields an efficient description of the scene,},
author = {Goedem{\'{e}}, Toon and Nuttin, Marnix and Tuytelaars, Tinne and {Van Gool}, Luc},
doi = {10.1007/s11263-006-0025-9},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Goedem{\'{e}} et al. - 2007 - Omnidirectional vision based topological navigation.pdf:pdf},
isbn = {0920-5691},
issn = {09205691},
journal = {International Journal of Computer Vision},
keywords = {Omnidirectional vision,Topological maps,Visual servoing,Wide baseline matching},
number = {3},
pages = {219--236},
title = {{Omnidirectional vision based topological navigation}},
volume = {74},
year = {2007}
}
@article{Thrun2006,
abstract = {It is anticipatedthat theMars ScienceLaboratory rover, namedCuriosity,will traverse 10–20 kmon the surface of Mars during its primary mission. In preparation for this traverse, Earth-based tests were performed using Mars weight vehicles. These vehicles were driven over Mars analog bedrock, cohesive soil, and cohesionless sand at various slopes. Vehicle slip was characterized on each of these terrains versus slope for direct upslope driving. Results show that slopes up to 22 degrees are traversable on smooth bedrock and that slopes up to 28 degrees are traversable on some cohesive soils. In cohesionless sand, results show a sharp transition between moderate slip on 10 degree slopes and vehicle embedding at 17 degrees. For cohesionless sand, data are also presented showing the relationship between vehicle slip and wheel sinkage. Side by side testing of the Mars Exploration Rover test vehicle and the Mars Science Laboratory test vehicle show how increased wheel diameter leads to better slope climbing ability in sand for vehicles with nearly identical ground pressure. Lastly, preliminary data from Curiosity's initial driving on Mars are presented and compared to the Earth-based testing, showing good agreement for the driving done during the first 250 Martian days.},
archivePrefix = {arXiv},
arxivId = {10.1.1.91.5767},
author = {Thrun, Sebastian and Montemerlo, Mike and Dahlkamp, Hendrik and Stavens, David and Aron, Andrei and Diebel, James and Fong, Philip and Gale, John and Halpenny, Morgan and Hoffmann, Gabriel and Lau, Kenny and Oakley, Celia and Palatucci, Mark and Pratt, Vaughan and Stang, Pascal and Strohband, Sven and Dupont, Cedric and Jendrossek, Lars-Erik and Koelen, Christian and Markey, Charles and Rummel, Carlo and van Niekerk, Joe and Jensen, Eric and Alessandrini, Philippe and Bradski, Gary and Davies, Bob and Ettinger, Scott and Kaehler, Adrian and Nefian, Ara and Mahoney, Pamela},
doi = {10.1002/rob.20147},
eprint = {10.1.1.91.5767},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Thrun et al. - 2006 - Stanley The robot that won the DARPA Grand Challenge.pdf:pdf},
isbn = {9783902661623},
issn = {15564959},
journal = {Journal of Field Robotics},
keywords = {Backstepping,Non-linear control,Path-following},
month = {sep},
number = {9},
pages = {661--692},
pmid = {22164016},
title = {{Stanley: The robot that won the DARPA Grand Challenge}},
url = {http://doi.wiley.com/10.1002/rob.20147},
volume = {23},
year = {2006}
}
@article{Li2008,
abstract = {For vision SLAM, most of the current work focuses on employing point features as landmarks, few take advantage of lines. In this paper, an effective EKF-based monocular SLAM approach using vertical straight lines is proposed. The vertical straight line is represented with the inverse-depth form which allows an efficient and accurate representation of its depth uncertainty and can also increase the computation speed of the SLAM algorithm, compared with the arbitrary direction line. Detailed extraction, representation, initialization and matching method of vertical line features are described. The estimation process is depicted, and experiment results in a static indoor environment are presented and discussed.},
author = {Li, Chao and Huang, Yalou and Kang, Yewei and Yuan, Jing},
doi = {10.1109/WCICA.2008.4593403},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Li et al. - 2008 - Monocular SLAM using vertical straight lines with inverse-depth representation.pdf:pdf},
isbn = {978-1-4244-2113-8},
journal = {World Congress on Intelligent Control and Automation},
number = {863},
pages = {3015--3020},
title = {{Monocular SLAM using vertical straight lines with inverse-depth representation}},
url = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=4593403},
year = {2008}
}
@article{Saxena2007,
author = {Saxena, Ashutosh and Chung, Sung H. and Ng, Andrew Y.},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Saxena, Chung, Ng - 2007 - 3-D Depth Reconstruction from a Single Still Image.pdf:pdf},
journal = {International Journal of Computer Visional of computer vision},
number = {1},
pages = {53--69},
title = {{3-D Depth Reconstruction from a Single Still Image}},
volume = {76},
year = {2007}
}
@incollection{Kosov2009,
address = {Berlin, Heidelberg},
author = {Kosov, Sergey and Seidel, Hans-peter},
booktitle = {Advances in Visual Computing. ISVC 2009. Lecture Notes in Computer Science, vol 5875},
doi = {10.1007/978-3-642-10331-5_74},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Kosov, Seidel - 2009 - Accurate Real-Time Disparity Estimation with Variational Methods.pdf:pdf},
pages = {796--807},
publisher = {Springer},
title = {{Accurate Real-Time Disparity Estimation with Variational Methods}},
year = {2009}
}
@article{Hafner2005,
abstract = {This paper is devoted to a description of experiments with rats, mostly at the author's laboratory, and to indicating the significance of these findings on rats for the clinical behavior of men. While all students agree as to the facts reported, they disagree on theory and explanation. 5 kinds of experiments (latent learning, vicarious trial and error, searching for the stimulus, hypotheses, and spatial orientation) are described and discussed. The conditions which favor (cognitive) narrow strip-maps and which favor broad comprehensive maps in rats and in men are considered. Narrow strip-maps seem to be indicated by (1) a damaged brain, (2) an inadequate arrangement of environmentally presented cues, (3) a surplus of repetitions on the original trained-on path, and (4) the presence of too strongly frustrating conditions. The fourth point is elaborated. It is contended that some of the psychological mechanisms which clinical psychologists and other students of personality have uncovered as factors underlying many individual and social maladjustments can be interpreted "as narrowings of our cognitive maps due to too strong motivations or to too intense frustrations."},
author = {Hafner, V. V.},
doi = {10.1177/105971230501300202},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Hafner - 2005 - Cognitive Maps in Rats and Robots.pdf:pdf},
isbn = {1939-1471(Electronic);0033-295X(Print)},
issn = {1059-7123},
journal = {Adaptive Behavior},
keywords = {CONDITIONING THERAPY},
month = {jun},
number = {2},
pages = {87--96},
pmid = {18870876},
title = {{Cognitive Maps in Rats and Robots}},
url = {http://adb.sagepub.com/cgi/doi/10.1177/105971230501300202},
volume = {13},
year = {2005}
}
@article{Milford2014,
abstract = {Mobile robots and animals alike must effectively navigate their environments in order to achieve their goals. For animals goal-directed navigation facilitates finding food, seeking shelter or migration; similarly robots perform goal-directed navigation to find a charging station, get out of the rain or guide a person to a destination. This similarity in tasks extends to the environment as well; increasingly, mobile robots are operating in the same underwater, ground and aerial environments that animals do. Yet despite these similarities, goal-directed navigation research in robotics and biology has proceeded largely in parallel, linked only by a small amount of interdisciplinary research spanning both areas. Most state-of-the-art robotic navigation systems employ a range of sensors, world representations and navigation algorithms that seem far removed from what we know of how animals navigate; their navigation systems are shaped by key principles of navigation in 'real-world' environments including dealing with uncertainty in sensing, landmark observation and world modelling. By contrast, biomimetic animal navigation models produce plausible animal navigation behaviour in a range of laboratory experimental navigation paradigms, typically without addressing many of these robotic navigation principles. In this paper, we attempt to link robotics and biology by reviewing the current state of the art in conventional and biomimetic goal-directed navigation models, focusing on the key principles of goal-oriented robotic navigation and the extent to which these principles have been adapted by biomimetic navigation models and why.},
author = {Milford, Michael and Schulz, Ruth},
doi = {10.1098/rstb.2013.0484},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Milford, Schulz - 2014 - Principles of goal-directed spatial robot navigation in biomimetic models.pdf:pdf},
isbn = {1471-2970},
issn = {0962-8436},
journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
keywords = {computational biology,neuroscience},
number = {1655},
pages = {20130484--20130484},
pmid = {25267826},
title = {{Principles of goal-directed spatial robot navigation in biomimetic models}},
url = {http://rstb.royalsocietypublishing.org/cgi/doi/10.1098/rstb.2013.0484},
volume = {369},
year = {2014}
}
@article{Kuipers1991,
author = {Kuipers, B and Byun, Y.-T.},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Kuipers, Byun - 1991 - A Robot Exploration and Mapping Strategy Based on a Semantic Hierarchiy of Spatial Representations.pdf:pdf},
journal = {Journal of Robotics and Autonomous Systems},
keywords = {cognitive map,distinctive place,environmental map-,highlight,large-scale space,ping,robot exploration,spatial reasoning,topological map},
mendeley-tags = {highlight},
pages = {47--63},
title = {{A Robot Exploration and Mapping Strategy Based on a Semantic Hierarchiy of Spatial Representations}},
volume = {8},
year = {1991}
}
@article{Roggeman2014,
author = {Roggeman, H{\'{e}}l{\`{e}}ne and Marzat, Julien and Sanfourche, Martial and Plyer, Aur{\'{e}}lien},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Roggeman et al. - 2014 - Embedded vision-based localization and model predictive control for autonomous exploration.pdf:pdf},
journal = {IROS Workshop on Visual Control of Mobile Robots (ViCoMoR)},
pages = {13--20},
title = {{Embedded vision-based localization and model predictive control for autonomous exploration}},
year = {2014}
}
@article{Reiser2013,
abstract = {As an animal translates through the world, its eyes will experience a radiating pattern of optic flow in which there is a focus of expansion directly in front and a focus of contraction behind. For flying fruit flies, recent experiments indicate that flies actively steer away from patterns of expansion. Whereas such a reflex makes sense for avoiding obstacles, it presents a paradox of sorts because an insect could not navigate stably through a visual scene unless it tolerated flight towards a focus of expansion during episodes of forward translation. One possible solution to this paradox is that a fly's behavior might change such that it steers away from strong expansion, but actively steers towards weak expansion. In this study, we use a tethered flight arena to investigate the influence of stimulus strength on the magnitude and direction of turning responses to visual expansion in flies. These experiments indicate that the expansion-avoidance behavior is speed dependent. At slower speeds of expansion, flies exhibit an attraction to the focus of expansion, whereas the behavior transforms to expansion avoidance at higher speeds. Open-loop experiments indicate that this inversion of the expansion-avoidance response depends on whether or not the head is fixed to the thorax. The inversion of the expansion-avoidance response with stimulus strength has a clear manifestation under closed-loop conditions. Flies will actively orient towards a focus of expansion at low temporal frequency but steer away from it at high temporal frequency. The change in the response with temporal frequency does not require motion stimuli directly in front or behind the fly. Animals in which the stimulus was presented within 120 deg sectors on each side consistently steered towards expansion at low temporal frequency and steered towards contraction at high temporal frequency. A simple model based on an array of Hassenstein-Reichardt type elementary movement detectors suggests that the inversion of the expansion-avoidance reflex can explain the spatial distribution of straight flight segments and collision-avoidance saccades when flies fly freely within an open circular arena.},
author = {Reiser, Michael B and Dickinson, Michael H},
doi = {10.1242/jeb.074732},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Reiser, Dickinson - 2013 - Visual motion speed determines a behavioral switch from forward flight to expansion avoidance in Drosophila.pdf:pdf},
isbn = {1477-9145 (Electronic)$\backslash$r0022-0949 (Linking)},
issn = {1477-9145},
journal = {The Journal of experimental biology},
keywords = {Animals,Avoidance Learning,Avoidance Learning: physiology,Behavior, Animal,Behavior, Animal: physiology,Drosophila melanogaster,Drosophila melanogaster: physiology,Female,Flight, Animal,Flight, Animal: physiology,Motion Perception,Motion Perception: physiology,Orientation,Photic Stimulation,Time Factors,Visual Perception,Visual Perception: physiology},
number = {Pt 4},
pages = {719--32},
pmid = {23197097},
title = {{Visual motion speed determines a behavioral switch from forward flight to expansion avoidance in Drosophila.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/23197097},
volume = {216},
year = {2013}
}
@inproceedings{Marzat2009,
author = {Marzat, J. and Dumortier, Y. and Ducrot, Andre},
booktitle = {WSCG '2009: Full Papers Proceedings: The 17th International Conference in Central Europe on Computer Graphics, Visualization and Computer Vision in co-operation with EUROGRAPHICS},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Marzat, Dumortier, Ducrot - 2009 - Real-time dense and accurate parallel optical flow using CUDA.pdf:pdf},
isbn = {9788086943930},
keywords = {cuda,gpu,image processing,monocular vision,optical flow,parallel processing},
pages = {105--112},
title = {{Real-time dense and accurate parallel optical flow using CUDA}},
url = {http://julien.marzat.free.fr/2008{\_}Stage{\_}Ingenieur{\_}INRIA/WSCG09{\_}Marzat{\_}Dumortier{\_}Ducrot.pdf},
year = {2009}
}
@inproceedings{Moravec1985,
author = {Moravec, H. and Elfes, A.},
booktitle = {Proceedings. 1985 IEEE International Conference on Robotics and Automation},
doi = {10.1109/ROBOT.1985.1087316},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Moravec, Elfes - 1985 - High resolution maps from wide angle sonar.pdf:pdf},
pages = {116--121},
publisher = {Institute of Electrical and Electronics Engineers},
title = {{High resolution maps from wide angle sonar}},
url = {http://ieeexplore.ieee.org/document/1087316/},
volume = {2},
year = {1985}
}
@article{Nagel1983,
abstract = {A local approach for interframe displacement estimates is developed by minimization of the squared differences between a second-order Taylor expansion of gray values from one frame and the observed gray values within the same window from the next frame. If the second-order terms in the Taylor expansion are significant, a system of two coupled nonlinear equations for the two unknown components of the displacement vector can be derived. In the special case of “gray value corners,” these equations can be simplified to facilitate a closed form solution. An iterative refinement procedure is developed to extend these estimates for image regions which do not exhibit exactly the properties of “gray value corners.” The minimization approach is generalized in such a way that the approach of Horn and Schanck (Artif. Intell. 17, 1981, 185–203) can be recognized as a special case of this generalized form which should be applicable even across occluding edges. It thus appears to be an interesting model for the local computation of optical flow.},
author = {Nagel, Hans-Hellmut},
doi = {10.1016/S0734-189X(83)80030-9},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Nagel - 1983 - Displacement vectors derived from second-order intensity variations in image sequences.pdf:pdf},
issn = {0734189X},
journal = {Computer Vision, Graphics, and Image Processing},
keywords = {optical flow},
mendeley-tags = {optical flow},
number = {1},
pages = {85--117},
title = {{Displacement vectors derived from second-order intensity variations in image sequences}},
url = {http://www.sciencedirect.com/science/article/pii/S0734189X83800309},
volume = {21},
year = {1983}
}
@article{Dittmar2010,
author = {Dittmar, Laura and Baird, Emily and Boeddeker, Norbert and Egelhaaf, Martin},
doi = {10.1242/jeb.043737},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Dittmar et al. - 2010 - Goal seeking in honeybees matching of optic flow snapshots.pdf:pdf},
journal = {Journal of Experimental Biology},
keywords = {honeybee,landmark navigation,snapshot matching,vision},
number = {213},
pages = {2913--2923},
title = {{Goal seeking in honeybees: matching of optic flow snapshots?}},
year = {2010}
}
@article{Le2014,
author = {Le, Q and Learning, T Mikolov - International Conference on Machine and undefined 2014},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Le, Learning, 2014 - 2014 - Distributed representations of sentences and documents.pdf:pdf},
journal = {Jmlr.Org},
title = {{Distributed representations of sentences and documents}},
url = {http://www.jmlr.org/proceedings/papers/v32/le14.pdf},
volume = {32},
year = {2014}
}
@article{Marcus2018,
abstract = {Although deep learning has historical roots going back decades, neither the term "deep learning" nor the approach was popular just over five years ago, when the field was reignited by papers such as Krizhevsky, Sutskever and Hinton's now classic (2012) deep network model of Imagenet. What has the field discovered in the five subsequent years? Against a background of considerable progress in areas such as speech recognition, image recognition, and game playing, and considerable enthusiasm in the popular press, I present ten concerns for deep learning, and suggest that deep learning must be supplemented by other techniques if we are to reach artificial general intelligence.},
archivePrefix = {arXiv},
arxivId = {1801.00631},
author = {Marcus, Gary},
eprint = {1801.00631},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Marcus - 2018 - Deep Learning A Critical Appraisal.pdf:pdf},
journal = {arXiv preprint arXiv:1801.00631},
pages = {1--27},
title = {{Deep Learning: A Critical Appraisal}},
url = {http://arxiv.org/abs/1801.00631},
year = {2018}
}
@inproceedings{Sivic2003,
author = {Sivic, Josef and Zisserman, Andrew},
booktitle = {Computer Vision, 2003. Proceedings. Ninth IEEE International Conference on},
doi = {10.1109/ICCV.2003.1238663},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Sivic, Zisserman - 2003 - Video Google a text retrieval approach to object matching in videos.pdf:pdf},
isbn = {0-7695-1950-4},
keywords = {highlight},
mendeley-tags = {highlight},
number = {Iccv},
pages = {1470--1477},
publisher = {IEEE},
title = {{Video Google: a text retrieval approach to object matching in videos}},
url = {http://ieeexplore.ieee.org/document/1238663/},
year = {2003}
}
@article{Barry2015,
abstract = {As MAVs increase in functionality and utility, lightweight perception becomes critical to increasing autonomy. Cameras present an attractive solution for on-board sensing, due to their low weight, small power consumption, and dense information stream. High-speed stereo vision provides 3D data for obstacle avoidance, but generally has substantial processing requirements which are especially difficult to achieve on small aircraft. Here, we compare two solutions to this problem: dense stereo on an FPGA and sparse stereo on an ARM processor. We detail the design considerations and performance of both systems on duplicate fixed-wing aircraft platforms flying near obstacles in an outdoor environment.},
author = {Barry, Andrew J. and Oleynikova, Helen and Honegger, Dominik and Pollefeys, Marc and Tedrake, Russ},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Barry et al. - 2015 - Fast Onboard Stereo Vision for UAVs.pdf:pdf},
journal = {IROS Workshop},
pages = {7},
title = {{Fast Onboard Stereo Vision for UAVs}},
url = {http://groups.csail.mit.edu/robotics-center/public{\_}papers/Barry15a.pdf},
year = {2015}
}
@article{Ooi2001,
abstract = {A biological system is often more efficient when it takes advantage of the regularities in its environment. Like other terrestrial creatures, our spatial sense relies on the regularities associated with the ground surface. A simple, but important, ecological fact is that the field of view of the ground surface extends upwards from near (feet) to infinity (horizon). It forms the basis of a trigonometric relationship wherein the further an object on the ground is, the higher in the field of view it looks, with an object at infinity being seen at the horizon. Here, we provide support for the hypothesis that the visual system uses the angular declination below the horizon for distance judgement. Using a visually directed action task, we found that when the angular declination was increased by binocularly viewing through base-up prisms, the observer underestimated distance. After adapting to the same prisms, however, the observer overestimated distance on prism removal. Most significantly, we show that the distance overestimation as an after-effect of prism adaptation was due to a lowered perceived eye level, which reduced the object's angular declination below the horizon.},
author = {Ooi, Teng Leng and Wu, Bing and He, Zijiang J.},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Ooi, Wu, He - 2001 - Distance determined by the angular declination below the horizon.pdf:pdf},
journal = {Nature},
pages = {197--200},
title = {{Distance determined by the angular declination below the horizon}},
volume = {414},
year = {2001}
}
@article{Basiri2016,
abstract = {In a team of autonomous drones, individual knowledge about the relative location of teammates is essential. Existing relative positioning solutions for teams of small drones mostly rely on external systems such as motion tracking cameras or GPS satellites that might not always be accessible. In this letter, we describe an onboard solution to measure the 3-D relative direction between drones using sound as the main source of information. First, we describe a method to measure the directions of other robots from perceiving their engine sounds in the absence of self-engine noise. We then extend the method to use active acoustic signaling to obtain the relative directions in the presence of self-engine noise, to increase the detection range, and to discriminate the identity of robots. Methods are evaluated in real world experiments and a fully autonomous leader-following behavior is illustrated with two drones using the proposed system.},
author = {Basiri, Meysam and Schill, Felix and Lima, Pedro and Floreano, Dario},
doi = {10.1109/LRA.2016.2527833},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Basiri et al. - 2016 - On-Board Relative Bearing Estimation for Teams of Drones Using Sound.pdf:pdf},
isbn = {9781467380256},
issn = {23773766},
journal = {IEEE Robotics and Automation Letters},
keywords = {Aerial Robotics,Localization,Swarms},
number = {2},
pages = {820--827},
title = {{On-Board Relative Bearing Estimation for Teams of Drones Using Sound}},
volume = {1},
year = {2016}
}
@inproceedings{Pinggera2014,
author = {Pinggera, Peter and Pfeiffer, David and Franke, Uwe and Mester, Rudolf},
booktitle = {European Conference on Computer Vision},
doi = {10.1007/978-3-319-10605-2_7},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Pinggera et al. - 2014 - Know Your Limits Accuracy of Long Range Stereoscopic Object Measurements in Practice.pdf:pdf},
pages = {96--111},
publisher = {Springer},
title = {{Know Your Limits: Accuracy of Long Range Stereoscopic Object Measurements in Practice}},
year = {2014}
}
@inproceedings{Gaidon2016,
archivePrefix = {arXiv},
arxivId = {arXiv:1605.06457v1},
author = {Gaidon, Adrien and Wang, Qiao and Cabon, Yohann and Vig, Eleonora},
booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
eprint = {arXiv:1605.06457v1},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Gaidon et al. - 2016 - Virtual Worlds as Proxy for Multi-Object Tracking Analysis.pdf:pdf},
pages = {4340--4349},
title = {{Virtual Worlds as Proxy for Multi-Object Tracking Analysis}},
year = {2016}
}
@inproceedings{Menze2015,
abstract = {This paper proposes a novel model and dataset for 3D scene flow estimation with an application to autonomous driving. Taking advantage of the fact that outdoor scenes often decompose into a small number of independently mov- ing objects, we represent each element in the scene by its rigid motion parameters and each superpixel by a 3D plane as well as an index to the corresponding object. This minimal representation increases robustness and leads to a discrete-continuous CRF where the data term decomposes into pairwise potentials between superpixels and objects. Moreover, our model intrinsically segments the scene into its constituting dynamic components. We demonstrate the performance of our model on existing benchmarks as well as a novel realistic dataset with scene flow ground truth. We obtain this dataset by annotating 400 dynamic scenes from the KITTI raw data collection using detailed 3D CAD models for all vehicles in motion. Our experiments also re- veal novel challenges which cannot be handled by existing methods. 1.},
author = {Menze, Moritz and Geiger, Andreas},
booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
file = {:home/tom/Documents/phd/mendeley{\_}pdf//Menze, Geiger - 2015 - Object Scene Flow for Autonomous Vehicles.pdf:pdf},
title = {{Object Scene Flow for Autonomous Vehicles}},
year = {2015}
}
@article{Forster2017,
author = {Forster, Christian and Zhang, Zichao and Gassner, Michael and Werlberger, Manuel and Scaramuzza, Davide},
doi = {10.1109/TRO.2016.2623335},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Forster et al. - 2017 - SVO Semidirect Visual Odometry for Monocular and Multicamera Systems.pdf:pdf},
journal = {IEEE Transactions on Robotics},
number = {2},
pages = {249--265},
title = {{SVO: Semidirect Visual Odometry for Monocular and Multicamera Systems}},
volume = {33},
year = {2017}
}
@inproceedings{Engel2015,
author = {Engel, Jakob and St{\"{u}}ckler, J{\"{o}}rg and Cremers, Daniel},
booktitle = {Intelligent Robots and Systems (IROS), 2015 IEEE/RSJ International Conference on},
doi = {10.1109/IROS.2015.7353631},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Engel, St{\"{u}}ckler, Cremers - 2015 - Large-Scale Direct SLAM with Stereo Cameras.pdf:pdf},
isbn = {9781479999941},
pages = {1935--1942},
publisher = {IEEE},
title = {{Large-Scale Direct SLAM with Stereo Cameras}},
year = {2015}
}
@article{Smith2007,
abstract = {Insects are able to navigate reliably between food and nest using only visual information. This behavior has inspired many models of visual landmark guidance, some of which have been tested on autonomous robots. The majority of these models work by comparing the agent's current view with a view of the world stored when the agent was at the goal. The region from which agents can successfully reach home is therefore limited to the goal's visual locale, that is, the area around the goal where the visual scene is not radically different to the goal position. Ants are known to navigate over large distances using visually guided routes consisting of a series of visual memories. Taking inspiration from such route navigation, we propose a framework for linking together local navigation methods. We implement this framework on a robotic platform and test it in a series of environments in which local navigation methods fail. Finally, we show that the framework is robust to environments of varying complexity.},
author = {Smith, L and Philippides, A and Graham, P and Baddeley, B and Husbands, P},
doi = {10.1177/1059712307082091},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Smith et al. - 2007 - Linked Local Navigation for Visual Route Guidance.pdf:pdf},
issn = {1059-7123},
journal = {Adaptive Behavior},
keywords = {average landmark vector,biomimetic robotics,navigation,route learning,sequential homing,snapshot,snapshot model,view-based homing},
mendeley-tags = {sequential homing,snapshot model},
number = {3},
pages = {257--271},
title = {{Linked Local Navigation for Visual Route Guidance}},
volume = {15},
year = {2007}
}
@inproceedings{Michels2005,
address = {New York, New York, USA},
author = {Michels, Jeff and Saxena, Ashutosh and Ng, Andrew Y.},
booktitle = {Proceedings of the 22nd international conference on Machine learning - ICML '05},
doi = {10.1145/1102351.1102426},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Michels, Saxena, Ng - 2005 - High speed obstacle avoidance using monocular vision and reinforcement learning.pdf:pdf},
isbn = {1595931805},
pages = {593--600},
publisher = {ACM Press},
title = {{High speed obstacle avoidance using monocular vision and reinforcement learning}},
url = {http://portal.acm.org/citation.cfm?doid=1102351.1102426},
year = {2005}
}
@article{Collett2004,
abstract = {Animals typically have several navigational strategies available to them. Interactions between these strategies can reduce navigational errors and may lead to the emergence of new capacities.},
author = {Collett, Thomas S. and Graham, Paul},
doi = {10.1016/j.cub.2004.06.013},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Collett, Graham - 2004 - Animal navigation Path integration, visual landmarks and cognitive maps.pdf:pdf},
isbn = {0960-9822 (Print)$\backslash$n0960-9822 (Linking)},
issn = {09609822},
journal = {Current Biology},
keywords = {biology,path integration},
mendeley-tags = {biology,path integration},
number = {12},
pages = {475--477},
pmid = {15203020},
title = {{Animal navigation: Path integration, visual landmarks and cognitive maps}},
volume = {14},
year = {2004}
}
@article{Parker2016,
abstract = {Many aspects of our perceptual experience are dominated by the fact that our two eyes point forward. Whilst the location of our eyes leaves the environment behind our head inaccessible to vision, co-ordinated use of our two eyes gives us direct access to the three-dimensional structure of the scene in front of us, through the mechanism of stereoscopic vision. Scientific understanding of the different brain regions involved in stereoscopic vision and three-dimensional spatial cognition is changing rapidly, with consequent influences on fields as diverse as clinical practice in ophthalmology and the technology of virtual reality devices.},
author = {Parker, Andrew J.},
doi = {10.1098/rstb.2015.0251},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Parker - 2016 - Vision in our three-dimensional world.pdf:pdf},
issn = {14712970},
journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
keywords = {Cortical processing,Neuroscience,Stereoscopic depth,Vision},
number = {1697},
pmid = {27269595},
title = {{Vision in our three-dimensional world}},
volume = {371},
year = {2016}
}
@article{Leutenegger2015,
author = {Leutenegger, Stefan and Lynen, Simon and Bosse, Michael and Siegwart, Roland and Furgale, Paul},
doi = {10.1177/0278364914554813},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Leutenegger et al. - 2015 - Keyframe-based visual–inertial odometry using nonlinear optimization.pdf:pdf},
journal = {The International Journal of Robotics Research},
keywords = {bundle adjustment,imu,inertial measurement unit,inertial odometry,keyframes,robotics,sensor fusion,simultaneous localization and mapping,slam,stereo camera,visual},
number = {3},
pages = {314--334},
title = {{Keyframe-based visual–inertial odometry using nonlinear optimization}},
volume = {34},
year = {2015}
}
@article{Churchill2013,
author = {Churchill, David and Vardy, Andrew},
doi = {10.1007/s10846-012-9730-5},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Churchill, Vardy - 2013 - An orientation invariant visual homing algorithm.pdf:pdf},
issn = {09210296},
journal = {Journal of Intelligent and Robotic Systems: Theory and Applications},
keywords = {Robot navigation,Visual homing},
number = {1},
pages = {3--29},
title = {{An orientation invariant visual homing algorithm}},
volume = {71},
year = {2013}
}
@article{Taylor2015,
annote = {Mooie plaatjes. Verder niet echt geschikt.},
author = {Taylor, Camillo J. and Cowley, Anthony and Kettler, Rafe and Ninomiya, Kai and Gupta, Mayank and Niu, Boyang},
doi = {10.1109/IROS.2015.7354271},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Taylor et al. - 2015 - Mapping with depth panoramas.pdf:pdf},
isbn = {9781479999941},
issn = {21530866},
journal = {IEEE International Conference on Intelligent Robots and Systems},
keywords = {Gravity,Optimization,Robot sensing systems,Surface morphology,Surface treatment,Three-dimensional displays},
pages = {6265--6272},
title = {{Mapping with depth panoramas}},
volume = {2015-Decem},
year = {2015}
}
@article{Weiss2013,
archivePrefix = {arXiv},
arxivId = {10.1.1.91.5767},
author = {Weiss, Stephan and Achtelik, Markus W. and Lynen, Simon and Achtelik, Michael C. and Kneip, Laurent and Chli, Margarita and Siegwart, Roland},
doi = {10.1002/rob.21466},
eprint = {10.1.1.91.5767},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Weiss et al. - 2013 - Monocular Vision for Long-term Micro Aerial Vehicle State Estimation A Compendium.pdf:pdf},
isbn = {9783902661623},
issn = {15564959},
journal = {Journal of Field Robotics},
keywords = {Backstepping,Non-linear control,Path-following},
month = {sep},
number = {5},
pages = {803--831},
pmid = {22164016},
title = {{Monocular Vision for Long-term Micro Aerial Vehicle State Estimation: A Compendium}},
url = {http://doi.wiley.com/10.1002/rob.21466},
volume = {30},
year = {2013}
}
@inproceedings{Lucas1981a,
author = {Lucas, Bruce D and Kanade, Takeo},
booktitle = {Proceedings of Image Understanding Workshop},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Lucas, Kanade - 1981 - An Iterative Image Registration Technique with an Application to Stereo Vision.pdf:pdf},
pages = {121--130},
title = {{An Iterative Image Registration Technique with an Application to Stereo Vision}},
volume = {130},
year = {1981}
}
@article{Chu2018,
abstract = {The last few years have seen approaches trying to com-bine the increasing popularity of depth sensors and the suc-cess of the convolutional neural networks. Using depth as additional channel alongside the RGB input has the scale variance problem present in image convolution based ap-proaches. On the other hand, 3D convolution wastes a large amount of memory on mostly unoccupied 3D space, which consists of only the surface visible to the sensor. Instead, we propose SurfConv, which " slides " compact 2D filters along the visible 3D surface. SurfConv is formulated as a simple depth-aware multi-scale 2D convolution, through a new Data-Driven Depth Discretization (D 4) scheme. We demonstrate the effectiveness of our method on indoor and outdoor 3D semantic segmentation datasets. Our method achieves state-of-the-art performance while using less than 30{\%} parameters used by the 3D convolution based ap-proaches.},
archivePrefix = {arXiv},
arxivId = {1812.01519},
author = {Chu, Hang and Ma, Wei-Chiu and Kundu, Kaustav and Urtasun, Raquel and Fidler, Sanja},
doi = {10.1109/CVPR.2018.00317},
eprint = {1812.01519},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Chu et al. - 2018 - SurfConv Bridging 3D and 2D Convolution for RGBD Images.pdf:pdf},
isbn = {9686914951},
journal = {Cvpr},
title = {{SurfConv: Bridging 3D and 2D Convolution for RGBD Images}},
url = {https://github.com/chuhang/SurfConv},
year = {2018}
}
@article{Kendoul2009,
abstract = {The problem considered in this paper involves the design of a vision-based autopilot for small and micro Unmanned Aerial Vehicles (UAVs). The proposed autopilot is based on an optic flow-based vision system for autonomous localization and scene mapping, and a nonlinear control system for flight control and guidance. This paper focusses on the development of a real-time 3D vision algorithm for estimating optic flow, aircraft self-motion and depth map, using a low-resolution onboard camera and a low-cost Inertial Measurement Unit (IMU). Our implementation is based on 3 Nested Kalman Filters (3NKF) and results in an efficient and robust estimation process. The vision and control algorithms have been implemented on a quadrotor UAV, and demonstrated in real-time flight tests. Experimental results show that the proposed vision-based autopilot enabled a small rotorcraft to achieve fully-autonomous flight using information extracted from optic flow. {\textcopyright} 2009 Elsevier B.V. All rights reserved.},
author = {Kendoul, Farid and Fantoni, Isabelle and Nonami, Kenzo},
doi = {10.1016/j.robot.2009.02.001},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Kendoul, Fantoni, Nonami - 2009 - Optic flow-based vision system for autonomous 3D localization and control of small aerial vehicles.pdf:pdf},
isbn = {9781848211278},
issn = {09218890},
journal = {Robotics and Autonomous Systems},
keywords = {Autonomous localization,Flight guidance and control,Optic flow,Structure-From-Motion (SFM),UAV,Visual SLAM,experiment,low complexity,monocular,optical flow,state estimation},
mendeley-tags = {experiment,low complexity,monocular,optical flow,state estimation},
number = {6-7},
pages = {591--602},
publisher = {Elsevier B.V.},
title = {{Optic flow-based vision system for autonomous 3D localization and control of small aerial vehicles}},
url = {http://dx.doi.org/10.1016/j.robot.2009.02.001},
volume = {57},
year = {2009}
}
@article{Milford2008,
abstract = {—This paper describes a biologically inspired approach to vision-only simultaneous localization and mapping (SLAM) on ground-based platforms. The core SLAM system, dubbed RatSLAM, is based on computational models of the rodent hip-pocampus, and is coupled with a lightweight vision system that provides odometry and appearance information. RatSLAM builds a map in an online manner, driving loop closure and relocaliza-tion through sequences of familiar visual scenes. Visual ambiguity is managed by maintaining multiple competing vehicle pose esti-mates, while cumulative errors in odometry are corrected after loop closure by a map correction algorithm. We demonstrate the map-ping performance of the system on a 66 km car journey through a complex suburban road network. Using only a web camera oper-ating at 10 Hz, RatSLAM generates a coherent map of the entire environment at real-time speed, correctly closing more than 51 loops of up to 5 km in length. Index Terms—Bio-inspired robotics, monocular vision simulta-neous localization and mapping (SLAM).},
author = {Milford, M.J. and Wyeth, G.F.},
doi = {10.1109/TRO.2008.2004520},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Milford, Wyeth - 2008 - Mapping a Suburb With a Single Camera Using a Biologically Inspired SLAM System.pdf:pdf},
issn = {1552-3098},
journal = {IEEE Transactions on Robotics},
keywords = {biology,monocular,slam},
mendeley-tags = {biology,monocular,slam},
month = {oct},
number = {5},
pages = {1038--1053},
title = {{Mapping a Suburb With a Single Camera Using a Biologically Inspired SLAM System}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4627450},
volume = {24},
year = {2008}
}
@article{Bazeille2010,
abstract = {We address the problem of simultaneous localization and mapping (SLAM) by combining visual loop-closure detection with metrical information given by a robot odometry. The proposed algorithm extends a purely appearance-based loop-closure detection method based on bags of visual words 1 which is able to detect when the robot has returned back to a previously visited place. An efficient optimization algorithm is used to integrate odometry information in this method to generate a consistent topo-metrical map. The resulting algorithm which only requires a monocular camera and odometry data and is simple, and robust without requiring any a priori information on the environment.},
author = {Bazeille, S. and Filliat, D.},
doi = {10.1051/ro/2010021},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Bazeille, Filliat - 2010 - Combining Odometry and Visual Loop-Closure Detection for Consistent Topo-Metrical Mapping.pdf:pdf},
isbn = {0399-0559},
issn = {0399-0559},
journal = {RAIRO - Operations Research},
keywords = {mobile robot,monocular vision,odometry,slam},
pages = {365--377},
title = {{Combining Odometry and Visual Loop-Closure Detection for Consistent Topo-Metrical Mapping}},
volume = {44},
year = {2010}
}
@article{Kerl2015,
abstract = {Abstract We propose a dense continuous-time tracking and mapping method for RGB-D cameras. We parametrize the camera trajectory using continuous B-splines and optimize the trajectory through dense, direct image alignment. Our method also directly models rolling ...$\backslash$n},
author = {Kerl, Christian and Stuckler, Jorg and Cremers, Daniel},
doi = {10.1109/ICCV.2015.261},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Kerl, Stuckler, Cremers - 2015 - Dense continuous-time tracking and mapping with rolling shutter RGB-D cameras.pdf:pdf},
isbn = {9781467383912},
issn = {15505499},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
pages = {2264--2272},
pmid = {389660},
title = {{Dense continuous-time tracking and mapping with rolling shutter RGB-D cameras}},
volume = {2015 Inter},
year = {2015}
}
@misc{Mataric1990,
author = {Mataric, Maja J.},
booktitle = {Sab-90, From Animals to Animats},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Mataric - 1990 - Navigating with a rat brain A neurobiolocially-Inspired model for robot spatial representation.pdf:pdf},
pages = {169--175},
title = {{Navigating with a rat brain: A neurobiolocially-Inspired model for robot spatial representation}},
year = {1990}
}
@article{Besnerais2005,
abstract = { We study dense optical flow estimation using iterative registration of local window, also known as iterative Lucas-Kanade (LK) [B. Lucas et al, 1981]. We show that the usual iterative-warping scheme encounters divergence problems and propose a modified scheme with better behavior. It yields good results with a much lower cost than the exact dense LK algorithm, on simulated and real sequences.},
author = {Besnerais, Guy L. and Champagnat, Fr{\'{e}}d{\'{e}}ric},
doi = {10.1109/ICIP.2005.1529706},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Besnerais, Champagnat - 2005 - Dense optical flow by iterative local window registration.pdf:pdf},
isbn = {0780391349},
issn = {15224880},
journal = {Proceedings - International Conference on Image Processing, ICIP},
pages = {137--140},
title = {{Dense optical flow by iterative local window registration}},
volume = {1},
year = {2005}
}
@article{Stelzer2015,
abstract = {The travel range of a mobile robot is directly linked to its energy consumption. Apart from the actuators, the computing resources are the main energy consumers. Thus, robots that can navigate with minimal computational resources would be able to travel longer distances and, hence, would be valuable tools in applications such as search and rescue or planetary exploration. Inspired from the navigational abilities of insects, we developed the Trail-Map as a data structure for biologically inspired homing that can easily be scaled in case of memory or computational shortage. The Trail-Map can be built in constant time and enables constant time homing vector calculation. In this paper, we evaluate the Trail-Map-based homing performance of a simulated mobile robot equipped with an omnidirectional camera under the presence of sensor noise, such as odometry errors and observation errors. Further, we will show that Trail-Map-based homing outperforms SLAM methods in terms of computational resources while achieving a comparable homing performance},
author = {Stelzer, Annett and Suppa, Michael and Burgard, Wolfram},
doi = {10.1109/IROS.2015.7353482},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Stelzer, Suppa, Burgard - 2015 - Trail-Map-based homing under the presence of sensor noise.pdf:pdf},
isbn = {9781479999941},
issn = {21530866},
journal = {IEEE International Conference on Intelligent Robots and Systems},
keywords = {Actuators,Current measurement,Data structures,Mobile robots,Navigation,Robot sensing systems},
pages = {929--936},
title = {{Trail-Map-based homing under the presence of sensor noise}},
volume = {2015-Decem},
year = {2015}
}
@article{Maddern2012a,
abstract = {This paper describes a new system, dubbed Continuous Appearance-based Trajectory Simultaneous Localisation and Mapping (CAT-SLAM), which augments sequential appearance-based place recognition with local metric pose filtering to improve the frequency and reliability of appearance-based loop closure. As in other approaches to appearance-based mapping, loop closure is performed without calculating global feature geometry or performing 3D map construction. Loop-closure filtering uses a probabilistic distribution of possible loop closures along the robot's previous trajectory, which is represented by a linked list of previously visited locations linked by odometric information. Sequential appearance-based place recognition and local metric pose filtering are evaluated simultaneously using a Rao–Blackwellised particle filter, which weights particles based on appearance matching over sequential frames and the similarity of robot motion along the trajectory. The particle filter explicitly models both the likelihood of revisiting previous locations and exploring new locations. A modified resampling scheme counters particle deprivation and allows loop-closure updates to be performed in constant time for a given environment. We compare the performance of CAT-SLAM with FAB-MAP (a state-of-the-art appearance-only SLAM algorithm) using multiple real-world datasets, demonstrating an increase in the number of correct loop closures detected by CAT-SLAM.},
author = {Maddern, Will and Milford, Michael and Wyeth, Gordon},
doi = {10.1177/0278364912438273},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Maddern, Milford, Wyeth - 2012 - CAT-SLAM probabilistic localisation and mapping using a continuous appearance-based trajectory.pdf:pdf},
journal = {The International Journal of Robotics Research},
keywords = {appearance-based navigation},
mendeley-tags = {appearance-based navigation},
number = {4},
pages = {429--451},
title = {{CAT-SLAM: probabilistic localisation and mapping using a continuous appearance-based trajectory}},
volume = {31},
year = {2012}
}
@article{Cuijpers2000,
abstract = {Classically, it has been assumed that visual space can be represented by a metric. This means that the distance between points and the angle between lines can be uniquely defined. However, this assumption has never been tested. Also, measurements outdoors, where monocular cues are abundant, conflict with this model. This paper reports on two experiments in which the structure of visual space was investigated, using an exocentric pointing task. In the first experiment, we measured the influence of the separation between pointer and target and of the orientation of the stimuli with respect to the observer. This was done both monocularly and binocularly. It was found that the deviation of the pointer settings depended linearly on the orientation, indicating that visual space is anisotropic. The deviations for configurations that were symmetrical in the median plane were approximately the same, indicating that left/right symmetry was maintained. The results for monocular and binocular conditions were very different, which indicates that stereopsis was an important cue. In both conditions, there were large deviations from the veridical. In the second experiment, the relative distance of the pointer and the target with respect to the observer was varied in both the monocular and the binocular conditions. The relative distance turned out to be the main parameter for the ranges used (1-5 m). Any distance function must have an expanding and a compressing part in order to describe the data. In the binocular case, the results were much more consistent than in the monocular case and had a smaller standard deviation. Nevertheless, the systematic mispointings remained large. It can therefore be concluded that stereopsis improves space perception but does not improve veridicality.},
author = {Cuijpers, R. H. and Kappers, A. M.L. and Koenderink, J. J.},
doi = {10.3758/BF03212156},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Cuijpers, Kappers, Koenderink - 2000 - Investigation of visual space using an exocentric pointing task.pdf:pdf},
isbn = {9780195320176},
issn = {00315117},
journal = {Perception and Psychophysics},
number = {8},
pages = {1556--1571},
pmid = {11140179},
title = {{Investigation of visual space using an exocentric pointing task}},
volume = {62},
year = {2000}
}
@article{Santoso2017,
abstract = {— In this paper, we comprehensively discuss the current progress of visual–inertial (VI) navigation systems and sensor fusion research with a particular focus on small unmanned aerial vehicles, known as microaerial vehicles (MAVs). Such fusion has become very topical due to the complementary characteristics of the two sensing modalities. We discuss the pros and cons of the most widely implemented VI systems against the navigational and maneuvering capabilities of MAVs. Considering the issue of optimum data fusion from multiple heterogeneous sensors, we examine the potential of the most widely used advanced state estimation techniques (both linear and nonlinear as well as Bayesian and non-Bayesian) against various MAV design considerations. Finally, we highlight several research opportunities and potential challenges associated with each technique. Note to Practitioners—Robotic aircraft have been widely implemented to improve safety, efficiency, and productivity (e.g., agriculture, law enforcement, building inspections, and so on). As a part of its autonomous navigation system, this review aims to address several aspects of VI navigation systems both from data fusion and technological perspectives. Index Terms— Microaerial vehicles (MAVs), sensor fusion, visual–inertial (VI) navigation systems.},
annote = {not really useful :(},
author = {Santoso, Fendy and Garratt, Matthew A. and Anavatti, Sreenatha G.},
doi = {10.1109/TASE.2016.2582752},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Santoso, Garratt, Anavatti - 2017 - Visual-inertial navigation systems for aerial robotics Sensor fusion and technology.pdf:pdf},
issn = {15455955},
journal = {IEEE Transactions on Automation Science and Engineering},
keywords = {Microaerial vehicles (MAVs),Sensor fusion,Visual-inertial (VI) navigation systems},
number = {1},
pages = {260--275},
title = {{Visual-inertial navigation systems for aerial robotics: Sensor fusion and technology}},
volume = {14},
year = {2017}
}
@article{Lowry2016,
abstract = {Visual place recognition is a challenging problem due to the vast range of ways in which the appearance of real-world places can vary. In recent years, improvements in visual sensing capabilities, an ever-increasing focus on long-term mobile robot autonomy, and the ability to draw on state-of-the-art research in other disciplines—particularly recognition in computer vision and animal navigation in neuroscience—have all contributed to significant advances in visual place recognition systems. This paper presents a survey of the visual place recognition research landscape. We start by introducing the concepts behind place recognition—the role of place recognition in the animal kingdom, how a “place” is defined in a robotics context, and the major components of a place recognition system. Long-term robot operations have revealed that changing appearance can be a significant factor in visual place recognition failure; therefore, we discuss how place recognition solutions can implicitly or explicitly account for appearance change within the environment. Finally, we close with a discussion on the future of visual place recognition, in particular with respect to the rapid advances being made in the related fields of deep learning, semantic scene understanding, and video description.},
author = {Lowry, Stephanie and S{\"{u}}nderhauf, Niko and Newman, Paul and Leonard, John J. and Cox, David and Corke, Peter and Milford, Michael J.},
doi = {10.1109/TRO.2015.2496823},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Lowry et al. - 2016 - Visual Place Recognition A Survey.pdf:pdf},
issn = {15523098},
journal = {IEEE Transactions on Robotics},
keywords = {Place recognition,Visual place recognition,highlight},
mendeley-tags = {highlight},
number = {1},
pages = {1--19},
title = {{Visual Place Recognition: A Survey}},
volume = {32},
year = {2016}
}
@techreport{NTSB2017,
abstract = {This is a synopsis from the NTSB's report and does not include the Board's rationale for the conclusions, probable cause, and safety recommendations. NTSB staff is currently making final revisions to the report from which the attached conclusions and safety recommendations have been extracted. The final report and pertinent safety recommendation letters will be distributed to recommendation recipients as soon as possible. The attached information is subject to further review and editing to reflect changes adopted during the Board meeting.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {NTSB},
booktitle = {Highway Accident Report},
doi = {10.1093/jicru/ndl025},
eprint = {arXiv:1011.1669v3},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/NTSB - 2017 - Collision Between a Car Operating With Automated Vehicle Control Systems and a Tractor-Semitrailer Truck Near Williston, F.pdf:pdf},
isbn = {9789279304095},
issn = {1473-6691},
pmid = {16783161},
title = {{Collision Between a Car Operating With Automated Vehicle Control Systems and a Tractor-Semitrailer Truck Near Williston, Florida}},
url = {https://www.ntsb.gov/investigations/AccidentReports/Reports/HAR1702.pdf},
year = {2017}
}
@article{Hoy2015,
abstract = {ABSTRACT SUMMARY We review a range of techniques related to navigation of unmanned vehicles through unknown environments with obstacles, especially those that rigorously ensure collision avoidance (given certain assumptions about the system). This topic continues to be an active area of research, and we highlight some directions in which available approaches may be improved. The paper discusses models of the sensors and vehicle kinematics, assumptions about the environment, and performance criteria. Methods applicable to stationary obstacles, moving obstacles and multiple vehicles scenarios are all reviewed. In preference to global approaches based on full knowledge of the environment, particular attention is given to reactive methods based on local sensory data, with a special focus on recently proposed navigation laws based on model predictive and sliding mode control.},
author = {Hoy, Michael and Matveev, Alexey S and Savkin, Andrey V},
doi = {10.1017/S0263574714000289},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Hoy, Matveev, Savkin - 2015 - Algorithms for collision-free navigation of mobile robots in complex cluttered environments a survey.pdf:pdf},
isbn = {0263-5747},
issn = {1469-8668},
journal = {Robotica},
keywords = {deliberate obstacle avoidance,moving obstacles,obstacle avoidance,reactive obstacle avoidance,survey},
mendeley-tags = {deliberate obstacle avoidance,moving obstacles,obstacle avoidance,reactive obstacle avoidance,survey},
number = {03},
pages = {463--497},
title = {{Algorithms for collision-free navigation of mobile robots in complex cluttered environments: a survey}},
url = {http://journals.cambridge.org/article{\_}S0263574714000289},
volume = {33},
year = {2015}
}
@article{Humenberger2010,
abstract = {This paper introduces a new segmentation-based approach for disparity optimization in stereo vision. The main contribution is a significant enhancement of the matching quality at occlusions and textureless areas by segmenting either the left color image or the calculated texture image. The local cost calculation is done with a Census-based correlation method and is compared with standard sum of absolute differences. The confidence of a match is measured and only non-confident or non-textured pixels are estimated by calculating a disparity plane for the corresponding segment. The quality of the local optimized matches is increased by a modified Semi-Global Matching (SGM) step with subpixel accuracy. In contrast to standard SGM, not the whole image is used for disparity optimization but horizontal stripes of the image. It is shown that this modification significantly reduces the memory consumption by nearly constant matching quality and thus enables embedded realization. Using the Middlebury ranking as evaluation criterion, it is shown that the proposed algorithm performs well in comparison to the pure Census correlation. It reaches a top ten rank if subpixel accuracy is supposed. Furthermore, the matching quality of the algorithm, especially of the texture-based plane fitting, is shown on two real-world scenes where a significant enhancement could be achieved.},
author = {Humenberger, Martin and Engelke, Tobias and Kubinger, Wilfried},
doi = {10.1109/CVPRW.2010.5543769},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Humenberger, Engelke, Kubinger - 2010 - A Census-based stereo vision algorithm using modified Semi-Global Matching and plane fitting to.pdf:pdf},
isbn = {9781424470297},
issn = {2160-7508},
journal = {2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition - Workshops, CVPRW 2010},
pages = {77--84},
title = {{A Census-based stereo vision algorithm using modified Semi-Global Matching and plane fitting to improve matching quality}},
year = {2010}
}
@article{Fortun2015,
annote = {Image Understanding for Real-world Distributed Video Networks},
author = {Fortun, Denis and Bouthemy, Patrick and Kervrann, Charles},
doi = {https://doi.org/10.1016/j.cviu.2015.02.008},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Fortun, Bouthemy, Kervrann - 2015 - Optical flow modeling and computation A survey.pdf:pdf},
issn = {1077-3142},
journal = {Computer Vision and Image Understanding},
keywords = {Feature matching,Motion estimation,Occlusions,Optical flow,Optimization,Parametric models,Regularization},
number = {Supplement C},
pages = {1--21},
title = {{Optical flow modeling and computation: A survey}},
url = {http://www.sciencedirect.com/science/article/pii/S1077314215000429},
volume = {134},
year = {2015}
}
@inproceedings{Holzmann2016,
author = {Holzmann, Thomas and Fraundorfer, Friedrich and Bischof, Horst},
booktitle = {Proceedings of the 11th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications, 2016},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Holzmann, Fraundorfer, Bischof - 2016 - Direct Stereo Visual Odometry Based on Lines.pdf:pdf},
pages = {1--11},
title = {{Direct Stereo Visual Odometry Based on Lines}},
year = {2016}
}
@inproceedings{Sanfourche2013,
author = {Sanfourche, Martial and Vittori, Vincent and {Le Besnerais}, Guy},
booktitle = {Intelligent Robots and Systems (IROS), 2013 IEEE/RSJ International Conference on},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Sanfourche, Vittori, Le Besnerais - 2013 - eVO a realtime embedded stereo odometry for MAV applications.pdf:pdf},
isbn = {9781467363587},
pages = {2107--2114},
publisher = {IEEE},
title = {{eVO: a realtime embedded stereo odometry for MAV applications}},
year = {2013}
}
@article{Tola2010,
author = {Tola, Engin and Lepetit, Vincent and Fua, Pascal},
doi = {10.1109/TPAMI.2009.77},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Tola, Lepetit, Fua - 2010 - DAISY An Efficient Dense Descriptor Applied to Wide-Baseline Stereo.pdf:pdf},
issn = {0162-8828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
month = {may},
number = {5},
pages = {815--830},
title = {{DAISY: An Efficient Dense Descriptor Applied to Wide-Baseline Stereo}},
url = {http://ieeexplore.ieee.org/document/4815264/},
volume = {32},
year = {2010}
}
@article{VanHecke2017,
author = {van Hecke, Kevin and de Croon, Guido C.H.E. and Hennes, Daniel and Setterfield, Timothy P. and Saenz-Otero, Alvar and Izzo, Dario},
doi = {10.1016/j.actaastro.2017.07.038},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/van Hecke et al. - 2017 - Self-supervised learning as an enabling technology for future space exploration robots ISS experiments on mono.pdf:pdf},
issn = {00945765},
journal = {Acta Astronautica},
keywords = {persistent self-supervised learning},
month = {nov},
number = {February},
pages = {1--9},
title = {{Self-supervised learning as an enabling technology for future space exploration robots: ISS experiments on monocular distance learning}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0094576517302862},
volume = {140},
year = {2017}
}
@article{Baker2011,
abstract = {The quantitative evaluation of optical flow algorithms by Barron et al. led to significant advances in the performance of optical flow methods. The challenges for optical flow today go beyond the datasets and evaluation methods proposed in that paper and center on problems associated with nonrigid motion, real sensor noise, complex natural scenes, and motion discontinuities. Our goal is to establish a new set of benchmarks and evaluation methods for the next generation of optical flow algorithms. To that end, we contribute four types of data to test different aspects of optical flow algorithms: sequences with nonrigid motion where the ground-truth flow is determined by tracking hidden fluorescent texture; realistic synthetic sequences; high frame-rate video used to study interpolation error; and modified stereo sequences of static scenes. In addition to the average angular error used in Barron et al., we compute the absolute flow endpoint error, measures for frame interpolation error, improved statistics, and flow accuracy at motion boundaries and in textureless regions. We evaluate the performance of several well-known methods on this data to establish the current state of the art. Our database is freely available on the Web together with scripts for scoring and publication of the results at http://vision.middlebury.edu/flow/.},
archivePrefix = {arXiv},
arxivId = {1412.0767},
author = {Baker, Simon and Scharstein, Daniel and Lewis, J. P. and Roth, Stefan and Black, Michael J. and Szeliski, Richard},
doi = {10.1007/s11263-010-0390-2},
eprint = {1412.0767},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Baker et al. - 2011 - A database and evaluation methodology for optical flow.pdf:pdf},
isbn = {978-1-4244-1631-8},
issn = {09205691},
journal = {International Journal of Computer Vision},
keywords = {Algorithms,Benchmarks,Database,Evaluation,Metrics,Optical flow,Survey},
number = {1},
pages = {1--31},
pmid = {24356354},
title = {{A database and evaluation methodology for optical flow}},
volume = {92},
year = {2011}
}
@article{Mikolajczyk2005a,
author = {Mikolajczyk, K and Tuytelaars, T and Schmid, C and Zisserman, A and Matas, J and Schaffalitzky, F and Kadir, T and van Gool, L},
doi = {10.1007/s11263-005-3848-x},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Mikolajczyk et al. - 2005 - A Comparison of Affine Region Detectors.pdf:pdf},
journal = {Int J Comput Vision},
number = {1-2},
pages = {43--72},
title = {{A Comparison of Affine Region Detectors}},
volume = {65},
year = {2005}
}
@article{DeCroon2017,
archivePrefix = {arXiv},
arxivId = {arXiv:1709.08126v1},
author = {de Croon, G.C.H.E.},
eprint = {arXiv:1709.08126v1},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/de Croon - 2017 - Self-supervised learning When is fusion of the primary and secondary sensor cue useful.pdf:pdf},
journal = {arXiv preprint},
number = {arXiv:1709.08126},
title = {{Self-supervised learning: When is fusion of the primary and secondary sensor cue useful?}},
year = {2017}
}
@article{Vatavu2015,
abstract = {In this paper we present a stereovision-based approach for tracking multiple objects in crowded environments where, typically, the road lane markings are not visible and the surrounding infrastructure is not known. The proposed technique relies on measurement data provided by an intermediate occupancy grid derived from processing a stereovision-based elevation map and on free-form object delimiters extracted from this grid. Unlike other existing methods that track rigid objects using also rigid representations, we present a particle filter-based solution for tracking visual appearance-based free-form obstacle representations. At each step, the particle state is described by two components, i.e., the object's dynamic parameters and its estimated geometry. In order to solve the high-dimensionality state-space problem, a Rao-Blackwellized particle filter is used. By accurately modeling the object geometry using the polygonal lines instead of a 3-D box and, at the same time, separating the position and speed tracking from the geometry tracking at the estimator level, the proposed solution combines the efficiency of the rigid model with the benefits of a flexible object model.},
author = {Vatavu, Andrei and Danescu, Radu and Nedevschi, Sergiu},
doi = {10.1109/TITS.2014.2366248},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Vatavu, Danescu, Nedevschi - 2015 - Stereovision-based multiple object tracking in traffic scenarios using free-form obstacle delimiters.pdf:pdf},
isbn = {9781479929146},
issn = {15249050},
journal = {IEEE Transactions on Intelligent Transportation Systems},
keywords = {Object tracking,Rao-Blackwellization,particle filters,polygonal models,stereovision},
number = {1},
pages = {498--511},
title = {{Stereovision-based multiple object tracking in traffic scenarios using free-form obstacle delimiters and particle filters}},
volume = {16},
year = {2015}
}
@inproceedings{Eudes2018,
author = {Eudes, Alexandre and Marzat, Julien and Sanfourche, Martial and Moras, Julien and Bertrand, Sylvain},
booktitle = {Field and Service Robotics},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Eudes et al. - 2018 - Autonomous and Safe Inspection of an Industrial Warehouse by a Multi-rotor MAV.pdf:pdf},
pages = {221--235},
publisher = {Springer},
title = {{Autonomous and Safe Inspection of an Industrial Warehouse by a Multi-rotor MAV}},
year = {2018}
}
@article{Trzcinski2013,
abstract = {Binary key point descriptors provide an efficient alternative to their $\backslash$nfloating-point competitors as they enable faster processing while requiring less $\backslash$nmemory. In this paper, we propose a novel framework to learn an extremely $\backslash$ncompact binary descriptor we call Bin Boost that is very robust to illumination $\backslash$nand viewpoint changes. Each bit of our descriptor is computed with a boosted $\backslash$nbinary hash function, and we show how to efficiently optimize the different hash $\backslash$nfunctions so that they complement each other, which is key to compactness and $\backslash$nrobustness. The hash functions rely on weak learners that are applied directly $\backslash$nto the image patches, which frees us from any intermediate representation and $\backslash$nlets us automatically learn the image gradient pooling configuration of the $\backslash$nfinal descriptor. Our resulting descriptor significantly outperforms the $\backslash$nstate-of-the-art binary descriptors and performs similarly to the best $\backslash$nfloating-point descriptors at a fraction of the matching time and memory $\backslash$nfootprint.},
author = {Trzcinski, Tomasz and Christoudias, Mario and Fua, Pascal and Lepetit, Vincent},
doi = {10.1109/CVPR.2013.370},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Trzcinski et al. - 2013 - Boosting binary keypoint descriptors.pdf:pdf},
isbn = {978-0-7695-4989-7},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer  Vision and Pattern Recognition},
keywords = {Binary Embedding,Binary Local Feature Descriptors,Boosting},
pages = {2874--2881},
title = {{Boosting binary keypoint descriptors}},
year = {2013}
}
@article{Krajnik2010,
author = {Krajn{\'{i}}k, Tom{\'{a}}{\v{s}} and Faigl, Jan and Von{\'{a}}sek, Vojt{\v{e}}ch and Ko{\v{s}}nar, Karel and Kulich, Miroslav and Přeu{\v{c}}il, Libor},
doi = {10.1002/rob.20354},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Krajn{\'{i}}k et al. - 2010 - Simple yet stable bearing-only navigation.pdf:pdf},
journal = {Journal of Field Robotics},
number = {5},
pages = {511--533},
title = {{Simple yet stable bearing-only navigation}},
volume = {27},
year = {2010}
}
@article{Braillon2006,
author = {Braillon, Christophe and Pradalier, Cedric and Crowley, James L and Laugier, Christian and Gravir, Laboratoire and Rhone-alpes, Inria},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Braillon et al. - 2006 - Real-time moving obstacle detection using optical flow models.pdf:pdf},
journal = {Evaluation},
keywords = {monocular,moving obstacles,nieuwe referenties,obstacle detection,optical flow},
mendeley-tags = {monocular,moving obstacles,nieuwe referenties,obstacle detection,optical flow},
pages = {466--471},
title = {{Real-time moving obstacle detection using optical flow models}},
year = {2006}
}
@article{Burke2006,
abstract = {A compass based purely on visual information would be a useful navigational tool for robots working in environments where magnetic and GPS compass information is not available. A visual compass processes a pair of panoramic images taken from two different positions and estimates the orientation change between them. We present in this paper several well known algorithms useful for the development of a visual compass. These methods include a search for the minimum image distance in orientation space, a refined search method, and a frequencydomain approach known as phase correlation. A comparison on efficiency and effectiveness of these algorithms and their variations is given. We find that the refined search method exhibits improved efficiency with only a marginal impact on accuracy. The 2-D phase correlation method performed almost as well for small distances, but for large distances, the accuracy decreased significantly. The 1-D phase correlation performs better with respect to both accuracy and efficiency.},
author = {Burke, Andrew and Vardy, Andrew},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Burke, Vardy - 2006 - Visual compass methods for robot navigation.pdf:pdf},
journal = {Seventeenth annual Newfoundland electrical and computer engineering conf},
pages = {1--5},
title = {{Visual compass methods for robot navigation}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.71.6152{\&}rep=rep1{\&}type=pdf},
year = {2006}
}
@inproceedings{Bernini2014,
author = {Bernini, Nicola and Bertozzi, Massimo and Castangia, Luca and Patander, Marco and Sabbatelli, Mario},
booktitle = {Intelligent Transportation Systems (ITSC), 2014 IEEE 17th International Conference on},
doi = {10.1109/ITSC.2014.6957799},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Bernini et al. - 2014 - Real-Time Obstacle Detection using Stereo Vision for Autonomous Ground Vehicles A Survey.pdf:pdf},
isbn = {9781479960781},
number = {1},
pages = {873--878},
title = {{Real-Time Obstacle Detection using Stereo Vision for Autonomous Ground Vehicles: A Survey}},
year = {2014}
}
@incollection{Floreano2010,
abstract = {Aerodynamics, structural dynamics, and flight dynamics of natural flyers intersect with some of the richest problems in micro-air vehicles (MAVs), including massively unsteady three-dimensional separation, transition in boundary and shear layers, vortical flows, unsteady flight environment, aeroelasticity, and adaptive control being just a few examples. A challenge is that the scaling of both fluid dynamics and structural dynamics between smaller natural flyer and practical flying hardware/lab experiment (larger dimension) is fundamentally difficult. The interplay between flexible structures and aerodynamics motivated by the MAV development is discussed in this chapter. For fixed wings, membrane materials exhibit self-initiated vibration even in a steady free stream which lowers the effective angle of attack of the membrane structure compared to that of the rigid wing. For flapping wings, structural flexibility can enhance leading-edge suction via increasing the effective angle of attack, resulting in higher thrust generation. {\textcopyright} 2010 Springer-Verlag Berlin Heidelberg.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Zeil, Jochen and Boeddeker, Norbert and St{\"{u}}rzl, Wolfgang},
booktitle = {Flying Insects and Robots},
doi = {10.1007/978-3-540-89393-6},
eprint = {arXiv:1011.1669v3},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Zeil, Boeddeker, St{\"{u}}rzl - 2009 - Visual homing in insects and robots.pdf:pdf},
isbn = {9783540893929},
issn = {1098-6596},
pages = {87----100},
pmid = {25246403},
title = {{Visual homing in insects and robots}},
year = {2009}
}
@article{Ma2018,
abstract = {Depth completion, the technique of estimating a dense depth image from sparse depth measurements, has a variety of applications in robotics and autonomous driving. However, depth completion faces 3 main challenges: the irregularly spaced pattern in the sparse depth input, the difficulty in handling multiple sensor modalities (when color images are available), as well as the lack of dense, pixel-level ground truth depth labels. In this work, we address all these challenges. Specifically, we develop a deep regression model to learn a direct mapping from sparse depth (and color images) to dense depth. We also propose a self-supervised training framework that requires only sequences of color and sparse depth images, without the need for dense depth labels. Our experiments demonstrate that our network, when trained with semi-dense annotations, attains state-of-the- art accuracy and is the winning approach on the KITTI depth completion benchmark at the time of submission. Furthermore, the self-supervised framework outperforms a number of existing solutions trained with semi- dense annotations.},
archivePrefix = {arXiv},
arxivId = {1807.00275},
author = {Ma, Fangchang and Cavalheiro, Guilherme Venturelli and Karaman, Sertac},
eprint = {1807.00275},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Ma, Cavalheiro, Karaman - 2018 - Self-supervised Sparse-to-Dense Self-supervised Depth Completion from LiDAR and Monocular Camera.pdf:pdf},
journal = {arXiv preprint arXiv:1807.00275},
keywords = {rgb-d perception,sensor fusion,visual learning},
title = {{Self-supervised Sparse-to-Dense: Self-supervised Depth Completion from LiDAR and Monocular Camera}},
year = {2018}
}
@article{Cummins2008,
abstract = {This paper describes a probabilistic approach to the problem of recognizing places based on their appearance. The system we present is not limited to localization, but can determine that a new observation comes from a previously unseen place, and so augment its map. Effectively this is a SLAM system in the space of appearance. Our probabilistic approach allows us to explicitly account for perceptual aliasing in the environment--identical but indistinctive observations receive a low probability of having come from the same place. We achieve this by learning a generative model of place appearance. By partitioning the learning problem into two parts, new place models can be learned online from only a single observation of a place. The algorithm complexity is linear in the number of places in the map, and is particularly suitable for online loop closure detection in mobile robotics.},
author = {Cummins, M. and Newman, P.},
doi = {10.1177/0278364908090961},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Cummins, Newman - 2008 - FAB-MAP Probabilistic Localization and Mapping in the Space of Appearance.pdf:pdf},
isbn = {0278364908090},
issn = {0278-3649},
journal = {The International Journal of Robotics Research},
keywords = {Appearance-only SLAM,ap-,pearance based navigation,place recognition,topological slam},
mendeley-tags = {Appearance-only SLAM},
pages = {647--665},
title = {{FAB-MAP: Probabilistic Localization and Mapping in the Space of Appearance}},
volume = {27},
year = {2008}
}
@inproceedings{Geiger2011,
author = {Geiger, Andreas and Roser, Martin and Urtasun, Raquel},
booktitle = {Computer Vision – ACCV 2010},
doi = {10.1007/978-3-642-19315-6_3},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Geiger, Roser, Urtasun - 2011 - Efficient Large-Scale Stereo Matching.pdf:pdf},
isbn = {978-3-642-19314-9},
pages = {25--38},
title = {{Efficient Large-Scale Stereo Matching}},
year = {2011}
}
@inproceedings{Abeywardena2016,
archivePrefix = {arXiv},
arxivId = {1607.01486},
author = {Abeywardena, Dinuka and {Shoudong Huang} and Barnes, Ben and Dissanayake, Gamini and Kodagoda, Sarath},
booktitle = {2016 IEEE International Conference on Robotics and Automation (ICRA)},
doi = {10.1109/ICRA.2016.7487290},
eprint = {1607.01486},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Abeywardena et al. - 2016 - Fast, on-board, model-aided visual-inertial odometry system for quadrotor micro aerial vehicles.pdf:pdf},
isbn = {978-1-4673-8026-3},
month = {may},
pages = {1530--1537},
publisher = {IEEE},
title = {{Fast, on-board, model-aided visual-inertial odometry system for quadrotor micro aerial vehicles}},
url = {http://ieeexplore.ieee.org/document/7487290/},
year = {2016}
}
@article{Steux2010,
abstract = {This paper presents a Laser-SLAM algorithm which can be programmed in less than 200 lines C-language program. The first idea aimed to develop and implement a simple SLAM algorithm providing good performances without exceeding 200 lines in a C-language program. We use a robotic platform called MinesRover, a six wheels robot with several sensors. We based our work and calculations on a laser sensor and the odometry of the robot. The article presents the different capabilities of the platform and the way we use them in order to improve our programs. We also illustrates the difficulties encountered during the programming and testing of the algorithm. This work shows the possibility to perform complex tasks using simple and easily programmable algorithms.},
author = {Steux, Bruno and {El Hamzaoui}, Oussama},
doi = {10.1109/ICARCV.2010.5707402},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Steux, El Hamzaoui - 2010 - tinySLAM A SLAM algorithm in less than 200 lines C-language program.pdf:pdf},
isbn = {9781424478132},
journal = {11th International Conference on Control, Automation, Robotics and Vision, ICARCV 2010},
keywords = {Localization,Mapping,Particle filter,SLAM,low complexity,slam},
mendeley-tags = {low complexity,slam},
number = {December},
pages = {1975--1979},
title = {{tinySLAM: A SLAM algorithm in less than 200 lines C-language program}},
year = {2010}
}
@article{Shen2012,
abstract = {In this paper, we propose a stochastic differential equation-based exploration algorithm to enable exploration in three-dimensional indoor environments with a payload-constrained micro-aerial vehicle (MAV). We are able to address computation, memory, and sensor limitations by using a map representation which is dense for the known occupied space but sparse for the free space. We determine regions for further exploration based on the evolution of a stochastic differential equation that simulates the expansion of a system of particles with Newtonian dynamics. The regions of most significant particle expansion correlate to unexplored space. After identifying and processing these regions, the autonomous MAV navigates to these locations to enable fully autonomous exploration. The performance of the approach is demonstrated through numerical simulations and experimental results in single- and multi-floor indoor experiments.},
author = {Shen, Shaojie and Michael, Nathan and Kumar, Vijay},
doi = {10.1109/ICRA.2012.6225146},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Shen, Michael, Kumar - 2012 - Autonomous indoor 3D exploration with a micro-aerial vehicle.pdf:pdf},
isbn = {9781467314039},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
keywords = {exploration,slam},
mendeley-tags = {exploration,slam},
pages = {9--15},
title = {{Autonomous indoor 3D exploration with a micro-aerial vehicle}},
year = {2012}
}
@inproceedings{Olson2006,
author = {Olson, Edwin and Leonard, John and Teller, Seth},
booktitle = {Robotics and Automation, 2006. ICRA 2006. Proceedings 2006 IEEE International Conference on},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Olson, Leonard, Teller - 2006 - Fast Iterative Alignment of Pose Graphs with Poor Initial Estimates.pdf:pdf},
isbn = {0780395050},
number = {May},
pages = {2262--2269},
title = {{Fast Iterative Alignment of Pose Graphs with Poor Initial Estimates}},
year = {2006}
}
@inproceedings{Kerl2013,
author = {Kerl, Christian and Sturm, Jurgen and Cremers, Daniel},
booktitle = {2013 IEEE International Conference on Robotics and Automation},
doi = {10.1109/ICRA.2013.6631104},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Kerl, Sturm, Cremers - 2013 - Robust odometry estimation for RGB-D cameras.pdf:pdf},
isbn = {978-1-4673-5643-5},
month = {may},
pages = {3748--3754},
publisher = {IEEE},
title = {{Robust odometry estimation for RGB-D cameras}},
url = {http://ieeexplore.ieee.org/document/6631104/},
year = {2013}
}
@article{Oliva2001,
author = {Oliva, Aude and Torralba, Antonio},
doi = {10.1023/A:1011139631724},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Oliva, Torralba - 2001 - Modeling the Shape of the Scene A Holistic Representation of the Spatial Envelope.pdf:pdf},
issn = {09205691},
journal = {International journal of computer vision},
keywords = {energy spectrum,natural images,principal components,scene recognition,spatial layout},
number = {3},
pages = {145--175},
title = {{Modeling the Shape of the Scene: A Holistic Representation of the Spatial Envelope}},
volume = {42},
year = {2001}
}
@incollection{Oliva2006,
abstract = {Humans can recognize the gist of a novel image in a single glance, independent of its complexity. How is this remarkable feat accomplished? On the basis of behavioral and computational evidence, this paper describes a formal approach to the representation and the mechanism of scene gist understanding, based on scenecentered, rather than objectcentered primitives. We show that the structure of a scene image can be estimated by the mean of global image features, providing a statistical summary of the spatial layout properties (Spatial Envelope representation) of the scene. Global features are based on configurations of spatial scales and are estimated without invoking segmentation or grouping operations. The scenecentered approach is not an alternative to local image analysis but would serve as a feedforward and parallel pathway of visual processing, able to quickly constrain local feature analysis and enhance object recognition in cluttered natural scenes.},
author = {Oliva, Aude and Torralba, Antonio},
booktitle = {Progress in Brain Research},
doi = {10.1016/S0079-6123(06)55002-2},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Oliva, Torralba - 2006 - Building the gist of a scene the role of global image features in recognition.pdf:pdf},
issn = {00796123},
keywords = {GIST},
mendeley-tags = {GIST},
pages = {23--36},
title = {{Building the gist of a scene: the role of global image features in recognition}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0079612306550022},
volume = {155},
year = {2006}
}
@article{Mejias2006,
author = {Mejias, L. and Campoy, Pascual and Saripalli, S. and Sukhatme, G.S.},
doi = {10.1109/ROBOT.2006.1642078},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Mejias et al. - 2006 - A visual servoing approach for tracking features in urban areas using an autonomous helicopter.pdf:pdf},
isbn = {0-7803-9505-0},
journal = {Proceedings 2006 IEEE International Conference on Robotics and Automation, 2006. ICRA 2006.},
keywords = {a visual servoing approach,an autonomous helicopter,ceedings of the 2006,florida - may 2006,for tracking features,ieee international conference on,in urban areas using,orlando,robotics and automation},
number = {May},
pages = {2503--2508},
title = {{A visual servoing approach for tracking features in urban areas using an autonomous helicopter}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1642078},
year = {2006}
}
@inproceedings{Zhou2017,
author = {Zhou, Tinghui and Brown, Matthew and Snavely, Noah and Lowe, David G},
booktitle = {CVPR},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Zhou et al. - 2017 - Unsupervised Learning of Depth and Ego-Motion from Video.pdf:pdf},
pages = {7},
title = {{Unsupervised Learning of Depth and Ego-Motion from Video}},
year = {2017}
}
@article{Wyeth2009,
abstract = {The paper discusses robot navigation from biological inspiration. The authors sought to build a model of the rodent brain that is suitable for practical robot navigation. The core model, dubbed RatSLAM, has been demonstrated to have exactly the same advantages described earlier: it can build, maintain, and use maps simultaneously over extended periods of time and can construct maps of large and complex areas from very weak geometric information. The work contrasts with other efforts to embody models of rat brains in robots. The article describes the key elements of the known biology of the rat brain in relation to navigation and how the RatSLAM model captures the ideas from biology in a fashion suitable for implementation on a robotic platform. The paper then outline RatSLAM's performance in two difficult robot navigation challenges, demonstrating how a cognitive robotics approach to navigation can produce results that rival other state of the art approaches in robotics.},
author = {Wyeth, G. and Milford, M.},
doi = {10.1109/MRA.2009.933620},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Wyeth, Milford - 2009 - Spatial cognition for robots.pdf:pdf},
issn = {1070-9932},
journal = {IEEE Robotics {\&} Automation Magazine},
keywords = {Neurorobotics,SLAM,biologically inspired robots,learning and adaptive systems},
number = {3},
pages = {24--32},
title = {{Spatial cognition for robots}},
volume = {16},
year = {2009}
}
@article{Yousif2015,
abstract = {This paper is intended to pave the way for new researchers in the field of robotics and autonomous systems, particularly thosewho are interested in robot localization and mapping. We discuss the fundamentals of robot navigation requirements and provide a reviewof the state of the art tech- niques that form the bases of established solutions formobile robots localization and mapping. The topicswediscuss range from basic localization techniques such as wheel odometry and dead reckoning, to the more advance Visual Odometry (VO) and Simultaneous Localization and Mapping (SLAM) techniques. We discuss VO in both monocular and stereo vision systems using feature matching/tracking and optical flow techniques.We discuss and compare the basics of most common SLAM methods such as the Extended Kalman Fil- ter SLAM (EKF-SLAM), Particle Filter and the most recent RGB-D SLAM. We also provide techniques that form the building blocks to those methods such as feature extraction (i.e. SIFT, SURF, FAST), feature matching, outlier removal and data association techniques.},
author = {Yousif, Khalid and Bab-Hadiashar, Alireza and Hoseinnezhad, Reza},
doi = {10.1007/s40903-015-0032-7},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Yousif, Bab-Hadiashar, Hoseinnezhad - 2015 - An Overview to Visual Odometry and Visual SLAM Applications to Mobile Robotics.pdf:pdf},
isbn = {2199-854X},
issn = {2363-6912},
journal = {Intelligent Industrial Systems},
keywords = {Autonomous,Mapping,Navigation,RGB-D,Visual Odometry,Visual SLAM,autonomous,mapping,navigation,rgb-d,visual odometry,visual slam},
number = {4},
pages = {289--311},
publisher = {Springer Singapore},
title = {{An Overview to Visual Odometry and Visual SLAM: Applications to Mobile Robotics}},
url = {http://link.springer.com/10.1007/s40903-015-0032-7},
volume = {1},
year = {2015}
}
@article{Chao2014,
abstract = {Optical flow has been widely used by insects and birds to support navigation functions. Such information has appealing capabilities for application to ground and aerial robots, espe-cially for navigation and collision avoidance in urban or indoor areas. The purpose of this paper is to provide a survey of existing optical flow techniques for robotics navigation applications. Detailed comparisons are made among different optical-flow-aided navigation solutions with em-phasis on the sensor hardware as well as optical flow motion models. A summary of current re-search status and future research directions are further discussed.},
author = {Chao, Haiyang and Gu, Yu and Napolitano, Marcello},
doi = {10.1007/s10846-013-9923-6},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Chao, Gu, Napolitano - 2014 - A survey of optical flow techniques for robotics navigation applications.pdf:pdf},
isbn = {9781479908172},
issn = {15730409},
journal = {Journal of Intelligent and Robotic Systems: Theory and Applications},
keywords = {Optical flow,Robotics navigation,Robotics sensing,Unmanned aerial vehicles,survey},
mendeley-tags = {survey},
number = {1-4},
pages = {361--372},
title = {{A survey of optical flow techniques for robotics navigation applications}},
volume = {73},
year = {2014}
}
@article{Cumming2001,
author = {Cumming, B. G. and DeAngelis, G. C.},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Cumming, DeAngelis - 2001 - The Physiology of Stereopsis.pdf:pdf},
journal = {Annu. Rev. Neurosci.},
keywords = {binocular vision,disparity,extrastriate cortex,striate cortex},
pages = {203--238},
pmid = {11283310},
title = {{The Physiology of Stereopsis}},
volume = {24},
year = {2001}
}
@article{Ye2019,
archivePrefix = {arXiv},
arxivId = {arXiv:1901.07647v1},
author = {Ye, Jong Chul and Sung, Woon Kyoung},
eprint = {arXiv:1901.07647v1},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Ye, Sung - 2019 - Understanding Geometry of Encoder-Decoder CNNs.pdf:pdf},
title = {{Understanding Geometry of Encoder-Decoder CNNs}},
year = {2019}
}
@article{Long,
author = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Long, Shelhamer, Darrell - Unknown - Fully Convolutional Networks for Semantic Segmentation.pdf:pdf},
title = {{Fully Convolutional Networks for Semantic Segmentation}}
}
@inproceedings{Lee2012a,
author = {Lee, Changmin and Kim, Daeeun},
booktitle = {Control, Automation and Systems (ICCAS), 2012 12th International Conference on},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Lee, Kim - 2012 - Visual Navigation using Pixel Intensity Information.pdf:pdf},
isbn = {9788993215045},
keywords = {intensity,landmark arrangement,landmark pixel,local-,reference map,snapshot model,visual navigation},
pages = {1457--1460},
title = {{Visual Navigation using Pixel Intensity Information}},
year = {2012}
}
@misc{Lucas1981,
abstract = {Image registration finds a variety of applications in computer vision. Unfortunately, traditional image registration techniques tend to be costly. We present a new image registration technique that makes use of the spatial intensity gradient of the images to find a good match using a type of Newton-Raphson iteration. Our technique is taster because it examines far fewer potential matches between the images than existing techniques Furthermore, this registration technique can be generalized to handle rotation, scaling and shearing. We show how our technique can be adapted tor use in a stereo vision system.},
author = {Lucas, Bruce D. and Kanade, Takeo},
booktitle = {Proceedings of the 7th International Joint Conference on Artificial Intelligence (IJCAI)},
doi = {Doi 10.1145/358669.358692},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Lucas, Kanade - 1981 - An iterative image registration technique with an application to stereo vision.pdf:pdf},
isbn = {0001-0782},
issn = {0001-0782},
pages = {674--679},
title = {{An iterative image registration technique with an application to stereo vision}},
url = {http://www.ri.cmu.edu/pub{\_}files/pub3/lucas{\_}bruce{\_}d{\_}1981{\_}1/lucas{\_}bruce{\_}d{\_}1981{\_}1.ps.gz},
volume = {2},
year = {1981}
}
@book{Bristeau2011,
abstract = {ABSTRACT$\backslash$nThis paper exposes the Navigation and Control technology embedded in a recently commercialized micro Unmanned Aerial Vehicle (UAV), the AR.Drone, which cost and performance are unprecedented among any commercial product for mass markets. The system relies on state-of-the-art indoor navigation systems combining low-cost inertial sensors, computer vision techniques, sonar, and accounting for aerodynamics models.},
author = {Bristeau, Pierre Jean and Callou, Fran{\c{c}}ois and Vissi{\`{e}}re, David and Petit, Nicolas},
booktitle = {IFAC Proceedings Volumes (IFAC-PapersOnline)},
doi = {10.3182/20110828-6-IT-1002.02327},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Bristeau et al. - 2011 - The Navigation and Control technology inside the AR.Drone micro UAV.pdf:pdf},
isbn = {9783902661937},
issn = {14746670},
keywords = {Data fusion,Flying robots,Low cost MEMS,ardrone},
mendeley-tags = {ardrone},
number = {PART 1},
pages = {1477--1484},
publisher = {IFAC},
title = {{The Navigation and Control technology inside the AR.Drone micro UAV}},
url = {http://dx.doi.org/10.3182/20110828-6-IT-1002.02327},
volume = {18},
year = {2011}
}
@article{Szegedy2013,
abstract = {Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties. First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains of the semantic information in the high layers of neural networks. Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extend. We can cause the network to misclassify an image by applying a certain imperceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.},
archivePrefix = {arXiv},
arxivId = {1312.6199},
author = {Szegedy, Christian and Zaremba, Wojciech and Sutskever, Ilya and Bruna, Joan and Erhan, Dumitru and Goodfellow, Ian and Fergus, Rob},
eprint = {1312.6199},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Szegedy et al. - 2013 - Intriguing properties of neural networks.pdf:pdf},
journal = {arXiv preprint arXiv:1312.6199},
title = {{Intriguing properties of neural networks}},
year = {2013}
}
@inproceedings{Matsumoto2000a,
author = {Matsumoto, Y. and Sakai, K. and Inaba, Masayuki and Inoue, Hirochika},
booktitle = {Proceedings. 2000 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2000) (Cat. No.00CH37113)},
doi = {10.1109/IROS.2000.895217},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Matsumoto et al. - 2000 - View-based approach to robot navigation.pdf:pdf},
isbn = {0-7803-6348-5},
pages = {1702--1708},
publisher = {IEEE},
title = {{View-based approach to robot navigation}},
url = {http://ieeexplore.ieee.org/document/895217/},
volume = {3},
year = {2000}
}
@article{Murillo2010,
author = {Murillo, A C and Campos, P and Kosecka, J and Guerrero, J J},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Murillo et al. - 2010 - Gist vocabularies in omnidirectional images for appearance based mapping and localization.pdf:pdf},
journal = {10th OMNIVIS, held with Robotics: Science and Systems (RSS)},
title = {{Gist vocabularies in omnidirectional images for appearance based mapping and localization}},
volume = {3},
year = {2010}
}
@article{Lee1999,
abstract = {An extension of the infomax algorithm of Bell and Sejnowski (1995) is presented that is able blindly to separate mixed signals with sub- and supergaussian source distributions. This was achieved by using a simple type of learning rule first derived by Girolami (1997) by choosing negentropy as a projection pursuit index. Parameterized probability distributions that have sub- and supergaussian regimes were used to derive a general learning rule that preserves the simple architecture proposed by Bell and Sejnowski (1995), is optimized using the natural gradient by Amari (1998), and uses the stability analysis of Cardoso and Laheld (1996) to switch between sub- and supergaussian regimes. We demonstrate that the extended infomax algorithm is able to separate 20 sources with a variety of source distributions easily. Applied to high-dimensional data from electroencephalographic recordings, it is effective at separating artifacts such as eye blinks and line noise from weaker electrical signals that arise from sources in the brain.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Lee, T W and Girolami, M and Sejnowski, T J},
doi = {10.1162/089976699300016719},
eprint = {arXiv:1011.1669v3},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Lee, Girolami, Sejnowski - 1999 - Independent component analysis using an extended infomax algorithm for mixed subgaussian and supergaus.pdf:pdf},
isbn = {0899-7667 (Print)$\backslash$r0899-7667 (Linking)},
issn = {0899-7667},
journal = {Neural computation},
pages = {417--441},
pmid = {9950738},
title = {{Independent component analysis using an extended infomax algorithm for mixed subgaussian and supergaussian sources.}},
volume = {11},
year = {1999}
}
@article{Smeur2016,
author = {Smeur, Ewoud J. J. and Chu, Qiping and de Croon, Guido C. H. E.},
doi = {10.2514/1.G001490},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Smeur, Chu, de Croon - 2016 - Adaptive Incremental Nonlinear Dynamic Inversion for Attitude Control of Micro Air Vehicles.pdf:pdf},
journal = {Journal of Guidance, Control, and Dynamics},
keywords = {indi},
mendeley-tags = {indi},
number = {3},
pages = {450--461},
title = {{Adaptive Incremental Nonlinear Dynamic Inversion for Attitude Control of Micro Air Vehicles}},
volume = {39},
year = {2016}
}
@article{Rublee2011,
abstract = {Feature matching is at the base of many computer vision problems, such as object recognition or structure from motion. Current methods rely on costly descriptors for detection and matching. In this paper, we propose a very fast binary descriptor based on BRIEF, called ORB, which is rotation invariant and resistant to noise. We demonstrate through experiments how ORB is at two orders of magnitude faster than SIFT, while performing as well in many situations. The efficiency is tested on several real-world applications, including object detection and patch-tracking on a smart phone.},
author = {Rublee, Ethan and Rabaud, Vincent and Konolige, Kurt and Bradski, Gary},
doi = {10.1109/ICCV.2011.6126544},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Rublee et al. - 2011 - ORB An efficient alternative to SIFT or SURF.pdf:pdf},
isbn = {9781457711015},
issn = {1550-5499},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
pages = {2564--2571},
pmid = {20033598},
title = {{ORB: An efficient alternative to SIFT or SURF}},
year = {2011}
}
@article{Ranftl2016,
author = {Ranftl, Rene and Vineet, Vibhav and Chen, Qifeng and Koltun, Vladlen},
doi = {10.1109/CVPR.2016.440},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Ranftl et al. - 2016 - Dense Monocular Depth Estimation in Complex Dynamic Scenes.pdf:pdf},
isbn = {9781467388511},
issn = {10636919},
journal = {Cvpr},
pages = {4058--4066},
title = {{Dense Monocular Depth Estimation in Complex Dynamic Scenes}},
year = {2016}
}
@book{Stm32dsp,
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Unknown - 2010 - UM0585 User manual - STM32F10x DSP library.pdf:pdf},
publisher = {STMicroelectronics},
title = {{UM0585 User manual - STM32F10x DSP library}},
year = {2010}
}
@article{Anzai2010,
abstract = {Neural mechanisms underlying depth perception are reviewed with respect to three computational goals: determining surface depth order, gauging depth intervals, and representing 3D surface geometry and object shape. Accumulating evidence suggests that these three computational steps correspond to different stages of cortical processing. Early visual areas appear to be involved in depth ordering, while depth intervals, expressed in terms of relative disparities, are likely represented at intermediate stages. Finally, 3D surfaces appear to be processed in higher cortical areas, including an area in which individual neurons encode 3D surface geometry, and a population of these neurons may therefore represent 3D object shape. How these processes are integrated to form a coherent 3D percept of the world remains to be understood. {\textcopyright} 2010 Elsevier Ltd.},
author = {Anzai, Akiyuki and DeAngelis, Gregory C.},
doi = {10.1016/j.conb.2010.04.006},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Anzai, DeAngelis - 2010 - Neural computations underlying depth perception.pdf:pdf},
isbn = {1873-6882 (Electronic)$\backslash$r0959-4388 (Linking)},
issn = {09594388},
journal = {Current Opinion in Neurobiology},
number = {3},
pages = {367--375},
pmid = {20451369},
publisher = {Elsevier Ltd},
title = {{Neural computations underlying depth perception}},
url = {http://dx.doi.org/10.1016/j.conb.2010.04.006},
volume = {20},
year = {2010}
}
@article{Lin2016,
abstract = {In this paper, we establish a theoretical connection between the classical Lucas {\&} Kanade (LK) algorithm and the emerging topic of Spatial Transformer Networks (STNs). STNs are of interest to the vision and learning communities due to their natural ability to combine alignment and classification within the same theoretical framework. Inspired by the Inverse Compositional (IC) variant of the LK algorithm, we present Inverse Compositional Spatial Transformer Networks (IC-STNs). We demonstrate that IC-STNs can achieve better performance than conventional STNs with less model capacity; in particular, we show superior performance in pure image alignment tasks as well as joint alignment/classification problems on real-world problems.},
archivePrefix = {arXiv},
arxivId = {1612.03897},
author = {Lin, Chen-Hsuan and Lucey, Simon},
doi = {10.1109/CVPR.2017.242},
eprint = {1612.03897},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Lin, Lucey - 2016 - Inverse Compositional Spatial Transformer Networks(2).pdf:pdf},
title = {{Inverse Compositional Spatial Transformer Networks}},
url = {http://arxiv.org/abs/1612.03897},
year = {2016}
}
@inproceedings{Nguyen2014,
abstract = {— This paper presents a vision-based qualitative 3D navigation technique as well as first results of adapting Funnel Lane theory into path-following control of quadrotor aerial ve-hicle. The image's Kanade-Lucas-Tomasi (KLT) corner features are detected along the reference path in order to build a funnel lane for navigation. Then a funnel-lane navigation calculation is developed to estimate the desired yaw angle and height for the next movement. The proposed algorithm uses the front camera, heading measurement and altimeter of the Ar.Drone quadrotor for navigation. The remarkable advantage of the proposed technique is independently working in GPS-denied environments without the support of the external tracking sys-tem as well as computationally efficient. As compared to other available approaches, at-least one matched feature is required during path following. The proposed navigation technique can be implemented for visual-homing, visual-servoing and visual-teach-and-repeat (VT{\&}R) applications. The proposed method is simulated in ROS and Gazebo simulator followed by a realtime experiment with the Ar.Drone quadrotor.},
author = {Nguyen, Trung and Mann, George K I and Gosine, Raymond G},
booktitle = {2014 International Conference on Unmanned Aircraft Systems (ICUAS)},
doi = {10.1109/ICUAS.2014.6842281},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Nguyen, Mann, Gosine - 2014 - Vision-based qualitative path-following control of quadrotor aerial vehicle.pdf:pdf},
isbn = {978-1-4799-2376-2},
month = {may},
pages = {412--417},
publisher = {IEEE},
title = {{Vision-based qualitative path-following control of quadrotor aerial vehicle}},
url = {http://ieeexplore.ieee.org/document/6842281/},
year = {2014}
}
@article{Basiri2014,
author = {Basiri, M and Schill, F and Floreano, D and Lima, P U},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Basiri et al. - 2014 - Audio-based Bearing-only Localization for Swarms of Micro Air Vehicles.pdf:pdf},
isbn = {9781479936854},
keywords = {aerial vehicles},
pages = {4729--4734},
title = {{Audio-based Bearing-only Localization for Swarms of Micro Air Vehicles}},
year = {2014}
}
@article{Lecun2015,
abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
archivePrefix = {arXiv},
arxivId = {arXiv:1312.6184v5},
author = {Lecun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
doi = {10.1038/nature14539},
eprint = {arXiv:1312.6184v5},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Lecun, Bengio, Hinton - 2015 - Deep learning.pdf:pdf},
isbn = {9780521835688},
issn = {14764687},
journal = {Nature},
number = {7553},
pages = {436--444},
pmid = {10463930},
title = {{Deep learning}},
volume = {521},
year = {2015}
}
@article{Scherer2007,
abstract = {Safe autonomous flight is essential for widespread acceptance of aircraft that must fly close to the ground. We have developed a method of collision avoidance that can be used in three dimensions in much the same way as autonomous ground vehicles that navigate over unexplored terrain. Safe navigation is accomplished by a combination of online environmental sensing, path planning and collision avoidance. Here we report results with an autonomous helicopter that operates at low elevations in uncharted environments some of which are densely populated with obstacles such as buildings, trees and wires. We have recently completed over 1000 successful runs in which the helicopter traveled between coarsely specified waypoints separated by hundreds of meters, at speeds up to 10 meters/sec at elevations of 5-10 meters above ground level. The helicopter safely avoids large objects like buildings and trees but also wires as thin as 6 mm. We believe this represents the first time an air vehicle has traveled this fast so close to obstacles. Here we focus on the collision avoidance method that learns to avoid obstacles by observing the performance of a human operator.},
annote = {Onboard, maar wel op een UAV met wingspan van 3m. SLAM, voxels, planning. Gebruikt Ladar. Niet geschikt voor MAVs.},
author = {Scherer, Sebastian and Singh, Sanjiv and Chamberlain, Lyle and Saripalli, Srikanth},
doi = {10.1109/ROBOT.2007.363619},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Scherer et al. - 2007 - Flying fast and low among obstacles.pdf:pdf},
isbn = {1424406021},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
keywords = {slam},
mendeley-tags = {slam},
number = {April},
pages = {2023--2029},
title = {{Flying fast and low among obstacles}},
year = {2007}
}
@article{Borenstein1989,
author = {Borenstein, J and Koren, Y},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Borenstein, Koren - 1989 - Real-time obstacle avoidance for fast mobile robots.pdf:pdf},
journal = {{\{}IEEE{\}} {\{}T{\}}ransactions on {\{}S{\}}ystems, {\{}M{\}}an, {\&} {\{}C{\}}ybernetics},
number = {5},
pages = {1179--1187},
title = {{{\{}R{\}}eal-time obstacle avoidance for fast mobile robots}},
volume = {19},
year = {1989}
}
@phdthesis{Lamers2016,
author = {Lamers, K},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Lamers - 2016 - Self-Supervised Monocular Distance Learning on a Lightweight Micro Air Vehicle.pdf:pdf},
school = {Delft University of Technology},
title = {{Self-Supervised Monocular Distance Learning on a Lightweight Micro Air Vehicle}},
type = {MSc},
year = {2016}
}
@article{Hillis2004,
abstract = {How does the visual system combine information from different depth cues to estimate three-dimensional scene parameters? We tested a maximum-likelihood estimation (MLE) model of cue combination for perspective (texture) and binocular disparity cues to surface slant. By factoring the reliability of each cue into the combination process, MLE provides more reliable estimates of slant than would be available from either cue alone. We measured the reliability of each cue in isolation across a range of slants and distances using a slant-discrimination task. The reliability of the texture cue increases as |slant| increases and does not change with distance. The reliability of the disparity cue decreases as distance increases and varies with slant in a way that also depends on viewing distance. The trends in the single-cue data can be understood in terms of the information available in the retinal images and issues related to solving the binocular correspondence problem. To test the MLE model, we measured perceived slant of two-cue stimuli when disparity and texture were in conflict and the reliability of slant estimation when both cues were available. Results from the two-cue study indicate, consistent with the MLE model, that observers weight each cue according to its relative reliability: Disparity weight decreased as distance and |slant| increased. We also observed the expected improvement in slant estimation when both cues were available. With few discrepancies, our data indicate that observers combine cues in a statistically optimal fashion and thereby reduce the variance of slant estimates below that which could be achieved from either cue alone. These results are consistent with other studies that quantitatively examined the MLE model of cue combination. Thus, there is a growing empirical consensus that MLE provides a good quantitative account of cue combination and that sensory information is used in a manner that maximizes the precision of perceptual estimates.},
author = {Hillis, James M. and Watt, Simon J. and Landy, Michael S. and Banks, Martin S.},
doi = {10.1167/4.12.1},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Hillis et al. - 2004 - Slant from texture and disparity cues Optimal cue combination.pdf:pdf},
isbn = {1534-7362 (Electronic)},
issn = {1534-7362},
journal = {Journal of Vision},
keywords = {bayesian perception,cue combination,depth perception,stereopsis,texture gradient},
month = {dec},
number = {12},
pages = {1},
pmid = {15669906},
title = {{Slant from texture and disparity cues: Optimal cue combination}},
url = {http://jov.arvojournals.org/article.aspx?doi=10.1167/4.12.1 http://www.physiology.org/doi/10.1152/jn.1999.81.3.1355},
volume = {4},
year = {2004}
}
@article{Chen2013,
abstract = {A dynamic path-planning algorithm is proposed for routing unmanned air vehicles (UAVs) in order to track ground targets under path constraints, wind effects, and obstacle avoidance requirements. We first present the tangent vector field guidance (TVFG) and the Lyapunov vector field guidance (LVFG) algorithms. We demonstrate that the TVFG outperforms the LVFG as long as a tangent line is available between the UAV's turning circle and an objective circle, which is a desired orbit pattern over a target. Based on a hybrid version of the TVFG and LVFG, we then derive a theoretically shortest path algorithm with UAV operational constraints given a target position and the current UAV dynamic state. This algorithm has the efficiency of the TVFG when UAV is outside the standoff circle and the ability to follow the path via the LVFG when inside the standoff circle. In addition we adopt point-mass approximation of the target state probability density function (pdf) for target motion prediction by exploiting road network information and target dynamics as well as obstacle avoidance strategies. Overall, the proposed technical approach is practical and competitive, supported by solid theoretical analysis on several aspects of the algorithm performance. With extensive simulations we show that the tangent-plus-Lyapunov vector field guidance (T+LVFG) algorithm provides effective and robust tracking performance in various scenarios, including a target moving according to waypoints or a random kinematics model in an environment that may include obstacles and/or winds.},
author = {Chen, Hongda and Chang, Kuochu and Agate, Craig S.},
doi = {10.1109/TAES.2013.6494384},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Chen, Chang, Agate - 2013 - UAV path planning with tangent-plus-lyapunov vector field guidance and obstacle avoidance.pdf:pdf},
isbn = {0018-9251},
issn = {00189251},
journal = {IEEE Transactions on Aerospace and Electronic Systems},
number = {2},
pages = {840--856},
title = {{UAV path planning with tangent-plus-lyapunov vector field guidance and obstacle avoidance}},
volume = {49},
year = {2013}
}
@article{Swaminathan2007,
abstract = {Catadioptric imaging systems consisting of conventional lens based cameras and mirrors are widely used in many vision applications. Most of the prior work in this area has focused either on aspects of mirror design or the application it is being used for. In contrast, little attention has been paid to the aspect of sharp image formation. In this paper we study how reflections in curved mirrors affect image formation from the perspective of lens focus. In particular we answer the question "How and where must the lens focus when imaging reflections in curved mirrors?". Using caustics to model the reflections, we show that the space of infinite scene depth is compressed within a finite volume which we call the "caustic volume". Analysis of the caustic volume is presented for common catadioptric systems. We also present a framework to derive the "optimal" focal plane at which to focus the lens for sharp image formation along with experimental verification.},
author = {Swaminathan, Rahul},
doi = {10.1109/ICCV.2007.4409205},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Swaminathan - 2007 - Focus in catadioptric imaging systems.pdf:pdf},
isbn = {978-1-4244-1630-1},
issn = {1550-5499},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
title = {{Focus in catadioptric imaging systems}},
year = {2007}
}
@article{Simo-Serra2015,
abstract = {Deep learning has revolutionalized image-level tasks such as classification, but patch-level tasks, such as correspondence, still rely on hand-crafted features, e.g. SIFT. In this paper we use Convolutional Neural Net-works (CNNs) to learn discriminant patch representations and in particular train a Siamese network with pairs of (non-)corresponding patches. We deal with the large num-ber of potential pairs with the combination of a stochastic sampling of the training set and an aggressive mining strat-egy biased towards patches that are hard to classify. By using the L 2 distance during both training and test-ing we develop 128-D descriptors whose euclidean dis-tances reflect patch similarity, and which can be used as a drop-in replacement for any task involving SIFT. We demon-strate consistent performance gains over the state of the art, and generalize well against scaling and rotation, perspec-tive transformation, non-rigid deformation, and illumina-tion changes. Our descriptors are efficient to compute and amenable to modern GPUs, and are publicly available.},
author = {Simo-Serra, Edgar and Trulls, Eduard and Ferraz, Luis and Kokkinos, Iasonas and Fua, Pascal and Moreno-Noguer, Francesc},
doi = {10.1109/ICCV.2015.22},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Simo-Serra et al. - 2015 - Discriminative Learning of Deep Convolutional Feature Point Descriptors.pdf:pdf},
isbn = {978-1-4673-8391-2},
issn = {15505499},
journal = {International Conference on Computer Vision},
pages = {118--126},
title = {{Discriminative Learning of Deep Convolutional Feature Point Descriptors}},
url = {http://infoscience.epfl.ch/record/213228},
year = {2015}
}
@article{Nister2006,
abstract = {A recognition scheme that scales efficiently to a large number of objects is presented. The efficiency and quality is exhibited in a live demonstration that recognizes CD-covers from a database of 40000 images of popular music CD{\&}{\#}146;s. The scheme builds upon popular techniques of indexing descriptors extracted from local regions, and is robust to background clutter and occlusion. The local region descriptors are hierarchically quantized in a vocabulary tree. The vocabulary tree allows a larger and more discriminatory vocabulary to be used efficiently, which we show experimentally leads to a dramatic improvement in retrieval quality. The most significant property of the scheme is that the tree directly defines the quantization. The quantization and the indexing are therefore fully integrated, essentially being one and the same. The recognition quality is evaluated through retrieval on a database with ground truth, showing the power of the vocabulary tree approach, going as high as 1 million images.},
author = {Nist{\'{e}}r, David and Stew{\'{e}}nius, Henrik},
doi = {10.1109/CVPR.2006.264},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Nist{\'{e}}r, Stew{\'{e}}nius - 2006 - Scalable recognition with a vocabulary tree.pdf:pdf},
isbn = {0769525970},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
keywords = {bag-of-words},
mendeley-tags = {bag-of-words},
pages = {2161--2168},
title = {{Scalable recognition with a vocabulary tree}},
volume = {2},
year = {2006}
}
@article{Schmidt2010,
abstract = {The detection and matching of feature points is crucial in many computer vision systems. Successful establishing of points correspondences between concurrent frames is important in such tasks as visual odometry, structure from motion or simultaneous localization and mapping. This paper compares of the performance of the well established, single scale detectors and descriptors and the increasingly popular, multi-scale approaches. {\textcopyright} 2010 Springer-Verlag Berlin Heidelberg.},
author = {Schmidt, Adam and Kraft, Marek and Kasi{\'{n}}ski, Andrzej},
doi = {10.1007/978-3-642-15907-7_31},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Schmidt, Kraft, Kasi{\'{n}}ski - 2010 - An evaluation of image feature detectors and descriptors for robot navigation.pdf:pdf},
isbn = {3642159060},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {salient features},
mendeley-tags = {salient features},
number = {PART 2},
pages = {251--259},
title = {{An evaluation of image feature detectors and descriptors for robot navigation}},
volume = {6375 LNCS},
year = {2010}
}
@article{Bell1995,
author = {Bell, Anthony J. and Sejnowski, Terrence J.},
doi = {10.1162/neco.1995.7.6.1129},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Bell, Sejnowski - 1995 - An Information-Maximization Approach to Blind Separation and Blind Deconvolution.pdf:pdf},
issn = {0899-7667},
journal = {Neural Computation},
month = {nov},
number = {6},
pages = {1129--1159},
title = {{An Information-Maximization Approach to Blind Separation and Blind Deconvolution}},
url = {file:///C:/MyHome/Recent{\_}paper/bell.blind.pdf http://www.mitpressjournals.org/doi/abs/10.1162/neco.1995.7.6.1129},
volume = {7},
year = {1995}
}
@article{Engel2013,
abstract = {We propose a fundamentally novel approach to real-time visual odometry for a monocular camera. It allows to ben- efit from the simplicity and accuracy of dense tracking – which does not depend on visual features – while running in real-time on a CPU. The key idea is to continuously estimate a semi-dense inverse depth map for the current frame, which in turn is used to track the motion of the camera using dense image alignment. More specifically, we estimate the depth of all pixels which have a non-negligible image gradient. Each estimate is represented as a Gaussian probability distribution over the inverse depth. We propagate this information over time, and update it with new measurements as new images arrive. In terms of tracking accuracy and computational speed, the proposed method compares favorably to both state-of-the-art dense and feature-based visual odometry and SLAM algorithms. As our method runs in real-time on a CPU, it is of large practical value for robotics and augmented reality applications. 1.Towards},
author = {Engel, Jakob and Sturm, Jurgen and Cremers, Daniel},
doi = {10.1109/ICCV.2013.183},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Engel, Sturm, Cremers - 2013 - Semi-dense visual odometry for a monocular camera.pdf:pdf},
isbn = {9781479928392},
issn = {1550-5499},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
keywords = {SLAM,dense,monocular,stereo,visual odometry},
pages = {1449--1456},
pmid = {6130318},
title = {{Semi-dense visual odometry for a monocular camera}},
year = {2013}
}
@article{Cummins2011,
abstract = {The paper discusses robot navigation from biological inspiration. The authors sought to build a model of the rodent brain that is suitable for practical robot navigation. The core model, dubbed RatSLAM, has been demonstrated to have exactly the same advantages described earlier: it can build, maintain, and use maps simultaneously over extended periods of time and can construct maps of large and complex areas from very weak geometric information. The work contrasts with other efforts to embody models of rat brains in robots. The article describes the key elements of the known biology of the rat brain in relation to navigation and how the RatSLAM model captures the ideas from biology in a fashion suitable for implementation on a robotic platform. The paper then outline RatSLAM's performance in two difficult robot navigation challenges, demonstrating how a cognitive robotics approach to navigation can produce results that rival other state of the art approaches in robotics.},
archivePrefix = {arXiv},
arxivId = {0010-0285/97},
author = {Cummins, M. and Newman, P.},
doi = {10.1177/0278364910385483},
eprint = {97},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Cummins, Newman - 2011 - Appearance-only SLAM at large scale with FAB-MAP 2.0.pdf:pdf},
isbn = {0010-0285 (Print)$\backslash$r0010-0285 (Linking)},
issn = {0278-3649},
journal = {The International Journal of Robotics Research},
keywords = {Appearance-based localization,Appearance-only SLAM,Bayes methods,Bayesian filtering framework,Cameras,Cognitive mapping,Databases,FAB-MAP,FABMAP,Fingerprints of places,Generalization,Humans,Image recognition,Image representation,Models,Multi-modal perception,Neurorobotics,Pattern Recognition,Photic Stimulation,Photic Stimulation: methods,Psychological,Recognition,Recognition (Psychology),Robotics,Robots,SLAM,Social Environment,Space Perception,Space Perception: physiology,Spatial representations,Stimulus,Stimulus: physiology,Topological navigation,Urban areas,Visual,Visual Fields,Visual Fields: physiology,Vocabulary,appearance based localization,appearance-based localization and mapping,appearance-based navigation,autonomous robotic systems,biologically inspired,biologically inspired robots,computer vision,energy spectrum,fab-map,filtering theory,gist descriptor,global descriptors,image features,image representation,invariant robust feature,learning and adaptive systems,localization,location recognition,loop detection,natural images,omnidirectional images,panoramic datasets,panoramic gist descriptor,persistent navigation and mapping,principal components,ratslam,recognition,robot navigation,robot vision,robotics,scene recognition,slam,spatial layout,topological slam,urban environment localization,vision based topological localization},
mendeley-tags = {Appearance-only SLAM},
number = {9},
pages = {1100--1123},
pmid = {396576},
primaryClass = {0010-0285},
title = {{Appearance-only SLAM at large scale with FAB-MAP 2.0}},
url = {http://ijr.sagepub.com/cgi/doi/10.1177/0278364910385483},
volume = {30},
year = {2011}
}
@article{Plagemann2010,
abstract = {We present a novel approach to estimating depth from single omnidirectional camera images by learning the relationship between visual features and range measurements available during a training phase. Our model not only yields the most likely distance to obstacles in all directions, but also the predictive uncertainties for these estimates. This information can be utilized by a mobile robot to build an occupancy grid map of the environment or to avoid obstacles during exploration-tasks that typically require dedicated proximity sensors such as laser range finders or sonars. We show in this paper how an omnidirectional camera can be used as an alternative to such range sensors. As the learning engine, we apply Gaussian processes, a nonparametric approach to function regression, as well as a recently developed extension for dealing with input-dependent noise. In practical experiments carried out in different indoor environments with a mobile robot equipped with an omnidirectional camera system, we demonstrate that our system is able to estimate range with an accuracy comparable to that of dedicated sensors based on sonar or infrared light. ?? 2010 Elsevier B.V. All rights reserved.},
author = {Plagemann, Christian and Stachniss, Cyrill and Hess, J{\"{u}}rgen and Endres, Felix and Franklin, Nathan},
doi = {10.1016/j.robot.2010.02.008},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Plagemann et al. - 2010 - A nonparametric learning approach to range sensing from omnidirectional vision.pdf:pdf},
isbn = {0921-8890},
issn = {09218890},
journal = {Robotics and Autonomous Systems},
keywords = {Gaussian processes,Learning,Omnidirectional vision,Range sensing},
number = {6},
pages = {762--772},
publisher = {Elsevier B.V.},
title = {{A nonparametric learning approach to range sensing from omnidirectional vision}},
url = {http://dx.doi.org/10.1016/j.robot.2010.02.008},
volume = {58},
year = {2010}
}
@inproceedings{Jiang2017,
abstract = {As an agent moves through the world, the apparent motion of scene elements is (usually) inversely proportional to their depth. It is natural for a learning agent to associate image patterns with the magnitude of their displacement over time: as the agent moves, faraway mountains don't move much; nearby trees move a lot. This natural relationship between the appearance of objects and their motion is a rich source of information about the world. In this work, we start by training a deep network, using fully automatic supervision, to predict relative scene depth from single images. The relative depth training images are automatically derived from simple videos of cars moving through a scene, using recent motion segmentation techniques, and no human-provided labels. This proxy task of predicting relative depth from a single image induces features in the network that result in large improvements in a set of downstream tasks including semantic segmentation, joint road segmentation and car detection, and monocular (absolute) depth estimation, over a network trained from scratch. The improvement on the semantic segmentation task is greater than those produced by any other automatically supervised methods. Moreover, for monocular depth estimation, our unsupervised pre-training method even outperforms supervised pre-training with ImageNet. In addition, we demonstrate benefits from learning to predict (unsupervised) relative depth in the specific videos associated with various downstream tasks. We adapt to the specific scenes in those tasks in an unsupervised manner to improve performance. In summary, for semantic segmentation, we present state-of-the-art results among methods that do not use supervised pre-training, and we even exceed the performance of supervised ImageNet pre-trained models for monocular depth estimation, achieving results that are comparable with state-of-the-art methods.},
archivePrefix = {arXiv},
arxivId = {1712.04850},
author = {Jiang, Huaizu and Learned-Miller, Erik and Larsson, Gustav and Maire, Michael and Shakhnarovich, Greg},
booktitle = {Proceedings of the European Conference on Computer Vision (ECCV)},
eprint = {1712.04850},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Jiang et al. - 2018 - Self-Supervised Relative Depth Learning for Urban Scene Understanding.pdf:pdf},
keywords = {monocular depth estimation,self-supervised learning,semantic segmentation,understanding,unsupervised domain adaptation,urban scene},
pages = {19--35},
title = {{Self-Supervised Relative Depth Learning for Urban Scene Understanding}},
url = {http://arxiv.org/abs/1712.04850},
year = {2018}
}
@article{Schmid2013a,
abstract = {We introduce our new quadrotor platform for re- alizing autonomous navigation in unknown indoor/outdoor en- vironments. Autonomous waypoint navigation, obstacle avoid- ance and flight control is implemented on-board. The system does not require a special environment, artificial markers or an external reference system. We developed a monolithic, mechanically damped perception unit which is equipped with a stereo camera pair, an Inertial Measurement Unit (IMU), two processor-and an FPGA board. Stereo images are processed on the FPGA by the Semi-Global Matching algorithm. Keyframe- based stereo odometry is fused with IMU data compensating for time delays that are induced by the vision pipeline. The system state estimate is used for control and on-board 3D mapping. An operator can set waypoints in the map, while the quadrotor autonomously plans its path avoiding obstacles. We show experiments with the quadrotor flying from inside a building to the outside and vice versa, traversing a window and a door respectively. A video of the experiments is part of this work. To the best of our knowledge, this is the first autonomously flying system with complete on-board processing that performs waypoint navigation with obstacle avoidance in geometrically unconstrained, complex indoor/outdoor environments.},
author = {Schmid, Korbinian and Tomic, Teodor and Ruess, Felix and Hirschmuller, Heiko and Suppa, Michael},
doi = {10.1109/IROS.2013.6696922},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Schmid et al. - 2013 - Stereo vision based indooroutdoor navigation for flying robots.pdf:pdf},
isbn = {9781467363587},
issn = {21530858},
journal = {IEEE International Conference on Intelligent Robots and Systems},
pages = {3955--3962},
title = {{Stereo vision based indoor/outdoor navigation for flying robots}},
year = {2013}
}
@phdthesis{Hillen2013,
author = {Hillen, Lorenz},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Hillen - 2013 - From Local Visual Homing Towards Navigation of Autonomous Cleaning Robots.pdf:pdf},
keywords = {highlight},
mendeley-tags = {highlight},
school = {Universit{\"{a}}t Bielefeld},
title = {{From Local Visual Homing Towards Navigation of Autonomous Cleaning Robots}},
year = {2013}
}
@article{Sturm2012,
abstract = {In this paper, we present a novel benchmark for the evaluation of RGB-D SLAM systems. We recorded a large set of image sequences from a Microsoft Kinect with highly accurate and time-synchronized ground truth camera poses from a motion capture system. The sequences contain both the color and depth images in full sensor resolution (640 × 480) at video frame rate (30 Hz). The ground-truth trajectory was obtained from a motion-capture system with eight high-speed tracking cameras (100 Hz). The dataset consists of 39 sequences that were recorded in an office environment and an industrial hall. The dataset covers a large variety of scenes and camera motions. We provide sequences for debugging with slow motions as well as longer trajectories with and without loop closures. Most sequences were recorded from a handheld Kinect with unconstrained 6-DOF motions but we also provide sequences from a Kinect mounted on a Pioneer 3 robot that was manually navigated through a cluttered indoor environment. To stimulate the comparison of different approaches, we provide automatic evaluation tools both for the evaluation of drift of visual odometry systems and the global pose error of SLAM systems. The benchmark website [1] contains all data, detailed descriptions of the scenes, specifications of the data formats, sample code, and evaluation tools.},
author = {Sturm, Jrgen and Engelhard, Nikolas and Endres, Felix and Burgard, Wolfram and Cremers, Daniel},
doi = {10.1109/IROS.2012.6385773},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Sturm et al. - 2012 - A benchmark for the evaluation of RGB-D SLAM systems(2).pdf:pdf},
isbn = {9781467317375},
issn = {21530858},
journal = {IEEE International Conference on Intelligent Robots and Systems},
pages = {573--580},
pmid = {6385773},
title = {{A benchmark for the evaluation of RGB-D SLAM systems}},
year = {2012}
}
@article{Sturzl2008,
author = {St{\"{u}}rzl, Wolfgang and Cheung, Allen and Cheng, Ken and Zeil, Jochen},
doi = {10.1037/0097-7403.34.1.1},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/St{\"{u}}rzl et al. - 2008 - The Information Content of Panoramic Images I The Rotational Errors and the Similarity of Views in Rectangular E.pdf:pdf},
journal = {Journal of Experimental Psychology: Animal Behavior Processes},
keywords = {and human children have,at the corner,been trained to,box,close to one of,find a reward hidden,geometry,in learning the task,navigation,rats,rectangular,rhesus monkeys,spatial orientation,the corners of a,they made systematic errors,view-based homing},
number = {1},
pages = {1--14},
title = {{The Information Content of Panoramic Images I: The Rotational Errors and the Similarity of Views in Rectangular Experimental Arenas}},
volume = {34},
year = {2008}
}
@article{Mancini2016,
abstract = {Obstacle Detection is a central problem for any robotic system, and critical for autonomous systems that travel at high speeds in unpredictable environment. This is often achieved through scene depth estimation, by various means. When fast motion is considered, the detection range must be longer enough to allow for safe avoidance and path planning. Current solutions often make assumption on the motion of the vehicle that limit their applicability, or work at very limited ranges due to intrinsic constraints. We propose a novel appearance-based Object Detection system that is able to detect obstacles at very long range and at a very high speed ({\~{}}300Hz), without making assumptions on the type of motion. We achieve these results using a Deep Neural Network approach trained on real and synthetic images and trading some depth accuracy for fast, robust and consistent operation. We show how photo-realistic synthetic images are able to solve the problem of training set dimension and variety typical of machine learning approaches, and how our system is robust to massive blurring of test images.},
archivePrefix = {arXiv},
arxivId = {1607.06349},
author = {Mancini, Michele and Costante, Gabriele and Valigi, Paolo and Ciarfuglia, Thomas A.},
doi = {10.1109/IROS.2016.7759632},
eprint = {1607.06349},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Mancini et al. - 2016 - Fast robust monocular depth estimation for Obstacle Detection with fully convolutional networks.pdf:pdf},
isbn = {9781509037629},
issn = {21530866},
journal = {IEEE International Conference on Intelligent Robots and Systems},
pages = {4296--4303},
title = {{Fast robust monocular depth estimation for Obstacle Detection with fully convolutional networks}},
volume = {2016-Novem},
year = {2016}
}
@article{Swain1991,
abstract = {Computer vision is embracing a new research focus in which the aim is to develop visual skills for robots that allow them to interact with a dynamic, realistic environment. To achieve this aim, new kinds of vision algorithms need to be developed which run in real time and subserve the robot's goals. Two fundamental goals are determining the location of a known object. Color can be successfully used for both tasks. This article demonstrates that color histograms of multicolored objects provide a robust, efficient cue for indexing into a large database of models. It shows that color histograms are stable object representations in the presence of occlusion and over change in view, and that they can differentiate among a large number of objects. For solving the identification problem, it introduces a technique called Histogram Intersection, which matches model and image histograms and a fast incremental version of Histogram Intersection, which allows real-time indexing into a large database of stored models. For solving the location problem it introduces an algorithm called Histogram Backprojection, which performs this task efficiently in crowded scenes.},
author = {Swain, Michael J. and Ballard, Dana H.},
doi = {10.1007/BF00130487},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Swain, Ballard - 1991 - Color indexing.pdf:pdf},
isbn = {0920569115731405},
issn = {15731405},
journal = {International Journal of Computer Vision},
keywords = {Histogram - color},
mendeley-tags = {Histogram - color},
number = {1},
pages = {11--32},
title = {{Color indexing}},
volume = {7},
year = {1991}
}
@article{Yang2015,
abstract = {3D path planning of unmanned aerial vehicle (UAV) targets at finding an optimal and collision free path in a 3D cluttered environment while taking into account the geometric, physical and temporal constraints. Although a lot of works have been done to solve UAV 3D path planning problem, there lacks a comprehensive survey on this topic, let alone the recently published works that focus on this field. This paper analyses the most successful UAV 3D path planning algorithms that developed in recent years. This paper classifies the UAV 3D path planning methods into five categories, sampling-based algorithms, node-based algorithms, mathematical model based algorithms, Bio-inspired algorithms, and multi-fusion based algorithms. For each category a critical analysis and comparison is given. Furthermore a comprehensive applicable analysis for each kind of method is presented after considering its working mechanism and time complexity. Key},
author = {Yang, Liang and Qi, Juntong and Xiao, Jizhong and Yong, Xia},
doi = {10.1109/WCICA.2014.7053093},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Yang et al. - 2015 - A literature review of UAV 3D path planning.pdf:pdf},
isbn = {9781479958252},
journal = {Proceedings of the World Congress on Intelligent Control and Automation (WCICA)},
keywords = {Mathematic model based algorithms,Multi-fusion based algorithms,Node based algorithms,Sampling based algorithms,UAV 3D path planning,motion planning,survey},
mendeley-tags = {motion planning,survey},
number = {March},
pages = {2376--2381},
title = {{A literature review of UAV 3D path planning}},
volume = {2015-March},
year = {2015}
}
@inproceedings{Tarrio2015,
abstract = {In this work we present a novel algorithm for realtime visual odometry for a monocular camera. The main idea is to develop an approach between classical feature-based vi-sual odometry systems and modern direct dense/semi-dense methods, trying to benefit from the best attributes of both. Similar to feature-based systems, we extract information from the images, instead of working with raw image inten-sities as direct methods. In particular, the information ex-tracted are the edges present in the image, while the rest of the algorithm is designed to take advantage of the struc-tural information provided when pixels are treated as edges. Edge extraction is an efficient and higly parallelizable op-eration. The edge depth information extracted is dense enough to allow acceptable surface fitting, similar to mod-ern semi-dense methods. This is a valuable attribute that feature-based odometry lacks. Experimental results show that the proposed method has similar drift than state of the art feature-based and direct methods, and is a simple algo-rithm that runs at realtime and can be parallelized. Finally, we have also developed an inertial aided version that suc-cessfully stabilizes an unmanned air vehicle in complex in-door environments using only a frontal camera, while run-ning the complete solution in the embedded hardware on board the vehicle.},
author = {Tarrio, Juan Jose and Pedre, Sol},
booktitle = {2015 IEEE International Conference on Computer Vision (ICCV)},
doi = {10.1109/ICCV.2015.87},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Tarrio, Pedre - 2015 - Realtime Edge-Based Visual Odometry for a Monocular Camera.pdf:pdf},
isbn = {978-1-4673-8391-2},
keywords = {monocular},
mendeley-tags = {monocular},
month = {dec},
pages = {702--710},
publisher = {IEEE},
title = {{Realtime Edge-Based Visual Odometry for a Monocular Camera}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7410444},
year = {2015}
}
@article{Liu2016a,
abstract = {In this article, we tackle the problem of depth estimation from single monocular images. Compared with depth estimation using multiple images such as stereo depth perception, depth from monocular images is much more challenging. Prior work typically focuses on exploiting geometric priors or additional sources of information, most using hand-crafted features. Recently, there is mounting evidence that features from deep convolutional neural networks (CNN) set new records for various vision applications. On the other hand, considering the continuous characteristic of the depth values, depth estimations can be naturally formulated as a continuous conditional random field (CRF) learning problem. Therefore, here we present a deep convolutional neural field model for estimating depths from single monocular images, aiming to jointly explore the capacity of deep CNN and continuous CRF. In particular, we propose a deep structured learning scheme which learns the unary and pairwise potentials of continuous CRF in a unified deep CNN framework. We then further propose an equally effective model based on fully convolutional networks and a novel superpixel pooling method, which is {\$}\backslashsim 10{\$} times faster, to speedup the patch-wise convolutions in the deep model. With this more efficient model, we are able to design deeper networks to pursue better performance. Experiments on both indoor and outdoor scene datasets demonstrate that the proposed method outperforms state-of-the-art depth estimation approaches.},
archivePrefix = {arXiv},
arxivId = {1502.7411},
author = {Liu, Fayao and Shen, Chunhua and Lin, Guosheng and Reid, Ian},
doi = {10.1109/TPAMI.2015.2505283},
eprint = {1502.7411},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Liu et al. - 2016 - Learning Depth from Single Monocular Images Using Deep Convolutional Neural Fields.pdf:pdf},
isbn = {0162-8828},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Depth estimation,conditional random field (CRF),deep convolutional neural networks (CNN),fully convolutional networks,superpixel pooling},
number = {10},
pages = {2024--2039},
pmid = {26660697},
title = {{Learning Depth from Single Monocular Images Using Deep Convolutional Neural Fields}},
volume = {38},
year = {2016}
}
@incollection{Bottou2010,
abstract = {Being still in its early stages, operational risk modeling has, so far, mainly been concentrated on the marginal distributions of frequencies and severities within the context of the Loss Distribution Approach (LDA). In this study, drawing on a fairly large realworld data set, we analyze the effects of competing strategies for dependence modeling. In particular, we estimate tail dependence both via copulas as well as nonparametrically, and analyze its effect on aggregate riskcapital estimates.},
address = {Heidelberg},
author = {Bottou, L{\'{e}}on},
booktitle = {Proceedings of COMPSTAT'2010},
doi = {10.1007/978-3-7908-2604-3_16},
editor = {Lechevallier, Yves and Saporta, Gilbert},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Bottou - 2010 - Large-Scale Machine Learning with Stochastic Gradient Descent.pdf:pdf},
isbn = {978-3-7908-2603-6},
issn = {02692155},
keywords = {efficiency,online learning,stochastic gradient descent},
pages = {177--186},
publisher = {Physica-Verlag HD},
title = {{Large-Scale Machine Learning with Stochastic Gradient Descent}},
url = {http://link.springer.com/10.1007/978-3-7908-2604-3 http://www.springerlink.com/index/10.1007/978-3-7908-2604-3{\_}16},
year = {2010}
}
@article{Forster2014,
abstract = {We propose a semi-direct monocular visual odometry algorithm that is precise, robust, and faster than current state-of-the-art methods. The semi-direct approach eliminates the need of costly feature extraction and robust matching techniques for motion estimation. Our algorithm operates directly on pixel intensities, which results in subpixel precision at high frame-rates. A probabilistic mapping method that explicitly models outlier measurements is used to estimate 3D points, which results in fewer outliers and more reliable points. Precise and high frame-rate motion estimation brings increased robustness in scenes of little, repetitive, and high-frequency texture. The algorithm is applied to micro-aerial-vehicle state- estimation in GPS-denied environments and runs at 55 frames per second on the onboard embedded computer and at more than 300 frames per second on a consumer laptop. We call our approach SVO (Semi-direct Visual Odometry) and release our implementation as open-source software.},
author = {Forster, Christian and Pizzoli, Matia and Scaramuzza, Davide},
doi = {10.1109/ICRA.2014.6906584},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Forster, Pizzoli, Scaramuzza - 2014 - SVO Fast semi-direct monocular visual odometry.pdf:pdf},
isbn = {9781479936847},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
keywords = {Forster2014},
mendeley-tags = {Forster2014},
pages = {15--22},
pmid = {6576973927449638915},
title = {{SVO: Fast semi-direct monocular visual odometry}},
year = {2014}
}
@article{Fuentes-Pacheco2015,
abstract = {Visual SLAM (simultaneous localization and mapping) refers to the problem of using images, as the only source of external information, in order to establish the position of a robot, a vehicle, or a moving camera in an environment, and at the same time, con-struct a representation of the explored zone. SLAM is an essential task for the autonomy of a robot. Nowadays, the problem of SLAM is considered solved when range sensors such as lasers or sonar are used to built 2D maps of small static environments. However SLAM for dynamic, complex and large scale environments, using vision as the sole external sensor, is an active area of research. The computer vision techniques employed in visual SLAM, such as detection, description and matching of salient features, image recognition and retrieval, among others, are still susceptible of improvement. The objective of this article is to pro-vide new researchers in the field of visual SLAM a brief and comprehensible review of the state-of-the-art.},
author = {Fuentes-Pacheco, Jorge and Ruiz-Ascencio, Jos{\'{e}} and Rend{\'{o}}n-Mancha, Juan Manuel},
doi = {10.1007/s10462-012-9365-8},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Fuentes-Pacheco, Ruiz-Ascencio, Rend{\'{o}}n-Mancha - 2015 - Visual simultaneous localization and mapping a survey.pdf:pdf},
issn = {0269-2821},
journal = {Artificial Intelligence Review},
keywords = {highlight,slam,survey},
mendeley-tags = {highlight,slam,survey},
month = {jan},
number = {1},
pages = {55--81},
title = {{Visual simultaneous localization and mapping: a survey}},
url = {http://link.springer.com/10.1007/s10462-012-9365-8},
volume = {43},
year = {2015}
}
@article{Ta2014,
abstract = {We propose a solution towards the problem of autonomous flight in man-made indoor environments with a micro aerial vehicle (MAV), using a frontal camera, a downward-facing sonar, and odometry inputs. While steering an MAV towards distant features that we call vistas, we build a map of the environment in a parallel tracking and mapping fashion to infer the wall structure and avoid lateral collisions in real-time. Our framework overcomes the limitations of traditional monocular SLAM approaches that are prone to failure when operating in feature-poor environments and when the camera purely rotates. First, we overcome the common dependency on feature-rich environments by detecting Wall-Floor Features(WFFs), a novel type of low-dimensional landmarks that are specifically designed for man-made environments to capture the geometric structure of the scene. We show that WFFs not only reveal the structure of the scene, but can also be tracked reliably. Second, we cope with difficult robot motions and environments by fusing the visual data with odometry measurements in a principled manner. This allows the robot to continue tracking when it purely rotates and when it temporarily navigates across a completely featureless environment. We demonstrate our results on a small commercially available quad-rotor platform flying in a typical feature-poor indoor environment.},
annote = {Explore indoor areas by moving towards vistas. Possibly low complexity (at least for slam) because of the simplistic and sparse floor ceiling features.},
author = {Ta, D. N. and Ok, K. and Dellaert, F.},
doi = {10.1016/j.robot.2014.03.010},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Ta, Ok, Dellaert - 2014 - Vistas and parallel tracking and mapping with Wall-Floor Features Enabling autonomous flight in man-made envir.pdf:pdf},
isbn = {0921-8890},
issn = {09218890},
journal = {Robotics and Autonomous Systems},
keywords = {Autonomous navigation,Indoor,MAV,Monocular,Parallel tracking and mapping,Quadrotor,SLAM,Vistas,Wall-Floor Features,expansion rate,experiment,exploration,highlight,low complexity,monocular,scenario: corridor,slam},
mendeley-tags = {expansion rate,experiment,exploration,highlight,low complexity,monocular,scenario: corridor,slam},
number = {11},
pages = {1657--1667},
title = {{Vistas and parallel tracking and mapping with Wall-Floor Features: Enabling autonomous flight in man-made environments}},
volume = {62},
year = {2014}
}
@article{Strecha2012,
author = {Strecha, Christoph and Bronstein, A. M and Bronstein, Michael M and Fua, Pascal},
doi = {10.1109/TPAMI.2011.103},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Strecha et al. - 2012 - LDAHash Improved Matching with Smaller Descriptors.pdf:pdf},
issn = {0162-8828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
month = {jan},
number = {1},
pages = {66--78},
title = {{LDAHash: Improved Matching with Smaller Descriptors}},
url = {http://ieeexplore.ieee.org/document/5770264/},
volume = {34},
year = {2012}
}
@article{Collett2013,
abstract = {A wide variety of insects use spatial memories in behaviours like holding a position in air or flowing water, in returning to a place of safety, and in foraging. The Hymenoptera, in particular, have evolved life-histories requiring reliable spatial memories to support the task of provisioning their young. Behavioural experiments, primarily on social bees and ants, reveal the mechanisms by which these memories are employed for guidance to spatial goals and suggest how the memories, and the processing streams that use them, may be organized. We discuss three types of memory-based guidance which, together, can explain a large part of observed insect spatial behaviour. Two of these, alignment image-matching and positional image-matching, are based on an insect's remembered views of its surroundings: The first uses views to keep to a familiar heading and the second to head towards a familiar place. The third type of guidance is based on a process of path integration by which an insect monitors its distance and direction from its nest through odometric and compass information. To a large degree, these guidance mechanisms appear to involve modular computational systems. We discuss the lack of evidence for cognitive maps in insects, and in particular the evidence against a map based on path integration, in which view-based and path integration memories might be combined. We suggest instead that insects have a collective of separate guidance systems, which cooperate and train each other, and together provide reliable guidance over a range of conditions. ?? 2013 Elsevier Ltd.},
author = {Collett, Matthew and Chittka, Lars and Collett, Thomas S.},
doi = {10.1016/j.cub.2013.07.020},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Collett, Chittka, Collett - 2013 - Spatial memory in insect navigation.pdf:pdf},
isbn = {0960-9822},
issn = {09609822},
journal = {Current Biology},
number = {17},
pages = {R789--R800},
pmid = {24028962},
publisher = {Elsevier},
title = {{Spatial memory in insect navigation}},
url = {http://dx.doi.org/10.1016/j.cub.2013.07.020},
volume = {23},
year = {2013}
}
@article{Courbon2010,
abstract = {This paper presents a vision-based navigation strategy for a vertical take-off and landing (VTOL) unmanned aerial vehicle (UAV) using a single embedded camera observing natural landmarks. In the proposed approach, images of the environment are first sampled, stored and organized as a set of ordered key images (visual path) which provides a visual memory of the environment. The robot navigation task is then defined as a concatenation of visual path subsets (called visual route) linking the current observed image and a target image belonging to the visual memory. The UAV is controlled to reach each image of the visual route using a vision-based control law adapted to its dynamic model and without explicitly planning any trajectory. This framework is largely substantiated by experiments with an X4-flyer equipped with a fisheye camera. ?? 2010 Elsevier Ltd.},
author = {Courbon, Jonathan and Mezouar, Youcef and Gu{\'{e}}nard, Nicolas and Martinet, Philippe},
doi = {10.1016/j.conengprac.2010.03.004},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Courbon et al. - 2010 - Vision-based navigation of unmanned aerial vehicles.pdf:pdf},
isbn = {0967-0661},
issn = {09670661},
journal = {Control Engineering Practice},
keywords = {Monocular vision,UAV,Visual memory,Visual navigation},
number = {7},
pages = {789--799},
title = {{Vision-based navigation of unmanned aerial vehicles}},
volume = {18},
year = {2010}
}
@article{Baker1999,
abstract = {Conventional video cameras have limited fields of view which make$\backslash$nthem restrictive for certain applications in computational vision. A$\backslash$ncatadioptric sensor uses a combination of lenses and mirrors placed in a$\backslash$ncarefully arranged configuration to capture a much wider field of view.$\backslash$nWhen designing a catadioptric sensor, the shape of the mirror(s) should$\backslash$nideally be selected to ensure that the complete catadioptric system has$\backslash$na single effective viewpoint. In this paper, we derive the complete$\backslash$nclass of single-lens single-mirror catadioptric sensors which have a$\backslash$nsingle viewpoint and an expression for the spatial resolution of a$\backslash$ncatadioptric sensor in terms of the resolution of the camera used to$\backslash$nconstruct it. We also include a preliminary analysis of the defocus blur$\backslash$ncaused by the use of a curved mirror},
author = {Baker, S. and Nayar, S.K.},
doi = {10.1109/ICCV.1998.710698},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Baker, Nayar - 1999 - A theory of catadioptric image formation.pdf:pdf},
isbn = {81-7319-221-9},
journal = {Sixth International Conference on Computer Vision (IEEE Cat. No.98CH36271)},
number = {2},
pages = {35--42},
title = {{A theory of catadioptric image formation}},
url = {http://ieeexplore.ieee.org/document/710698/},
volume = {35},
year = {1999}
}
@article{Bouguet1981,
abstract = {Pyramid LK $\Upsilon$르},
archivePrefix = {arXiv},
arxivId = {3629719},
author = {Bouguet, Jean-yves and Tarasenko, Viacheslav and Lucas, Bruce D and Kanade, Takeo},
doi = {10.1109/HPDC.2004.1323531},
eprint = {3629719},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Bouguet et al. - 1981 - Pyramidal Implementation of the Lucas Kanade Feature Tracker Description of the algorithm.pdf:pdf},
isbn = {0966-842X (Print)$\backslash$n0966-842X (Linking)},
issn = {0966842X},
journal = {Imaging},
keywords = {image pyramids,images,lucas kanade algorithm,opencv library,pixel coordinate vector is,then the lower right,tracking and motion},
number = {x},
pages = {1--9},
pmid = {16140533},
title = {{Pyramidal Implementation of the Lucas Kanade Feature Tracker Description of the algorithm}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.49.2019{\&}rep=rep1{\&}type=pdf{\%}5Cnhttp://trac.assembla.com/dilz{\_}mgr/export/272/doc/ktl-tracking/algo{\_}tracking.pdf},
volume = {130},
year = {1981}
}
@article{Wang2017,
abstract = {We propose Stereo Direct Sparse Odometry (Stereo DSO) as a novel method for highly accurate real-time visual odometry estimation of large-scale environments from stereo cameras. It jointly optimizes for all the model parameters within the active window, including the intrinsic/extrinsic camera parameters of all keyframes and the depth values of all selected pixels. In particular, we propose a novel approach to integrate constraints from static stereo into the bundle adjustment pipeline of temporal multi-view stereo. Real-time optimization is realized by sampling pixels uniformly from image regions with sufficient intensity gradient. Fixed-baseline stereo resolves scale drift. It also reduces the sensitivities to large optical flow and to rolling shutter effect which are known shortcomings of direct image alignment methods. Quantitative evaluation demonstrates that the proposed Stereo DSO outperforms existing state-of-the-art visual odometry methods both in terms of tracking accuracy and robustness. Moreover, our method delivers a more precise metric 3D reconstruction than previous dense/semi-dense direct approaches while providing a higher reconstruction density than feature-based methods.},
archivePrefix = {arXiv},
arxivId = {1708.07878},
author = {Wang, Rui and Schw{\"{o}}rer, Martin and Cremers, Daniel},
doi = {10.1109/ICCV.2017.421},
eprint = {1708.07878},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Wang, Schw{\"{o}}rer, Cremers - 2017 - Stereo DSO Large-Scale Direct Sparse Visual Odometry with Stereo Cameras.pdf:pdf},
title = {{Stereo DSO: Large-Scale Direct Sparse Visual Odometry with Stereo Cameras}},
url = {http://arxiv.org/abs/1708.07878},
year = {2017}
}
@article{Cakir2017,
abstract = {We propose two deep neural network architectures for classification of arbitrary-length electrocardiogram (ECG) recordings and evaluate them on the atrial fibrillation (AF) classification data set provided by the PhysioNet/CinC Challenge 2017. The first architecture is a deep convolutional neural network (CNN) with averaging-based feature aggregation across time. The second architecture combines convolutional layers for feature extraction with long-short term memory (LSTM) layers for temporal aggregation of features. As a key ingredient of our training procedure we introduce a simple data augmentation scheme for ECG data and demonstrate its effectiveness in the AF classification task at hand. The second architecture was found to outperform the first one, obtaining an {\$}F{\_}1{\$} score of {\$}82.1{\$}{\%} on the hidden challenge testing set.},
archivePrefix = {arXiv},
arxivId = {1710.06122},
author = {Cakir, Emre and Parascandolo, Giambattista and Heittola, Toni and Huttunen, Heikki and Virtanen, Tuomas},
doi = {10.1109/TASLP.2017.2690575},
eprint = {1710.06122},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Cakir et al. - 2017 - Convolutional Recurrent Neural Networks for Polyphonic Sound Event Detection.pdf:pdf},
isbn = {9789881701282},
issn = {2329-9290},
journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
month = {jun},
number = {6},
pages = {1291--1303},
title = {{Convolutional Recurrent Neural Networks for Polyphonic Sound Event Detection}},
url = {http://arxiv.org/abs/1710.06122 http://www.cinc.org/archives/2017/pdf/070-060.pdf http://ieeexplore.ieee.org/document/7933050/},
volume = {25},
year = {2017}
}
@book{Gibson1950,
abstract = {The principal subject of this book is the visual perception of space. Chapters cover theories of perception, the visual field and visual world, formation of retinal images, a psychophysical theory of perception, stimulus variables for visual depth and distance, size and shape constancy, geometrical space and form, mean, learning, and spatial behavior. 121 references.},
address = {Oxford, England},
author = {Gibson, James J.},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Gibson - 1950 - The perception of the visual world.pdf:pdf},
publisher = {Houghton Mifflin},
title = {{The perception of the visual world}},
year = {1950}
}
@article{Serres2008,
abstract = {In our project on the autonomous guidance of Micro-Air Vehicles (MAVs) in confined indoor and outdoor environments, we have developed a vision based autopilot, with which a miniature hovercraft travels along a corridor by automatically controlling both its speed and its clearance from the walls. A hovercraft is an air vehicle endowed with natural roll and pitch stabilization characteristics, in which planar flight control systems can be developed conveniently. Our hovercraft is fully actuated by two rear and two lateral thrusters. It travels at a constant altitude (similar to 2 mm) and senses the environment by means of two lateral eyes that measure the right and left optic flows (OFs). The visuo-motor control system, which is called LORA III (Lateral Optic flow Regulation Autopilot, Mark III), is a dual OF regulator consisting of two intertwined feedback loops, each of which has its own OF set-point and controls the vehicle's translation in one degree of freedom (surge or sway). Our computer-simulated experiments show that the hovercraft can navigate along a straight or tapered corridor at a relatively high speed (up to 1 m/s). It also reacts to any major step perturbations in the lateral OF (provided by a moving wall) and to any disturbances caused by a tapered corridor. The minimalistic visual system (comprised of only 4 pixels) suffices for the hovercraft to be able to control both its clearance from the walls and its forward speed jointly, without ever measuring speed and distance. The non-emissive visual sensors and the simple control system developed here are suitable for use on MAVs with a permissible avionic payload of only a few grams. This study also accounts quantitatively for previous ethological findings on honeybees flying freely in a straight or tapered corridor.},
author = {Serres, J. and Dray, D. and Ruffier, F. and Franceschini, N.},
doi = {10.1007/s10514-007-9069-0},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Serres et al. - 2008 - A vision-based autopilot for a miniature air vehicle Joint speed control and lateral obstacle avoidance.pdf:pdf},
isbn = {0929-5593},
issn = {09295593},
journal = {Autonomous Robots},
keywords = {Autopilot,Biomimetics,Bionics,Biorobotics,Hovercraft,Insect flight,MAV (micro-air vehicle),Motion detection,OF (optic flow),Urban canyon navigation,experiment,low complexity,monocular,obstacle avoidance,optical flow,reactive obstacle avoidance,scenario: corridor,turn away from flow},
mendeley-tags = {experiment,low complexity,monocular,obstacle avoidance,optical flow,reactive obstacle avoidance,scenario: corridor,turn away from flow},
number = {1-2},
pages = {103--122},
title = {{A vision-based autopilot for a miniature air vehicle: Joint speed control and lateral obstacle avoidance}},
volume = {25},
year = {2008}
}
@article{Marzat2015,
abstract = {To cite this version: Julien Marzat, Julien Moras, Aur{\'{e}}lien Plyer, Alexandre Eudes, Pascal Morin. Vision-based lo-calization, mapping and control for autonomous MAV: EuRoC challenge results. 15th ONERA-DLR Aerospace Symposium (ODAS 2015), May 2015, Toulouse, France. {\textless}hal-01178916{\textgreater}},
author = {Marzat, Julien and Moras, Julien and Eudes, Alexandre and Morin, Pascal and Marzat, Julien and Moras, Julien and Eudes, Alexandre and Morin, Pascal},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Marzat et al. - 2015 - Vision-based localization, mapping and control for autonomous MAV EuRoC challenge results.pdf:pdf},
journal = {15th ONERADLR Aerospace Symposium (ODAS 2015)},
title = {{Vision-based localization, mapping and control for autonomous MAV: EuRoC challenge results}},
url = {https://hal.archives-ouvertes.fr/hal-01178916},
year = {2015}
}
@article{Mondrag??n2010,
abstract = {This paper presents an implementation of an aircraft pose and motion estimator using visual systems as the principal sensor for controlling an Unmanned Aerial Vehicle (UAV) or as a redundant system for an Inertial Measure Unit (IMU) and gyros sensors. First, we explore the applications of the unified theory for central catadioptric cameras for attitude and heading estimation, explaining how the skyline is projected on the catadioptric image and how it is segmented and used to calculate the UAV's attitude. Then we use appearance images to obtain a visual compass, and we calculate the relative rotation and heading of the aerial vehicle. Additionally, we show the use of a stereo system to calculate the aircraft height and to measure the UAV's motion. Finally, we present a visual tracking system based on Fuzzy controllers working in both a UAV and a camera pan and tilt platform. Every part is tested using the UAV COLIBRI platform to validate the different approaches, which include comparison of the estimated data with the inertial values measured onboard the helicopter platform and the validation of the tracking schemes on real flights.},
annote = {Skyline segmentation},
author = {Mondrag??n, Iv??n F. and Olivares-M??ndez, Miguel A. and Campoy, Pascual and Mart??nez, Carol and Mejias, Lu??s},
doi = {10.1007/s10514-010-9183-2},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Mondragn et al. - 2010 - Unmanned aerial vehicles UAVs attitude, height, motion estimation and control using visual systems.pdf:pdf},
isbn = {0929-5593},
issn = {09295593},
journal = {Autonomous Robots},
keywords = {Catadioptric systems,Fuzzy control,Motion estimation,Omnidirectional images,Stereo vision,Unmanned aerial vehicles (UAV),monocular,state estimation},
mendeley-tags = {monocular,state estimation},
number = {1},
pages = {17--34},
title = {{Unmanned aerial vehicles UAVs attitude, height, motion estimation and control using visual systems}},
volume = {29},
year = {2010}
}
@article{Thrun2002,
abstract = {This article provides a comprehensive introduction into the field of robotic mapping, with a focus on indoor mapping. It describes and compares various probabilistic techniques, as they are presently being applied to a vast array of mobile robot mapping problems. The history of robotic mapping is also described, along with an extensive list of open research problems.},
archivePrefix = {arXiv},
arxivId = {1004.4027},
author = {Thrun, Sebastian},
doi = {10.1126/science.298.5594.699f},
eprint = {1004.4027},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Thrun - 2002 - Robotic Mapping A Survey.pdf:pdf},
isbn = {9781558608115},
issn = {00368075},
journal = {Science},
keywords = {bayes filters,expectation maximization algorithm,exploration,filters,kalman,mobile robots,robotic mapping},
number = {February},
pages = {1--35},
pmid = {634412},
title = {{Robotic Mapping: A Survey}},
volume = {298},
year = {2002}
}
@inproceedings{Chum2007,
abstract = {This paper proposes and compares two novel schemes for near duplicate image and videoshot detection. The first approach is based on global hierarchical colour histograms, using Locality Sensitive Hashing for fast retrieval. The second approach uses local feature descriptors (SIFT) and for retrieval exploits techniques used in the information retrieval community to compute approximate set intersections between documents using a minHash algorithm. The requirements for nearduplicate images vary according to the application, and we address two types of near duplicate definition: (i) being perceptually identical (e.g. up to noise, discretization effects, small photometric distortions etc); and (ii) being images of the same 3D scene (so allowing for viewpoint changes and partial occlusion). We define two shots to be nearduplicates if they share a large percentage of nearduplicate frames. We focus primarily on scalability to very large image and video databases, where fast query processing is necessary. Both methods are designed so that only a small amount of data need be stored for each image. In the case of nearduplicate shot detection it is shown that a weak approximation to histogram matching, consuming substantially less storage, is sufficient for good results. We demonstrate our methods on the TRECVID 2006 data set which contains approximately 165 hours of video (about 17.8M frames with 146K key frames), and also on feature films and pop videos.},
address = {New York, New York, USA},
author = {Chum, Ondřej and Philbin, James and Isard, Michael and Zisserman, Andrew},
booktitle = {Proceedings of the 6th ACM international conference on Image and video retrieval - CIVR '07},
doi = {10.1145/1282280.1282359},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Chum et al. - 2007 - Scalable near identical image and shot detection.pdf:pdf},
isbn = {9781595937339},
pages = {549--556},
publisher = {ACM Press},
title = {{Scalable near identical image and shot detection}},
url = {http://dl.acm.org/citation.cfm?doid=1282280.1282359},
year = {2007}
}
@article{Lin,
abstract = {In this paper, we establish a theoretical connection between the classical Lucas {\&} Kanade (LK) algorithm and the emerging topic of Spatial Transformer Networks (STNs). STNs are of interest to the vision and learning communities due to their natural ability to combine alignment and classification within the same theoretical framework. Inspired by the Inverse Compositional (IC) variant of the LK algorithm, we present Inverse Compositional Spatial Transformer Networks (IC-STNs). We demonstrate that IC-STNs can achieve better performance than conventional STNs with less model capacity; in particular, we show superior performance in pure image alignment tasks as well as joint alignment/classification problems on real-world problems.},
archivePrefix = {arXiv},
arxivId = {1612.03897},
author = {Lin, Chen-Hsuan and Lucey, Simon},
eprint = {1612.03897},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Lin, Lucey - 2016 - Inverse Compositional Spatial Transformer Networks.pdf:pdf},
journal = {arXiv preprint arXiv:1612.03897},
month = {dec},
title = {{Inverse Compositional Spatial Transformer Networks}},
url = {http://arxiv.org/abs/1612.03897},
year = {2016}
}
@article{Erkent2017,
author = {Erkent, Ozgur and Shukla, Dadhichi and Piater, Justus},
doi = {10.1109/IROS.2017.8206357},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Erkent, Shukla, Piater - 2017 - Visual task outcome verification using deep learning.pdf:pdf},
isbn = {9781538626825},
issn = {21530866},
journal = {IEEE International Conference on Intelligent Robots and Systems},
keywords = {Visual Learning,Deep Learning in Robotics and Auto},
pages = {4821--4827},
title = {{Visual task outcome verification using deep learning}},
volume = {2017-Septe},
year = {2017}
}
@inproceedings{Beevers2008,
author = {Beevers, Kristopher R and Huang, Wesley H},
booktitle = {IEEE International Conference on Robotics {\&} Automation (ICRA 2008)},
doi = {10.1109/ROBOT.2006.1642043},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Beevers, Huang - 2008 - An Embedded Implementation of SLAM with Sparse Sensing.pdf:pdf},
keywords = {low complexity,slam},
mendeley-tags = {low complexity,slam},
title = {{An Embedded Implementation of SLAM with Sparse Sensing}},
year = {2008}
}
@article{Choset2001,
abstract = {This paper presents a new method for simultaneous localization and$\backslash$nmapping that exploits the topology of the robot's free space to localize$\backslash$nthe robot on a partially constructed map. The topology of the$\backslash$nenvironment is encoded in a topological map; the particular topological$\backslash$nmap used in this paper is the generalized Voronoi graph (GVG), which$\backslash$nalso encodes some metric information about the robot's environment, as$\backslash$nwell. In this paper, we present the low-level control laws that generate$\backslash$nthe GVG edges and nodes, thereby allowing for exploration of an unknown$\backslash$nspace. With these prescribed control laws, the GVG can be viewed as an$\backslash$narbitrator for a hybrid control system that determines when to invoke a$\backslash$nparticular low-level controller from a set of controllers all working$\backslash$ntoward the high-level capability of mobile robot exploration. The main$\backslash$ncontribution, however, is using the graph structure of the GVG, via a$\backslash$ngraph matching process, to localize the robot. Experimental results$\backslash$nverify the described work},
author = {Choset, Howie and Nagatani, Keiji},
doi = {10.1109/70.928558},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Choset, Nagatani - 2001 - Topological simultaneous localization and mapping (SLAM) Toward exact localization without explicit localizati.pdf:pdf},
isbn = {1042-296X},
issn = {1042296X},
journal = {IEEE Transactions on Robotics and Automation},
keywords = {Exploration,Localization,Mapping,Mobile robots,Motion planning,Tologoical maps,Voronoi diagrams,low complexity,slam,topological slam},
mendeley-tags = {low complexity,slam,topological slam},
number = {2},
pages = {125--137},
title = {{Topological simultaneous localization and mapping (SLAM): Toward exact localization without explicit localization}},
volume = {17},
year = {2001}
}
@article{Kanellakis2017,
author = {Kanellakis, Christoforos and Nikolakopoulos, George},
doi = {10.1007/s10846-017-0483-z},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Kanellakis, Nikolakopoulos - 2017 - Survey on Computer Vision for UAVs Current Developments and Trends.pdf:pdf},
journal = {Journal of Intelligent and Robotic Systems},
keywords = {Obstacle avoidance,SLAM,Target tracking,UAVs,Visual servoing,obstacle avoidance,slam,target tracking,uavs,visual servoing},
pages = {141--168},
publisher = {Journal of Intelligent {\&} Robotic Systems},
title = {{Survey on Computer Vision for UAVs: Current Developments and Trends}},
volume = {87},
year = {2017}
}
@inproceedings{Barry2015a,
abstract = {We present a novel stereo vision algorithm that is capable of obstacle detection on a mobile-CPU processor at 120 frames per second. Our system performs a subset of standard block-matching stereo processing, searching only for obstacles at a single depth. By using an onboard IMU and state-estimator, we can recover the position of obstacles at all other depths, building and updating a full depth-map at framerate. Here, we describe both the algorithm and our implementation on a high-speed, small UAV, flying at over 20 MPH (9 m/s) close to obstacles. The system requires no external sensing or computation and is, to the best of our knowledge, the first high-framerate stereo detection system running onboard a small UAV.},
archivePrefix = {arXiv},
arxivId = {1407.7091},
author = {Barry, Andrew J. and Tedrake, Russ},
booktitle = {International Conference on Robotics and Automation (ICRA)},
doi = {10.1109/ICRA.2015.7139617},
eprint = {1407.7091},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Barry, Tedrake - 2015 - Pushbroom Stereo for High-Speed Navigation in Cluttered Environments.pdf:pdf},
isbn = {9781479969227},
keywords = {cartesian map,experiment,low complexity,nieuwe referenties,obstacle detection,stereo vision,voxel map},
mendeley-tags = {cartesian map,experiment,low complexity,nieuwe referenties,obstacle detection,stereo vision,voxel map},
pages = {3046--3052},
title = {{Pushbroom Stereo for High-Speed Navigation in Cluttered Environments}},
url = {http://arxiv.org/abs/1407.7091},
year = {2015}
}
@inproceedings{Chum2008,
author = {Chum, O. and Philbin, James and Zisserman, Andrew},
booktitle = {Procedings of the British Machine Vision Conference 2008},
doi = {10.5244/C.22.50},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Chum, Philbin, Zisserman - 2008 - Near Duplicate Image Detection min-Hash and tf-idf Weighting.pdf:pdf},
isbn = {1-901725-36-7},
pages = {50.1--50.10},
publisher = {British Machine Vision Association},
title = {{Near Duplicate Image Detection: min-Hash and tf-idf Weighting}},
url = {http://www.bmva.org/bmvc/2008/papers/119.html},
year = {2008}
}
@article{Sinai1998,
author = {Sinai, Michael J and Ooi, Teng Leng and He, Zijiang J},
doi = {10.1038/26747},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Sinai, Ooi, He - 1998 - Terrain influences the accurate judgement of distance.pdf:pdf},
issn = {0028-0836},
journal = {Nature},
month = {oct},
number = {6701},
pages = {497--500},
title = {{Terrain influences the accurate judgement of distance}},
url = {http://www.nature.com/articles/26747},
volume = {395},
year = {1998}
}
@article{Eade2006,
author = {Eade, E and Drummond, T},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Eade, Drummond - 2006 - Edge Landmarks in Monocular SLAM.pdf:pdf},
journal = {British Machine Vision Conf. (BMVC)},
pages = {1--10},
title = {{Edge Landmarks in Monocular SLAM}},
year = {2006}
}
@article{Fleer2017,
author = {Fleer, David and M{\"{o}}ller, Ralf},
doi = {10.1016/j.robot.2016.12.001},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Fleer, M{\"{o}}ller - 2017 - Comparing holistic and feature-based visual methods for estimating the relative pose of mobile robots.pdf:pdf},
issn = {0921-8890},
journal = {Robotics and Autonomous Systems},
keywords = {visual pose estimation},
pages = {51--74},
publisher = {Elsevier B.V.},
title = {{Comparing holistic and feature-based visual methods for estimating the relative pose of mobile robots}},
url = {http://dx.doi.org/10.1016/j.robot.2016.12.001},
volume = {89},
year = {2017}
}
@article{Kraetzschmar2004,
abstract = {Probabilistic occupancy grids are a common and well-proven spatial$\backslash$nrepresentation used in robot mapping. However, representing very$\backslash$nlarge environments with high resolution can still impose prohibitive$\backslash$nmemory requirements. A quadtree is a well-known data structure able$\backslash$nto achieve compact representations of large two-dimensional binary$\backslash$narrays. We extend this idea and present probabilistic quadtrees for$\backslash$nefficient representation and storage of probabilistic occupancy maps.$\backslash$nWe also discuss the implementation of probabilistic quadtrees and$\backslash$ntheir integration into our robotic middleware system Miro.},
author = {Kraetzschmar, Gerhard K and Kraetzschmar, Gerhard K and {Pag{\`{e}}s Gassull}, Guillem and {Pag{\`{e}}s Gassull}, Guillem and Uhl, Klaus and Uhl, Klaus},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Kraetzschmar et al. - 2004 - Probabilistic Quadtrees for Variable-Resolution Mapping of Large Environments.pdf:pdf},
journal = {Proceedings of the 5th IFAC/EURON Symposium on Intelligent Autonomous Vehicles},
keywords = {Probabilistic Quadtrees,Robot Mapping,Robot Middleware},
title = {{Probabilistic Quadtrees for Variable-Resolution Mapping of Large Environments}},
url = {http://smart.informatik.uni-ulm.de/PUBLICATIONS/},
year = {2004}
}
@article{Yu2013,
abstract = {This paper presents a vision-based collision avoidance technique for small and miniature air vehicles ({\{}MAVs{\}}) using local-level frame mapping and path planning. Using computer vision algorithms, a depth map that represents the range and bearing to obstacles is obtained. Based on the depth map, we estimate the range, azimuth to, and height of obstacles using an extended Kalman filter that takes into account the correlations between obstacles. We then construct maps in the local-level frame using cylindrical coordinates for three dimensional path planning and plan Dubins paths using the rapidly-exploring random tree algorithm. The behavior of our approach is analyzed and the characteristics of the environments where the local path planning technique guarantees collision-free paths and maneuvers the {\{}MAV{\}} to a specific goal region are described. Numerical results show the proposed technique is successful in solving path planning and multiple obstacle avoidance problems for fixed wing {\{}MAVs{\}}.},
annote = {Polar mapping in local level frame. Kalman filter mapping en update vam obstakels.

Avoidance planning met RRT. Lang stuk om guaranteed obstacle free aan te tonen.},
author = {Yu, Huili and Beard, Randy},
doi = {10.1007/s10514-012-9314-z},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Yu, Beard - 2013 - A vision-based collision avoidance technique for micro air vehicles using local-level frame mapping and path planning.pdf:pdf},
isbn = {10.1007/s10514-012-9314-z},
issn = {09295593},
journal = {Autonomous Robots},
keywords = {Collision avoidance,Computer vision,Micro Air Vehicle,Path planning,deliberate obstacle avoidance,nieuwe referenties,obstacle avoidance,polar map,scenario: city,simulation,stereo vision},
mendeley-tags = {deliberate obstacle avoidance,nieuwe referenties,obstacle avoidance,polar map,scenario: city,simulation,stereo vision},
number = {1-2},
pages = {93--109},
title = {{A vision-based collision avoidance technique for micro air vehicles using local-level frame mapping and path planning}},
volume = {34},
year = {2013}
}
@inproceedings{Byrne2006,
author = {Byrne, Jeffrey and Cosgrove, Martin and Mehra, Raman},
booktitle = {Robotics and Automation, 2006. ICRA 2006. Proceedings 2006 IEEE International Conference on},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Byrne, Cosgrove, Mehra - 2006 - Stereo Based Obstacle Detection for an Unmanned Air Vehicle.pdf:pdf},
pages = {2830--2835},
title = {{Stereo Based Obstacle Detection for an Unmanned Air Vehicle}},
year = {2006}
}
@inproceedings{Alcantarilla2012,
abstract = {In this paper, we introduce the concept of dense scene flow for visual SLAM applications. Traditional visual SLAM methods assume static features in the environment and that a dominant part of the scene changes only due to camera egomotion. These assumptions make traditional visual SLAM methods prone to failure in crowded real-world dynamic environments with many independently moving objects, such as the typical environments for the visually impaired. By means of a dense scene flow representation, moving objects can be detected. In this way, the visual SLAM process can be improved considerably, by not adding erroneous measurements into the estimation, yielding more consistent and improved localization and mapping results. We show large-scale visual SLAM results in challenging indoor and outdoor crowded environments with real visually impaired users. In particular, we performed experiments inside the Atocha railway station and in the city-center of Alcalá de Henares, both in Madrid, Spain. Our results show that the combination of visual SLAM and dense scene flow allows to obtain an accurate localization, improving considerably the results of traditional visual SLAM methods and GPS-based approaches.},
author = {Alcantarilla, P F and Yebes, J J and Almaz{\'{a}}n, J and Bergasa, L M},
booktitle = {2012 IEEE International Conference on Robotics and Automation},
doi = {10.1109/ICRA.2012.6224690},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Alcantarilla et al. - 2012 - On combining visual SLAM and dense scene flow to increase the robustness of localization and mapping in dyn.pdf:pdf},
issn = {1050-4729},
keywords = {Alcalá de Henares,Atocha railway station,Cameras,Feature extraction,GPS-based approach,Global Positioning System,Madrid,Optical imaging,Robustness,Simultaneous localization and mapping,Spain,Vectors,Visualization,camera egomotion,cameras,crowded real-world dynamic environments,dense scene flow representation,feature extraction,handicapped aids,image motion analysis,localization-mapping robustness,moving object detection,object detection,static features,visual SLAM applications,visually impaired users},
pages = {1290--1297},
title = {{On combining visual SLAM and dense scene flow to increase the robustness of localization and mapping in dynamic environments}},
year = {2012}
}
@article{Xu2015,
abstract = {Inspired by recent work in machine translation and object detection, we introduce an attention based model that automatically learns to describe the content of images. We describe how we can train this model in a deterministic manner using standard backpropagation techniques and stochastically by maximizing a variational lower bound. We also show through visualization how the model is able to automatically learn to fix its gaze on salient objects while generating the corresponding words in the output sequence. We validate the use of attention with state-of-the-art performance on three benchmark datasets: Flickr8k, Flickr30k and MS COCO.},
archivePrefix = {arXiv},
arxivId = {1502.03044},
author = {Xu, Kelvin and Ba, Jimmy and Kiros, Ryan and Cho, Kyunghyun and Courville, Aaron and Salakhutdinov, Ruslan and Zemel, Richard and Bengio, Yoshua},
doi = {10.1109/72.279181},
eprint = {1502.03044},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Xu et al. - 2015 - Show, Attend and Tell Neural Image Caption Generation with Visual Attention.pdf:pdf},
isbn = {1045-9227 VO - 5},
issn = {19410093},
pmid = {18267787},
title = {{Show, Attend and Tell: Neural Image Caption Generation with Visual Attention}},
url = {http://arxiv.org/abs/1502.03044},
year = {2015}
}
@article{Eberli2011,
abstract = {This paper presents a real-time vision based algorithm for 5 degrees-of-freedom pose estimation and set-point control for a Micro Aerial Vehicle (MAV). The camera is mounted on-board a quadrotor helicopter. Camera pose estimation is based on the appearance of two concentric circles which are used as landmark. We show that that by using a calibrated camera, conic sections, and the assumption that yaw is controlled independently, it is possible to determine the six degrees-of-freedom pose of the MAV. First we show how to detect the landmark in the image frame. Then we present a geometric approach for camera pose estimation from the elliptic appearance of a circle in perspective projection. Using this information we are able to determine the pose of the vehicle. Finally, given a set point in the image frame we are able to control the quadrotor such that the feature appears in the respective target position. The performance of the proposed method is presented through experimental results.},
author = {Eberli, Daniel and Scaramuzza, Davide and Weiss, Stephan and Siegwart, Roland},
doi = {10.1007/s10846-010-9494-8},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Eberli et al. - 2011 - Vision based position control for MAVs using one single circular landmark.pdf:pdf},
isbn = {0921-0296},
issn = {09210296},
journal = {Journal of Intelligent and Robotic Systems: Theory and Applications},
keywords = {Autonomous flight,Camera calibration,Computer vision,Helicopter,Micro aerial vehicle,Object detection,Pose estimation,Quadrocopter,Structure from motion},
number = {1-4},
pages = {495--512},
title = {{Vision based position control for MAVs using one single circular landmark}},
volume = {61},
year = {2011}
}
@article{Yairi2007,
author = {Yairi, Takehisa},
doi = {10.1145/1273496.1273631},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Yairi - 2007 - Map building without localization by dimensionality reduction techniques.pdf:pdf},
isbn = {9781595937933},
journal = {Proceedings of the 24th international conference on Machine learning - ICML '07},
keywords = {localization-free mapping},
mendeley-tags = {localization-free mapping},
pages = {1071--1078},
title = {{Map building without localization by dimensionality reduction techniques}},
url = {http://portal.acm.org/citation.cfm?doid=1273496.1273631},
year = {2007}
}
@article{Brox2004,
abstract = {We study an energy functional for computing optical flow that combines three assumptions: a brightness constancy assumption, a gradient constancy assumption, and a discontinuity-preserving spatio-temporal smoothness constraint. In order to allow for large displacements, linearisations in the two data terms are strictly avoided. We present a consistent numerical scheme based on two nested fixed point iterations. By proving that this scheme implements a coarse-to-fine warping strategy, we give a theoretical foundation for warping which has been used on a mainly experimental basis so far. Our evaluation demonstrates that the novel method gives significantly smaller angular errors than previous techniques for optical flow estimation. We show that it is fairly insensitive to parameter variations, and we demonstrate its excellent robustness under noise.},
archivePrefix = {arXiv},
arxivId = {cs/0703101v1},
author = {Brox, Thomas and Bruhn, Andr{\'{e}}s and Papenberg, Nils and Weickert, Joachim},
doi = {10.1007/978-3-540-24673-2_3},
eprint = {0703101v1},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Brox et al. - 2004 - High Accuracy Optical Flow Estimation Based on a Theory for Warping.pdf:pdf},
isbn = {9783540219811},
issn = {03029743},
journal = {Computer Vision - ECCV 2004},
pages = {25--36},
pmid = {22203715},
primaryClass = {cs},
title = {{High Accuracy Optical Flow Estimation Based on a Theory for Warping}},
volume = {3024},
year = {2004}
}
@article{Abouzahir1997,
author = {Abouzahir, Mohamed and Elouardi, Abdelhafid and Bouaziz, Samir and Latif, Rachid and Tajer, Abdelouahed},
doi = {10.1109/ICARCV.2014.7064524},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Abouzahir et al. - 1997 - FastSLAM 2.0 running on a low-cost embedded architecture.pdf:pdf},
isbn = {9781479951994},
journal = {2014 13th International Conference on Control Automation Robotics and Vision, ICARCV 2014},
keywords = {Embedded systems,FastSLAM 2.0,Parallel implementation},
number = {December},
pages = {1421--1426},
title = {{FastSLAM 2.0 running on a low-cost embedded architecture}},
volume = {2014},
year = {1997}
}
@inproceedings{Liu2012,
author = {Liu, Yang and Zhang, Hong},
booktitle = {2012 IEEE/RSJ International Conference on Intelligent Robots and Systems},
doi = {10.1109/IROS.2012.6386145},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Liu, Zhang - 2012 - Visual loop closure detection with a compact image descriptor.pdf:pdf},
isbn = {978-1-4673-1736-8},
pages = {1051--1056},
publisher = {IEEE},
title = {{Visual loop closure detection with a compact image descriptor}},
url = {http://ieeexplore.ieee.org/document/6386145/},
year = {2012}
}
@article{Fraundorfer2012,
abstract = {In this paper, we describe our autonomous vision-based quadrotor MAV system which maps and explores unknown environments. All algorithms necessary for autonomous mapping and exploration run on-board the MAV. Using a front-looking stereo camera as the main exteroceptive sensor, our quadrotor achieves these capabilities with both the Vector Field Histogram+ (VFH+) algorithm for local navigation, and the frontier-based exploration algorithm. In addition, we implement the Bug algorithm for autonomous wall-following which could optionally be selected as the substitute exploration algorithm in sparse environments where the frontier-based exploration under-performs. We incrementally build a 3D global occupancy map on-board the MAV. The map is used by the VFH+ and frontier-based exploration in dense environments, and the Bug algorithm for wall-following in sparse environments. During the exploration phase, images from the front-looking camera are transmitted over Wi-Fi to the ground station. These images are input to a large-scale visual SLAM process running off-board on the ground station. SLAM is carried out with pose-graph optimization and loop closure detection using a vocabulary tree. We improve the robustness of the pose estimation by fusing optical flow and visual odometry. Optical flow data is provided by a customized downward-looking camera integrated with a microcontroller while visual odometry measurements are derived from the front-looking stereo camera. We verify our approaches with experimental results.},
author = {Fraundorfer, Friedrich and Heng, Lionel and Honegger, Dominik and Lee, Gim Hee and Meier, Lorenz and Tanskanen, Petri and Pollefeys, Marc},
doi = {10.1109/IROS.2012.6385934},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Fraundorfer et al. - 2012 - Vision-based autonomous mapping and exploration using a quadrotor MAV.pdf:pdf},
isbn = {9781467317375},
issn = {21530858},
journal = {IEEE International Conference on Intelligent Robots and Systems},
keywords = {exploration},
mendeley-tags = {exploration},
pages = {4557--4564},
title = {{Vision-based autonomous mapping and exploration using a quadrotor MAV}},
year = {2012}
}
@article{Troiani2013,
abstract = {Abstract—We propose a novel method to estimate the relative motion between two consecutive camera views, which only requires the observation of a single feature in the scene and the knowledge of the angular rates from an inertial measurement unit, under ... },
author = {Troiani, Chiara and Martinelli, Agostino and Laugier, Christian and Scaramuzza, Davide},
doi = {10.1109/ECMR.2013.6698813},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Troiani et al. - 2013 - 1-Point-based monocular motion estimation for computationally-limited micro aerial vehicles.pdf:pdf},
isbn = {9781479902637},
journal = {2013 European Conference on Mobile Robots, ECMR 2013 - Conference Proceedings},
keywords = {experiment,low complexity,monocular,nieuwe referenties,simulation,state estimation},
mendeley-tags = {experiment,low complexity,monocular,nieuwe referenties,simulation,state estimation},
pages = {13--18},
title = {{1-Point-based monocular motion estimation for computationally-limited micro aerial vehicles}},
year = {2013}
}
@article{GrassiJunior2006,
abstract = {Omnidirectional vision systems can provide images with a 360° of field of view. For this reason, they can be useful to robotic applications such as navigation, teleoperation and visual servoing. An effective way to construct this type of vision system is by combining lenses and mirrors together resulting on a system that does not require the movement of the camera to the direction of attention of the robot. A typical construction is by mounting a convex mirror in front of a camera aligning the center of the mirror with the optical axis of the camera. The most common convex mirror shapes used are conic, parabolic, hyperbolic and spherical. In this work we present two types of mirror that were constructed: a spherical mirror, used in the initial tests, and a hyperbolic, used for actual robot tasks. The hyperbolic mirror was manufactured using an ultra-precision CNC machine. Additionally, a software was developed to create panoramic and perspective images from the image acquired by the system. This work shows the development of an omnidirectional vision system, presenting the formulation used to determine a suitable mirror shape, the mechanical solutions used to build a fully operational system, and the results of the developed algorithm. Copyright {\textcopyright} 2006 by ABCM.},
author = {{Grassi Junior}, Valdir and {Okamoto Junior}, Jun},
doi = {10.1590/S1678-58782006000100007},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Grassi Junior, Okamoto Junior - 2006 - Development of an omnidirectional vision system.pdf:pdf},
issn = {1678-5878},
journal = {Journal of the Brazilian Society of Mechanical Sciences and Engineering},
keywords = {hyperbolic mirror,image processing,mobile robot,vision system},
number = {1},
pages = {58--68},
title = {{Development of an omnidirectional vision system}},
volume = {28},
year = {2006}
}
@article{Lowe2004,
abstract = {This paper presents a method for extracting distinctive invariant features from images that can be used to perform reliable matching between different views of an object or scene. The features are invariant to image scale and rotation, and are shown to provide robust matching across a substantial range of affine distortion, change in 3D viewpoint, addition of noise, and change in illumination. The features are highly distinctive, in the sense that a single feature can be correctly matched with high probability against a large database of features from many images. This paper also describes an approach to using these features for object recognition. The recognition proceeds by matching individual features to a database of features from known objects using a fast nearest-neighbor algorithm, followed by a Hough transform to identify clusters belonging to a single object, and finally performing verification through least-squares solution for consistent pose parameters. This approach to recognition can robustly identify objects among clutter and occlusion while achieving near real-time performance.},
author = {Lowe, David G},
doi = {10.1023/B:VISI.0000029664.99615.94},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Lowe - 2004 - Distinctive Image Features from Scale-Invariant Keypoints.pdf:pdf},
isbn = {1568811012},
issn = {0920-5691},
journal = {International Journal of Computer Vision},
keywords = {Image matching,Invariant features,Object recognition,SIFT,Scale invariance},
mendeley-tags = {SIFT},
month = {nov},
number = {2},
pages = {91--110},
pmid = {20064111},
title = {{Distinctive Image Features from Scale-Invariant Keypoints}},
url = {http://link.springer.com/10.1023/B:VISI.0000029664.99615.94},
volume = {60},
year = {2004}
}
@article{Barron1994,
author = {Barron, J L and Fleet, D J and Beauchemin, S S},
doi = {10.1007/BF01420984},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Barron, Fleet, Beauchemin - 1994 - Performance of optical flow techniques.pdf:pdf},
issn = {0920-5691},
journal = {International Journal of Computer Vision},
month = {feb},
number = {1},
pages = {43--77},
title = {{Performance of optical flow techniques}},
url = {http://link.springer.com/10.1007/BF01420984},
volume = {12},
year = {1994}
}
@article{Chaumette2006,
abstract = {This paper is the first of a two-part series on the topic of visual servo control using computer vision data in the servo loop to control the motion of a robot. In this paper, we describe the basic techniques that are by now well established in the field. We first give a general overview of the formulation of the visual servo control problem. We then describe the two archetypal visual servo control schemes: image-based and position-based visual servo control. Finally, we discuss performance and stability issues that pertain to these two schemes, motivating the second article in the series, in which we consider advanced techniques},
author = {Chaumette, Fraņois and Hutchinson, Setrh},
doi = {10.1109/MRA.2006.250573},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Chaumette, Hutchinson - 2006 - Visual servo control. I. Basic approaches.pdf:pdf},
isbn = {1070-9932},
issn = {10709932},
journal = {IEEE Robotics and Automation Magazine},
keywords = {visual homing,visual servoing},
mendeley-tags = {visual homing,visual servoing},
number = {4},
pages = {82--90},
title = {{Visual servo control. I. Basic approaches}},
volume = {13},
year = {2006}
}
@article{Nieuwenhuisen2013,
abstract = {Reliably perceiving obstacles and avoiding collisions is key for the fully autonomous application of micro aerial vehicles (MAVs), Limiting factors for increasing autonomy and complexity of MAVs are limited onboard sensing and limited onboard processing power. In this paper, we propose a complete system with a multimodal sensor setup for omnidirectional obstacle perception. We developed a lightweight 3D laser scanner and visual obstacle detection using wide-angle stereo cameras. Detected obstacles are aggregated in egocentric grid maps. We implemented a fast reactive collision avoidance approach for safe operation in the vicinity of structures like buildings or vegetation.},
author = {Nieuwenhuisen, M and Droeschel, D and Schneider, J and Holz, D and Labe, T and Behnke, S},
doi = {10.1109/ECMR.2013.6698812},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Nieuwenhuisen et al. - 2013 - Multimodal obstacle detection and collision avoidance for micro aerial vehicles.pdf:pdf},
isbn = {VO -},
journal = {Mobile Robots (ECMR), 2013 European Conference on},
keywords = {AIS papers,Cameras,Collision avoidance,MAV,Measurement by laser beam,Oct 22 import,Robot sensing systems,Three-dimensional displays,Ultrasonic variables measurement,autonomous aerial vehicles,cameras,collision avoidance,egocentric grid maps,image matching,lightweight 3D laser scanner,microaerial vehicles,microrobots,multimodal obstacle detection,multimodal sensor setup,omnidirectional obstacle perception,onboard processing power,onboard sensing capability,optical scanners,reactive collision avoidance approach,robot vision,stereo image processing,visual obstacle detection,wide-angle stereo cameras},
pages = {7--12},
title = {{Multimodal obstacle detection and collision avoidance for micro aerial vehicles}},
url = {http://dx.doi.org/10.1109/ECMR.2013.6698812{\%}5Cnhttp://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6698812{\%}5Cnhttp://xplorestaging.ieee.org/iel7/6690584/6698803/06698812.pdf?arnumber=6698812{\%}5Cnhttp://dx.doi.org/10.1109/ecmr.2013.6698812{\%}5CnAll},
year = {2013}
}
@article{Krose2001,
author = {Kr{\"{o}}se, B.J.A and Vlassis, N and Bunschoten, R and Motomura, Y},
doi = {10.1016/S0262-8856(00)00086-X},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Kr{\"{o}}se et al. - 2001 - A probabilistic model for appearance-based robot localization.pdf:pdf},
issn = {02628856},
journal = {Image and Vision Computing},
keywords = {PCA,feature extraction,probabilistic modeling,robot localization},
mendeley-tags = {PCA},
month = {apr},
number = {6},
pages = {381--391},
title = {{A probabilistic model for appearance-based robot localization}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S026288560000086X},
volume = {19},
year = {2001}
}
@inproceedings{Maddern2012,
author = {Maddern, Will and Milford, Michael and Wyeth, Gordon},
booktitle = {2012 IEEE/RSJ International Conference on Intelligent Robots and Systems},
doi = {10.1109/IROS.2012.6386186},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Maddern, Milford, Wyeth - 2012 - Towards persistent indoor appearance-based localization, mapping and navigation using CAT-Graph.pdf:pdf},
isbn = {978-1-4673-1736-8},
keywords = {SLAM,appearancebased navigation,highlight,topological slam},
mendeley-tags = {SLAM,appearancebased navigation,highlight,topological slam},
month = {oct},
pages = {4224--4230},
publisher = {IEEE},
title = {{Towards persistent indoor appearance-based localization, mapping and navigation using CAT-Graph}},
url = {http://ieeexplore.ieee.org/document/6386186/},
year = {2012}
}
@article{Saxena2006,
abstract = {We consider the task of depth estimation from a single monocular image. We take a supervised learning approach to this problem, in which we begin by collecting a training set of monocular images (of unstructured outdoor environments which include forests, trees, buildings, etc.) and their corresponding ground-truth depthmaps. Then, we apply supervised learning to predict the depthmap as a function of the image. Depth estimation is a challenging problem, since local features alone are insufficient to estimate depth at a point, and one needs to consider the global context of the image. Our model uses a discriminatively-trained Markov Random Field (MRF) that incorporates multiscale local- and global-image features, and models both depths at individual points as well as the relation between depths at different points. We show that, even on unstructured scenes, our algorithm is frequently able to recover fairly accurate depthmaps.},
author = {Saxena, Ashutosh and Chung, Sung H and Ng, Andrew Y},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Saxena, Chung, Ng - 2006 - Learning Depth from Single Monocular Images.pdf:pdf},
journal = {Advances in Neural Information Processing Systems},
keywords = {monocular,obstacle detection},
mendeley-tags = {monocular,obstacle detection},
pages = {1161--1168},
title = {{Learning Depth from Single Monocular Images}},
year = {2006}
}
@inproceedings{Leutenegger2011,
author = {Leutenegger, Stefan and Chli, Margarita and Siegwart, Roland Y.},
booktitle = {2011 International Conference on Computer Vision},
doi = {10.1109/ICCV.2011.6126542},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Leutenegger, Chli, Siegwart - 2011 - BRISK Binary Robust invariant scalable keypoints.pdf:pdf},
isbn = {978-1-4577-1102-2},
month = {nov},
pages = {2548--2555},
publisher = {IEEE},
title = {{BRISK: Binary Robust invariant scalable keypoints}},
url = {http://ieeexplore.ieee.org/document/6126542/},
year = {2011}
}
@article{Vecera2004,
abstract = {Figure-ground assignment involves determining which visual regions are foreground figures and which are backgrounds. Although figure-ground processes provide important inputs to high-level vision, little is known about the reference frame in which the figure's features and parts are defined. Computational approaches have suggested a retinally based, viewer-centered reference frame for figure-ground assignment, but figural assignment could also be computed on the basis of environmental regularities in an environmental reference frame. The present research used a newly discovered cue, lower region, to examine the reference frame of figure-ground assignment. Possible reference frames were misaligned by changing the orientation of viewers by having them tilt their heads (Experiments 1 and 2) or turn them upside down (Experiment 3). The results of these experiments indicated that figure-ground perception followed the orientation of the viewer, suggesting a viewer-centered reference frame for figure-ground assignment.},
author = {Vecera, Shaun P.},
doi = {10.3758/BF03196720},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Vecera - 2004 - The reference frame of figure-ground assignment.pdf:pdf},
isbn = {1069-9384},
issn = {10699384},
journal = {Psychonomic Bulletin and Review},
number = {5},
pages = {909--915},
pmid = {15732702},
title = {{The reference frame of figure-ground assignment}},
volume = {11},
year = {2004}
}
@incollection{Dey2016a,
archivePrefix = {arXiv},
arxivId = {1411.6326v1},
author = {Dey, Debadeepta and Shankar, Kumar Shaurya and Zeng, Sam and Mehta, Rupesh and Agcayazi, M Talha and Eriksen, Christopher and Daftry, Shreyansh and Hebert, Martial and Bagnell, J Andrew},
booktitle = {Springer Tracts in Advanced Robotics},
editor = {Wettergreen, D. and Barfoot, T.},
eprint = {1411.6326v1},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Dey et al. - 2016 - Vision and Learning for Deliberative Monocular Cluttered Flight.pdf:pdf},
keywords = {on-board},
mendeley-tags = {on-board},
number = {Field and Service Robotics},
pages = {391--409},
publisher = {Springer},
title = {{Vision and Learning for Deliberative Monocular Cluttered Flight}},
volume = {113},
year = {2016}
}
@article{Lee2013,
abstract = {A camera poses a highly attractive choice as a sensor in implementing simultaneous localization and mapping (SLAM) for low-cost consumer robots such as home cleaning robots. This is due to its low cost, light weight, and low power consumption. However, most of the visual SLAMs available to date are not designed and, consequently, not suitable for use in a low-cost embedded SLAM for consumer robots. This article presents a computationally light yet performance-wise robust SLAM algorithm and its implementation as an embedded system for low-cost consumer robots using an upward-looking camera. Especially for a large-scale mapping of indoor environments, methods of pose graph optimization as well as submapping are employed. An occupancy grid map is used to integrate an efficient Kalman filter-based localization into a SLAM framework. Furthermore, an algorithmic visual compass is introduced as a means of reducing the computational complexity involved in pose graph optimization, taking advantage of the distinct geometric features of the scenes captured by an upward-looking camera. The proposed visual SLAM is implemented in a real home cleaning robot as an embedded system using an ARM11 processor. Extensive test results demonstrate the power of the proposed embedded visual SLAM in terms of not only its computational efficiency but also its performance robustness in realworld applications.},
author = {Lee, Seongsoo and Lee, Sukhan},
doi = {10.1109/MRA.2013.2283642},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Lee, Lee - 2013 - Embedded visual SLAM Applications for low-cost consumer robots.pdf:pdf},
isbn = {1070-9932 VO  - 20},
issn = {10709932},
journal = {IEEE Robotics and Automation Magazine},
number = {4},
pages = {83--95},
title = {{Embedded visual SLAM: Applications for low-cost consumer robots}},
volume = {20},
year = {2013}
}
@article{Timney1996,
author = {Timney, Brian and Keil, Kathy},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Timney, Keil - 1996 - Horses are sensitive to pictorial depth cues.pdf:pdf},
journal = {Perception},
pages = {1121--1128},
title = {{Horses are sensitive to pictorial depth cues}},
volume = {25},
year = {1996}
}
@article{Zhu2017b,
abstract = {Model pruning seeks to induce sparsity in a deep neural network's various connection matrices, thereby reducing the number of nonzero-valued parameters in the model. Recent reports (Han et al., 2015; Narang et al., 2017) prune deep networks at the cost of only a marginal loss in accuracy and achieve a sizable reduction in model size. This hints at the possibility that the baseline models in these experiments are perhaps severely over-parameterized at the outset and a viable alternative for model compression might be to simply reduce the number of hidden units while maintaining the model's dense connection structure, exposing a similar trade-off in model size and accuracy. We investigate these two distinct paths for model compression within the context of energy-efficient inference in resource-constrained environments and propose a new gradual pruning technique that is simple and straightforward to apply across a variety of models/datasets with minimal tuning and can be seamlessly incorporated within the training process. We compare the accuracy of large, but pruned models (large-sparse) and their smaller, but dense (small-dense) counterparts with identical memory footprint. Across a broad range of neural network architectures (deep CNNs, stacked LSTM, and seq2seq LSTM models), we find large-sparse models to consistently outperform small-dense models and achieve up to 10x reduction in number of non-zero parameters with minimal loss in accuracy.},
archivePrefix = {arXiv},
arxivId = {1710.01878},
author = {Zhu, Michael and Gupta, Suyog},
eprint = {1710.01878},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Zhu, Gupta - 2017 - To prune, or not to prune exploring the efficacy of pruning for model compression.pdf:pdf},
title = {{To prune, or not to prune: exploring the efficacy of pruning for model compression}},
url = {http://arxiv.org/abs/1710.01878},
year = {2017}
}
@inproceedings{Call2006,
author = {Call, Brandon and Beard, Randy and Taylor, Clark and Barber, Blake},
booktitle = {AIAA Guidance, Navigation, and Control Conference and Exhibit},
doi = {10.2514/6.2006-6541},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Call et al. - 2006 - Obstacle Avoidance For Unmanned Air Vehicles Using Image Feature Tracking.pdf:pdf},
number = {August},
pages = {3406--3414},
title = {{Obstacle Avoidance For Unmanned Air Vehicles Using Image Feature Tracking}},
year = {2006}
}
@article{Geirhos2018,
abstract = {Convolutional Neural Networks (CNNs) are commonly thought to recognise objects by learning increasingly complex representations of object shapes. Some recent studies suggest a more important role of image textures. We here put these conflicting hypotheses to a quantitative test by evaluating CNNs and human observers on images with a texture-shape cue conflict. We show that ImageNet-trained CNNs are strongly biased towards recognising textures rather than shapes, which is in stark contrast to human behavioural evidence and reveals fundamentally different classification strategies. We then demonstrate that the same standard architecture (ResNet-50) that learns a texture-based representation on ImageNet is able to learn a shape-based representation instead when trained on "Stylized-ImageNet", a stylized version of ImageNet. This provides a much better fit for human behavioural performance in our well-controlled psychophysical lab setting (nine experiments totalling 48,560 psychophysical trials across 97 observers) and comes with a number of unexpected emergent benefits such as improved object detection performance and previously unseen robustness towards a wide range of image distortions, highlighting advantages of a shape-based representation.},
archivePrefix = {arXiv},
arxivId = {1811.12231},
author = {Geirhos, Robert and Rubisch, Patricia and Michaelis, Claudio and Bethge, Matthias and Wichmann, Felix A. and Brendel, Wieland},
doi = {arXiv:1811.12231v1},
eprint = {1811.12231},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Geirhos et al. - 2018 - ImageNet-trained CNNs are biased towards texture increasing shape bias improves accuracy and robustness.pdf:pdf},
journal = {arXiv preprint},
month = {nov},
number = {2015},
pages = {1--20},
title = {{ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness}},
url = {http://arxiv.org/abs/1811.12231},
year = {2018}
}
@article{Franz2000,
abstract = {In the past decade, a large number of robots has been built that explicitly implement biological navigation behaviours. We review these biomimetic approaches using a framework that allows for a common description of biological and technical navigation behaviour. The review shows that biomimetic systems make significant contributions to two fields of research: First, they provide a real world test of models of biological navigation behaviour; second, they make new navigation mechanisms available for technical applications, most notably in the field of indoor robot navigation. While simpler insect navigation behaviours have been implemented quite successfully, the more complicated way-finding capabilities of vertebrates still pose a challenge to current systems.},
author = {Franz, Matthias O. and Mallot, Hanspeter A.},
doi = {10.1016/S0921-8890(99)00069-X},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Franz, Mallot - 2000 - Biomimetic robot navigation.pdf:pdf},
isbn = {4973150541},
issn = {09218890},
journal = {Robotics and Autonomous Systems},
keywords = {cognitive map,landmark,robot navigation,spatial behaviour,topological map},
number = {1},
pages = {133--153},
title = {{Biomimetic robot navigation}},
volume = {30},
year = {2000}
}
@article{Olah2017a,
author = {Olah, Chris and Mordvintsev, Alexander and Schubert, Ludwig},
doi = {10.23915/distill.00007},
journal = {Distill},
title = {{Feature Visualization}},
url = {https://distill.pub/2017/feature-visualization},
year = {2017}
}
@article{DeCroon2012,
abstract = {A visual cue is introduced that exploits the visual appearance of a single image to estimate the proximity to an obstacle. In particular, the appearance variation cue captures the variation in texture and / or color in the image, and is based on the assumption that there is less such variation when the camera is close to an obstacle. Random sampling is applied in order to evaluate the appearance variation fast enough for use in robotics. It is demonstrated that the randomly sampled appearance variation cue can be complementary to optic flow for obstacle detection; combining the two visual cues leads to better obstacle detection performance. Random sampling leads to sufficient computational efficiency for the cue's utilization in autonomous flight: a speed-up of a factor {\&}{\#}x223C;100 is attained, which allows the successful control of the 16-gram flapping wing MAV DelFly II.},
author = {{De Croon}, G. C H E and {De Weerdt}, E. and {De Wagter}, C. and Remes, B. D W and Ruijsink, R.},
doi = {10.1109/TRO.2011.2170754},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/De Croon et al. - 2012 - The appearance variation cue for obstacle avoidance.pdf:pdf},
isbn = {9781424493173},
issn = {15523098},
journal = {IEEE Transactions on Robotics},
keywords = {Micro air vehicle (MAV),obstacle avoidance},
number = {2},
pages = {529--534},
title = {{The appearance variation cue for obstacle avoidance}},
volume = {28},
year = {2012}
}
@article{Yang2000a,
abstract = {An efficient neural network method is proposed for real-time motion planning of a mobile robot or a multi-joint robot manipulator with safety consideration in a nonstationary environment. The optimal robot motion is planned through the dynamic neural activity landscape of the biologically inspired neural network without any prior knowledge of the dynamic environment and without any learning procedures. The planned real-time “comfortable” path does not suffer from the “too close” or the “too far” problems. The proposed model is stable, computationally efficient, and not very sensitive to parameter variations. The effectiveness and the efficiency are demonstrated through simulation and comparison studies. },
annote = {Vervolg op hun vorige paper, nu met veiligheidsmarge rondom states},
author = {Yang, Simon X and Meng, Max},
doi = {http://dx.doi.org/10.1016/S0921-8890(99)00113-X},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Yang, Meng - 2000 - An efficient neural network method for real-time motion planning with safety consideration.pdf:pdf},
issn = {0921-8890},
journal = {Robotics and Autonomous Systems},
keywords = {Efficient algorithms,Global stability,Neural networks,Obstacle clearance,Real-time motion planning,cartesian map,deliberate obstacle avoidance,motion planning,moving obstacles,obstacle avoidance,simulation,voxel map},
mendeley-tags = {cartesian map,deliberate obstacle avoidance,motion planning,moving obstacles,obstacle avoidance,simulation,voxel map},
number = {2–3},
pages = {115--128},
title = {{An efficient neural network method for real-time motion planning with safety consideration}},
url = {http://www.sciencedirect.com/science/article/pii/S092188909900113X},
volume = {32},
year = {2000}
}
@article{Khamis2018,
abstract = {This paper presents StereoNet, the first end-to-end deep architecture for real-time stereo matching that runs at 60 fps on an NVidia Titan X, producing high-quality, edge-preserved, quantization-free disparity maps. A key insight of this paper is that the network achieves a sub-pixel matching precision than is a magnitude higher than those of traditional stereo matching approaches. This allows us to achieve real-time performance by using a very low resolution cost volume that encodes all the information needed to achieve high disparity precision. Spatial precision is achieved by employing a learned edge-aware upsampling function. Our model uses a Siamese network to extract features from the left and right image. A first estimate of the disparity is computed in a very low resolution cost volume, then hierarchically the model re-introduces high-frequency details through a learned upsampling function that uses compact pixel-to-pixel refinement networks. Leveraging color input as a guide, this function is capable of producing high-quality edge-aware output. We achieve compelling results on multiple benchmarks, showing how the proposed method offers extreme flexibility at an acceptable computational budget.},
archivePrefix = {arXiv},
arxivId = {1807.08865},
author = {Khamis, Sameh and Fanello, Sean and Rhemann, Christoph and Kowdle, Adarsh and Valentin, Julien and Izadi, Shahram},
doi = {1807.08865},
eprint = {1807.08865},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Khamis et al. - 2018 - StereoNet Guided Hierarchical Refinement for Real-Time Edge-Aware Depth Prediction.pdf:pdf},
journal = {arXiv preprint arXiv:1807.08865},
keywords = {cost volume filtering,deep learning,depth estimation,edge-aware refinement,stereo matching},
title = {{StereoNet: Guided Hierarchical Refinement for Real-Time Edge-Aware Depth Prediction}},
url = {http://arxiv.org/abs/1807.08865},
year = {2018}
}
@article{Zhong2017,
abstract = {Exiting deep-learning based dense stereo matching methods often rely on ground-truth disparity maps as the training signals, which are however not always available in many situations. In this paper, we design a simple convolutional neural network architecture that is able to learn to compute dense disparity maps directly from the stereo inputs. Training is performed in an end-to-end fashion without the need of ground-truth disparity maps. The idea is to use image warping error (instead of disparity-map residuals) as the loss function to drive the learning process, aiming to find a depth-map that minimizes the warping error. While this is a simple concept well-known in stereo matching, to make it work in a deep-learning framework, many non-trivial challenges must be overcome, and in this work we provide effective solutions. Our network is self-adaptive to different unseen imageries as well as to different camera settings. Experiments on KITTI and Middlebury stereo benchmark datasets show that our method outperforms many state-of-the-art stereo matching methods with a margin, and at the same time significantly faster.},
archivePrefix = {arXiv},
arxivId = {1709.00930},
author = {Zhong, Yiran and Dai, Yuchao and Li, Hongdong},
eprint = {1709.00930},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Zhong, Dai, Li - 2017 - Self-Supervised Learning for Stereo Matching with Self-Improving Ability.pdf:pdf},
journal = {arXiv preprint arXiv:1709.00930},
month = {sep},
title = {{Self-Supervised Learning for Stereo Matching with Self-Improving Ability}},
year = {2017}
}
@article{Leishman2014b,
abstract = {GPS-denied aerial flight is a challenging research problem and requires knowledge of complex elements from several distinct disciplines. Additionally, aerial vehicles can present challenging constraints such as stringent payload limits and fast vehicle dynamics. In this paper we propose a new architecture to simplify some of the challenges that constrain GPS-denied aerial flight. At the core, the approach combines visual graph-SLAM with a multiplicative extended Kalman filter. More importantly, for the front end we depart from the common practice of estimating global states and instead keep the position and yaw states of the MEKF relative to the current node in the map. This relative navigation approach provides simple application of sensor measurement updates, intuitive definition of map edges and covariances, and the flexibility of using a globally consistent map when desired. We verify the approach with hardware flight-test results.},
author = {Leishman, Robert C. and McLain, Timothy W. and Beard, Randal W.},
doi = {10.1007/s10846-013-9914-7},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Leishman, McLain, Beard - 2014 - Relative navigation approach for vision-based aerial GPS-denied navigation.pdf:pdf},
isbn = {9781479908172},
issn = {09210296},
journal = {Journal of Intelligent and Robotic Systems: Theory and Applications},
keywords = {GPS-denied navigation,Quadrotor,Relative navigation,Vision-based autonomy},
number = {1-2},
pages = {97--111},
title = {{Relative navigation approach for vision-based aerial GPS-denied navigation}},
volume = {74},
year = {2014}
}
@incollection{Wedel2009a,
author = {Wedel, Andreas and Pock, Thomas and Zach, Christopher and Bischof, Horst and Cremers, Daniel},
booktitle = {Statistical and Geometrical Approaches to Visual Motion Analysis. Lecture Notes in Computer Science, vol 5604},
doi = {10.1007/978-3-642-03061-1_2},
editor = {Cremers, D. and Rosenhahn, B. and Yuille, A.L. and Schmidt, F.R.},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Wedel et al. - 2009 - An Improved Algorithm for TV-L 1 Optical Flow.pdf:pdf},
pages = {23--45},
publisher = {Springer, Berlin, Heidelberg},
title = {{An Improved Algorithm for TV-}},
volume = {1},
year = {2009}
}
@article{Ma2017,
abstract = {We consider the problem of dense depth prediction from a sparse set of depth measurements and a single RGB image. Since depth estimation from monocular images alone is inherently ambiguous and unreliable, we introduce additional sparse depth samples, which are either collected from a low-resolution depth sensor or computed from SLAM, to attain a higher level of robustness and accuracy. We propose the use of a single regression network to learn directly from the RGB-D raw data, and explore the impact of number of depth samples on prediction accuracy. Our experiments show that, as compared to using only RGB images, the addition of 100 spatially random depth samples reduces the prediction root-mean-square error by half in the NYU-Depth-v2 indoor dataset. It also boosts the percentage of reliable prediction from 59{\%} to 92{\%} on the more challenging KITTI driving dataset. We demonstrate two applications of the proposed algorithm: serving as a plug-in module in SLAM to convert sparse maps to dense maps, and creating much denser point clouds from low-resolution LiDARs. Codes and video demonstration are publicly available.},
archivePrefix = {arXiv},
arxivId = {1709.07492},
author = {Ma, Fangchang and Karaman, Sertac},
eprint = {1709.07492},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Ma, Karaman - 2017 - Sparse-to-Dense Depth Prediction from Sparse Depth Samples and a Single Image.pdf:pdf},
journal = {arXiv preprint arXiv:1709.07492},
title = {{Sparse-to-Dense: Depth Prediction from Sparse Depth Samples and a Single Image}},
year = {2017}
}
@article{Bhoi2019,
abstract = {Monocular depth estimation is often described as an ill-posed and inherently ambiguous problem. Estimating depth from 2D images is a crucial step in scene reconstruction, 3Dobject recognition, segmentation, and detection. The problem can be framed as: given a single RGB image as input, predict a dense depth map for each pixel. This problem is worsened by the fact that most scenes have large texture and structural variations, object occlusions, and rich geometric detailing. All these factors contribute to difficulty in accurate depth estimation. In this paper, we review five papers that attempt to solve the depth estimation problem with various techniques including supervised, weakly-supervised, and unsupervised learning techniques. We then compare these papers and understand the improvements made over one another. Finally, we explore potential improvements that can aid to better solve this problem.},
archivePrefix = {arXiv},
arxivId = {1901.09402},
author = {Bhoi, Amlaan},
eprint = {1901.09402},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Bhoi - 2019 - Monocular Depth Estimation A Survey.pdf:pdf},
title = {{Monocular Depth Estimation: A Survey}},
url = {http://arxiv.org/abs/1901.09402},
year = {2019}
}
@inproceedings{Augustine2012,
abstract = {—Metric maps provide a reliable basis for mobile robot navigation. However, such maps are in general quite resource expensive and do not scale very well. Aiming for a highly scalable map, we adopt theories of insect navigation to develop an algorithm which builds a topological map for global navigation. Similar to insect conduct, positions in space are memorized as snapshots, which are unique configurations of landmarks. Unlike conventional snapshot approaches, we do not simply store the landmarks as a set, but we build a landmark tree which enables us to easily free memory in case of a continuously growing map while still preserving the dominant information. The resulting navigation is not sensor specific and solely relies on the directions of arbitrary landmarks. The generated map enables a mobile robot to navigate between defined locations and let it retrace a previously pursued path. Finally, we verify the reliability of the Landmark-Tree Map (LT-Map) concept and its robustness on memory limitations.},
annote = {volgens de paper topological slam, maar zou ook onder appearance-based navigation kunnen vallen. Maar heeft wel loop closure toch? Verder uitzoeken...},
author = {Augustine, Marcus and Ortmeier, Frank and Mair, Elmar and Burschka, Darius and Stelzer, Annett and Suppa, Michael},
booktitle = {2012 IEEE International Conference on Robotics and Biomimetics (ROBIO)},
doi = {10.1109/ROBIO.2012.6490955},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Augustine et al. - 2012 - Landmark-Tree map A biologically inspired topological map for long-distance robot navigation.pdf:pdf},
isbn = {978-1-4673-2127-3},
keywords = {biology,highlight,low complexity,slam,todo},
mendeley-tags = {biology,highlight,low complexity,slam,todo},
month = {dec},
pages = {128--135},
publisher = {IEEE},
title = {{Landmark-Tree map: A biologically inspired topological map for long-distance robot navigation}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6490955},
year = {2012}
}
@article{Baddeley2011,
abstract = {It is known that ants learn long visually guided routes through complex terrain. However, the mechanisms by which visual information is first learned and then used to control a route direction are not well understood. In this article, we propose a parsimonious mechanism for visually guided route following. We investigate whether a simple approach, involving scanning the environment and moving in the direction that appears most familiar, can provide a model of visually guided route learning in ants. We implement view familiarity as a means of navigation by training a classifier to determine whether a given view is part of a route and using the confidence in this classification as a proxy for familiarity. Through the coupling of movement and viewing direction, a familiar view specifies a familiar direction of viewing and thus a familiar movement to make. We show the feasibility of our approach as a model of ant-like route acquisition by learning a series of nontrivial routes through an indoor environment using a large gantry robot equipped with a panoramic camera.},
author = {Baddeley, B. and Graham, P. and Philippides, A. and Husbands, P.},
doi = {10.1177/1059712310395410},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Baddeley et al. - 2011 - Holistic visual encoding of ant-like routes Navigation without waypoints.pdf:pdf},
isbn = {1059712310395},
issn = {1059-7123},
journal = {Adaptive Behavior},
keywords = {autonomous robotics,image classification,insect navigation,route learning,view-based homing},
number = {1},
pages = {3--15},
title = {{Holistic visual encoding of ant-like routes: Navigation without waypoints}},
url = {http://adb.sagepub.com/cgi/doi/10.1177/1059712310395410},
volume = {19},
year = {2011}
}
@techreport{Woodman2007,
abstract = {ntil recently the weight and size of inertial sensors has prohibited their use in domains such as human motion capture. Recent improvements in the performance of small and lightweight micro- machined electromechanical systems (MEMS) inertial sensors have made the application of inertial techniques to such problems possible. This has resulted in an increased interest in the topic of inertial navigation, however current introductions to the subject fail to sufficiently describe the error characteristics of inertial systems. We introduce inertial navigation, focusing on strapdown systems based onMEMS devices. A com- bination of measurement and simulation is used to explore the error characteristics of such systems. For a simple inertial navigation system (INS) based on the Xsens Mtx inertial measurement unit (IMU), we show that the average error in position grows to over 150 m after 60 seconds of operation. The propagation of orientation errors caused by noise perturbing gyroscope signals is identified as the critical cause of such drift. By simulation we examine the significance of individual noise processes perturbing the gyroscope signals, identifying white noise as the process which contributes most to the overall drift of the system. Sensor fusion and domain specific constraints can be used to reduce drift in INSs. For an example INS we show that sensor fusion usingmagnetometers can reduce the average error in position obtained by the system after 60 seconds from over 150 m to around 5 m. We conclude that whilst MEMS IMU technology is rapidly improving, it is not yet possible to build a MEMS based INS which gives sub-meter position accuracy for more than one minute of operation.},
archivePrefix = {arXiv},
arxivId = {ISSN 1476-2986},
author = {Woodman, Oliver J.},
booktitle = {University of Cambridge},
doi = {10.1017/S0373463300036341},
eprint = {ISSN 1476-2986},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Woodman - 2007 - An Introduction to Inertial Navigation.pdf:pdf},
isbn = {UCAM-CL-TR-696},
issn = {1678-4227},
pmid = {19838543},
title = {{An Introduction to Inertial Navigation}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/19863683},
year = {2007}
}
@article{Lee2012,
abstract = {In this paper we describe a vision-based algorithm to control a vertical-takeoff-and-landing unmanned aerial vehicle while tracking and landing on a moving platform. Specifically, we use image-based visual servoing (IBVS) to track the platform in two-dimensional image space and generate a velocity reference command used as the input to an adaptive sliding mode controller. Compared with other vision-based control algorithms that reconstruct a full three-dimensional representation of the target, which requires precise depth estimation, IBVS is computationally cheaper since it is less sensitive to the depth estimation allowing for a faster method to obtain this estimate. To enhance velocity tracking of the sliding mode controller, an adaptive rule is described to account for the ground effect experienced during the maneuver. Finally, the IBVS algorithm integrated with the adaptive sliding mode controller for tracking and landing is validated in an experimental setup using a quadrotor.},
author = {Lee, Daewon and Ryan, Tyler and Kim, H. Jin},
doi = {10.1109/ICRA.2012.6224828},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Lee, Ryan, Kim - 2012 - Autonomous landing of a VTOL UAV on a moving platform using image-based visual servoing.pdf:pdf},
isbn = {9781467314039},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
pages = {971--976},
title = {{Autonomous landing of a VTOL UAV on a moving platform using image-based visual servoing}},
year = {2012}
}
@article{Lee2010,
abstract = {For successful simultaneous localization and mapping (SLAM), perception of the environment is important. This paper proposes a scheme to autonomously detect visual features that can be used as natural landmarks for indoor SLAM. First, features are roughly selected from the camera image through entropy maps that measure the level of randomness of pixel information. Then, the saliency of each pixel is computed by measuring the level of similarity between the selected features and the given image. In the saliency map, it is possible to distinguish the salient features from the background. The robot estimates its pose by using the detected features and builds a grid map of the unknown environment by using a range sensor. The feature positions are stored in the grid map. Experimental results show that the feature detection method proposed in this paper can autonomously detect features in unknown environments reasonably well. (C) Koninklijke Brill NV, Leiden and The Robotics Society of Japan, 2010},
author = {Lee, Yong-Ju and Song, Jae-bok},
doi = {10.1163/016918610X512613},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Lee, Song - 2010 - Autonomous Salient Feature Detection through Salient Cues in an HSV Color Space for Visual Indoor Simultaneous Locali.pdf:pdf},
isbn = {0169-1864},
issn = {0169-1864},
journal = {Advanced Robotics},
keywords = {mobile robot,salient features,sift,slam,visual attention},
mendeley-tags = {slam},
number = {11},
pages = {1595--1613},
title = {{Autonomous Salient Feature Detection through Salient Cues in an HSV Color Space for Visual Indoor Simultaneous Localization and Mapping}},
url = {http://www.tandfonline.com/doi/abs/10.1163/016918610X512613},
volume = {24},
year = {2010}
}
@article{DeCristoforis2015,
abstract = {In this paper we present a vision-based navigation system for mobile robots equipped with a single, off-the-shelf camera in mixed indoor/outdoor environments. A hybrid approach is proposed, based on the teach-and-replay technique, which combines a path-following and a feature-based navigation algorithm. We describe the navigation algorithms and show that both of them correct the robot's lateral displacement from the intended path. After that, we claim that even though neither of the methods explicitly estimates the robot position, the heading corrections themselves keep the robot position error bound. We show that combination of the methods outperforms the pure feature-based approach in terms of localization precision and that this combination reduces map size and simplifies the learning phase. Experiments in mixed indoor/outdoor environments were carried out with a wheeled and a tracked mobile robots in order to demonstrate the validity and the benefits of the hybrid approach.},
author = {{De Crist{\'{o}}foris}, Pablo and Nitsche, Matias and Krajn{\'{i}}k, Tom{\'{a}}{\v{s}} and Pire, Taih{\'{u}} and Mejail, Marta},
doi = {10.1016/j.patrec.2014.10.010},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/De Crist{\'{o}}foris et al. - 2015 - Hybrid vision-based navigation for mobile robots in mixed indooroutdoor environments.pdf:pdf},
isbn = {0167-8655},
issn = {01678655},
journal = {Pattern Recognition Letters},
keywords = {Mixed indoor/outdoor environments,Mobile robotics,Vision-based navigation,highlight},
mendeley-tags = {highlight},
pages = {118--128},
title = {{Hybrid vision-based navigation for mobile robots in mixed indoor/outdoor environments}},
volume = {53},
year = {2015}
}
@article{Scharstein2002,
abstract = {Stereo matching is one of the most active research areas in computer vision. While a large number of algorithms for stereo correspondence have been developed, relatively little work has been done on characterizing their performance. In this paper, we present a taxonomy of dense, two-frame stereo methods designed to assess the different components and design decisions made in individual stereo algorithms. Using this taxonomy, we compare existing stereo methods and present experiments evaluating the performance of many different variants. In order to establish a common software platform and a collection of data sets for easy evaluation, we have designed a stand-alone, flexible C++ implementation that enables the evaluation of individual components and that can be easily extended to include new algorithms. We have also produced several new multiframe stereo data sets with ground truth, and are making both the code and data sets available on the Web},
author = {Scharstein, Daniel and Szeliski, Richard},
doi = {10.1023/A:1014573219977},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Scharstein, Szeliski - 2002 - A Taxonomy and Evaluation of Dense Two-Frame Stereo Correspondence Algorithms(2).pdf:pdf},
isbn = {0769513271},
issn = {09205691},
journal = {International Journal of Computer Vision},
keywords = {evaluation of performance,stereo correspondence software,stereo matching survey},
number = {1/3},
pages = {7--42},
pmid = {350},
title = {{A Taxonomy and Evaluation of Dense Two-Frame Stereo Correspondence Algorithms}},
url = {http://link.springer.com/10.1023/A:1014573219977},
volume = {47},
year = {2002}
}
@article{Shen2013a,
abstract = {This paper addresses the development of a light-weight autonomous quadrotor that uses cameras and an inexpensive IMU as its only sensors and onboard processors for estimation and control. We describe a fully-functional, integrated system with a focus on robust visual-inertial state estimation, and demonstrate the quadrotor's ability to autonomously travel at speeds up to 4 m/s and roll and pitch angles exceeding 20 degrees. The performance of the proposed system is demonstrated via challenging experiments in three dimensional indoor environments.},
author = {Shen, Shaojie and Mulgaonkar, Y and Michael, Nathan and Kumar, V},
doi = {10.15607/RSS.2013.IX.03},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Shen et al. - 2013 - Vision-based state estimation and trajectory control towards high-speed flight with a quadrotor.pdf:pdf},
journal = {Robotics: Science and {\ldots}},
title = {{Vision-based state estimation and trajectory control towards high-speed flight with a quadrotor}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.309.7992{\&}rep=rep1{\&}type=pdf},
year = {2013}
}
@phdthesis{Rannacher2009,
author = {Rannacher, Jens},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Rannacher - 2009 - Realtime 3D Motion Estimation on Graphics Hardware.pdf:pdf},
school = {Heidelberg University},
title = {{Realtime 3D Motion Estimation on Graphics Hardware}},
type = {Undergraduate Thesis},
year = {2009}
}
@article{Pu2018,
abstract = {Fusing disparity maps from different algorithms to exploit their complementary advantages is still challenging. Uncertainty estimation and complex disparity relationships between neighboring pixels limit the accuracy and robustness of the existing methods and there is no common method for depth fusion of different kind of data. In this paper, we introduce a method to incorporate supplementary information (intensity, gradient constraints etc.) into a Generative Adversarial Network to better refine each input disparity value. By adopting a multi-scale strategy, the disparity relationship in the fused disparity map is a better estimate of the real distribution. The approach includes a more robust object function to avoid blurry edges, impaints invalid disparity values and requires much fewer ground data to train. The algorithm can be generalized to different kinds of depth fusion. The experiments were conducted through simulation and real data, exploring different fusion opportunities: stereo-monocular fusion from coarse input, stereo-stereo fusion from moderately accurate input, fusion from accurate binocular input and Time of Flight sensor. The experiments show the superiority of the proposed algorithm compared with the most recent algorithms on the public Scene Flow and SYNTH3 datasets. The code is available from https://github.com/Canpu999},
archivePrefix = {arXiv},
arxivId = {1803.06657},
author = {Pu, Can and Song, Runzi and Li, Nanbo and Fisher, Robert B},
eprint = {1803.06657},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Pu et al. - 2018 - Sdf-GAN Semi-supervised Depth Fusion with Multi-scale Adversarial Networks.pdf:pdf},
keywords = {depth fusion,disparity fusion,gan,monocular vi-,sion,stereo vision,time of flight},
pages = {1--17},
title = {{Sdf-GAN: Semi-supervised Depth Fusion with Multi-scale Adversarial Networks}},
url = {http://arxiv.org/abs/1803.06657},
year = {2018}
}
@article{Ummenhofer2016,
abstract = {In this paper we formulate structure from motion as a learning problem. We train a convolutional network end-to-end to compute depth and camera motion from successive, unconstrained image pairs. The architecture is composed of multiple stacked encoder-decoder networks, the core part being an iterative network that is able to improve its own predictions. The network estimates not only depth and motion, but additionally surface normals, optical flow between the images and confidence of the matching. A crucial component of the approach is a training loss based on spatial relative differences. Compared to traditional two-frame structure from motion methods, results are more accurate and more robust. In contrast to the popular depth-from-single-image networks, DeMoN learns the concept of matching and, thus, better generalizes to structures not seen during training.},
archivePrefix = {arXiv},
arxivId = {1612.02401},
author = {Ummenhofer, Benjamin and Zhou, Huizhong and Uhrig, Jonas and Mayer, Nikolaus and Ilg, Eddy and Dosovitskiy, Alexey and Brox, Thomas},
doi = {10.1109/CVPR.2017.596},
eprint = {1612.02401},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Ummenhofer et al. - 2016 - DeMoN Depth and Motion Network for Learning Monocular Stereo.pdf:pdf},
journal = {arXiv preprint arXiv:1612.02401},
title = {{DeMoN: Depth and Motion Network for Learning Monocular Stereo}},
url = {http://arxiv.org/abs/1612.02401},
year = {2016}
}
@phdthesis{Vikenmark2006,
author = {Vikenmark, Daniel},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Vikenmark - 2006 - The Obstacle-Restriction Method (ORM) for Reactive Obstacle Avoidance in Difficult Scenarios in Three-Dimensional Wor.pdf:pdf},
school = {KTH},
title = {{The Obstacle-Restriction Method (ORM) for Reactive Obstacle Avoidance in Difficult Scenarios in Three-Dimensional Workspaces}},
type = {Master of Science Thesis},
url = {http://w3.nada.kth.se/utbildning/grukth/exjobb/rapportlistor/2006/rapporter06/vikenmark{\_}daniel{\_}06172.pdf},
year = {2006}
}
@article{Mancini2017,
abstract = {In this work, we give a new twist to monocular obstacle detection. Most of the existing approaches either rely on Visual SLAM systems or on depth estimation models to build 3D maps and detect obstacles. Despite their success, these methods are not specifically devised for monocular obstacle detection. In particular, they are not robust to appearance and camera intrinsics changes or texture-less scenarios. To overcome these limitations, we propose an end-to-end deep architecture that jointly learns to detect obstacle and estimate their depth. The multi task nature of this strategy strengthen both the obstacle detection task with more reliable bounding boxes and range measures and the depth estimation one with robustness to scenario changes. We call this architecture J-MOD{\$}{\^{}}{\{}2{\}}{\$} We prove the effectiveness of our approach with experiments on sequences with different appearance and focal lengths. Furthermore, we show its benefits on a set of simulated navigation experiments where a MAV explores an unknown scenario and plans safe trajectories by using our detection model.},
archivePrefix = {arXiv},
arxivId = {1709.08480},
author = {Mancini, Michele and Costante, Gabriele and Valigi, Paolo and Ciarfuglia, Thomas A.},
eprint = {1709.08480},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Mancini et al. - 2017 - J-MOD² Joint Monocular Obstacle Detection and Depth Estimation.pdf:pdf},
title = {{J-MOD²: Joint Monocular Obstacle Detection and Depth Estimation}},
url = {http://arxiv.org/abs/1709.08480},
year = {2017}
}
@article{Remolina2004,
abstract = {We present a general theory of topological maps whereby sensory input, topological and local metrical information are combined to define the topological maps explaining such information. Topological maps correspond to the minimal models of an axiomatic theory describing the relationships between the different sources of information explained by a map. We use a circumscriptive theory to specify the minimal models associated with this representation. The theory here proposed is independent of the exploration strategy the agent follows when building a map. We provide an algorithm to calculate the models of the theory. This algorithm supports different exploration strategies and facilitates map disambiguation when perceptual aliasing arises. ?? 2003 Elsevier B.V. All rights reserved.},
author = {Remolina, Emilio and Kuipers, Benjamin},
doi = {10.1016/S0004-3702(03)00114-0},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Remolina, Kuipers - 2004 - Towards a general theory of topological maps.pdf:pdf},
isbn = {00043702},
issn = {00043702},
journal = {Artificial Intelligence},
keywords = {Causal maps,Cognitive robotics,Map building,Nested abnormality theories (NATs),Spatial representations,Spatial semantic hierarchy (SSH),Topological maps,View graph},
number = {1},
pages = {47--104},
title = {{Towards a general theory of topological maps}},
volume = {152},
year = {2004}
}
@inproceedings{Petrides2017,
author = {Petrides, P and Kyrkou, C and Kolios, P and Theocharides, T and Panayiotou, C},
booktitle = {International Conference on Unmanned Aircraft Systems (ICUAS)},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Petrides et al. - 2017 - Towards a Holistic Performance Evaluation Framework for Drone-Based Object Detection.pdf:pdf},
isbn = {9781509044948},
pages = {1785--1793},
title = {{Towards a Holistic Performance Evaluation Framework for Drone-Based Object Detection}},
year = {2017}
}
@inproceedings{Huguet2007,
abstract = {This paper presents a method for scene flow estimation from a calibrated stereo image sequence. The scene flow contains the 3-D displacement field of scene points, so that the 2-D optical flow can be seen as a projection of the scene flow onto the images. We propose to recover the scene flow by coupling the optical flow estimation in both cameras with dense stereo matching between the images, thus reducing the number of unknowns per image point. The use of a variational framework allows us to properly handle discontinuities in the observed surfaces and in the 3-D displacement field. Moreover our approach handles occlusions both for the optical flow and the stereo. We obtain a partial differential equations system coupling both the optical flow and the stereo, which is numerically solved using an original multi- resolution algorithm. Whereas previous variational methods were estimating the 3-D reconstruction at time t and the scene flow separately, our method jointly estimates both in a single optimization. We present numerical results on synthetic data with ground truth information, and we also compare the accuracy of the scene flow projected in one camera with a state-of-the-art single-camera optical flow computation method. Results are also presented on a real stereo sequence with large motion and stereo discontinuities. Source code and sample data are available for the evaluation of the algorithm.},
author = {Huguet, F and Devernay, F},
booktitle = {2007 IEEE 11th International Conference on Computer Vision},
doi = {10.1109/ICCV.2007.4409000},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Huguet, Devernay - 2007 - A Variational Method for Scene Flow Estimation from Stereo Sequences.pdf:pdf},
issn = {1550-5499},
keywords = {2D optical flow,Cameras,Data flow computing,Image motion analysis,Image sequences,Layout,Optical computing,Optical coupling,Optimization methods,Partial differential equations,Three dimensional displays,calibrated stereo image sequence,dense stereo matching,image matching,image sequences,optical flow estimation,partial differential equations,partial differential equations system,scene flow estimation,state-of-the-art single-camera optical flow comput,stereo discontinuities,stereo image processing,variational method,variational techniques},
pages = {1--7},
title = {{A Variational Method for Scene Flow Estimation from Stereo Sequences}},
year = {2007}
}
@article{Pravitra2011,
abstract = {This paper presents a compact exploration strategy designed to be implemented onboard indoor Miniature Air Vehicles (MAVs) operating in cluttered and confined environments. The exploration strategy uses 2D range information from a laser range scanner to generate velocity commands by blending a Sensor-based Random Tree frontier planner with a wall-following velocity field generator. The combined approach leverages the efficient exploration capabilities of frontier-based guidance and ensures that the vehicles follows a path that is free of obstacles and conducive to maintaining good scan geometry through a wall-following approach. The strategy has been successfully implemented and tested on a Quadrotor MAV and simulation results are presented. Flight test results shall be included in the final version.},
author = {Pravitra, Chintasid and Chowdhary, Girish and Johnson, Eric},
doi = {10.1109/CDC.2011.6161200},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Pravitra, Chowdhary, Johnson - 2011 - A compact exploration strategy for indoor flight vehicles.pdf:pdf},
isbn = {9781612848006},
issn = {01912216},
journal = {Proceedings of the IEEE Conference on Decision and Control},
keywords = {exploration,slam},
mendeley-tags = {exploration,slam},
pages = {3572--3577},
title = {{A compact exploration strategy for indoor flight vehicles}},
year = {2011}
}
@article{Fennema1979,
abstract = {A method is described which quantifies the speed and direction of several moving objects in a sequence of digital images. A relationship between the time variation of intensity, the spatial gradient, and velocity has been developed which allows the determination of motion using clustering techniques. This paper describes these relationships, the clustering technique, and provides examples of the technique on real images containing several moving objects. {\textcopyright} 1979.},
author = {Fennema, Claude L. and Thompson, William B.},
doi = {10.1016/0146-664X(79)90097-2},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Fennema, Thompson - 1979 - Velocity determination in scenes containing several moving objects.pdf:pdf},
issn = {0146664X},
journal = {Computer Graphics and Image Processing},
keywords = {optical flow},
mendeley-tags = {optical flow},
number = {4},
pages = {301--315},
title = {{Velocity determination in scenes containing several moving objects}},
volume = {9},
year = {1979}
}
@article{Coombs1998,
abstract = {The lure of using motion vision as a fundamental element in the$\backslash$nperception of space drives this effort to use flow features as the sole$\backslash$ncues for robot mobility. Real-time estimates of image flow and flow$\backslash$ndivergence provide the robot's sense of space. The robot steers down a$\backslash$nconceptual corridor, comparing left and right peripheral flows. Large$\backslash$ncentral flow divergence warns the robot of impending collisions at$\backslash$n{\&}ldquo;dead ends{\&}rdquo;. When this occurs, the robot turns around and$\backslash$nresumes wandering. Behavior is generated by directly using flow-based$\backslash$ninformation in the 2D image sequence. Active mechanical gaze$\backslash$nstabilization simplifies the visual interpretation problems by reducing$\backslash$ncamera rotation. By combining corridor following and dead-end$\backslash$ndeflection, the robot has wandered around the lab at 30 cm/s for as long$\backslash$nas 20 min without collision. The ability to support this behavior in$\backslash$nreal-time with current equipment promises expanded capabilities as$\backslash$ncomputational power increases in the future},
author = {Coombs, David and Herman, Martin and Hong, Tsai Hong and Nashman, Marilyn},
doi = {10.1109/70.660840},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Coombs et al. - 1998 - Real-time obstacle avoidance using central flow divergence, and peripheral flow.pdf:pdf},
isbn = {0-8186-7042-8},
issn = {1042296X},
journal = {IEEE Transactions on Robotics and Automation},
keywords = {Active vision,Digital control,Feedback systems,Image motion analysis,Mobile robot motion planning,Mobile robots,Object identification,Real time systems,Recursive estimation,Robot vision systems},
number = {1},
pages = {49--59},
title = {{Real-time obstacle avoidance using central flow divergence, and peripheral flow}},
volume = {14},
year = {1998}
}
@article{Kern2012,
abstract = {Blowfly flight consists of two main components, saccadic turns and intervals of mostly straight gaze direction, although, as a consequence of inertia, flight trajectories usually change direction smoothly. We investigated how flight behavior changes depending on the surroundings and how saccadic turns and intersaccadic translational movements might be controlled in arenas of different width with and without obstacles. Blowflies do not fly in straight trajectories, even when traversing straight flight arenas; rather, they fly in meandering trajectories. Flight speed and the amplitude of meanders increase with arena width. Although saccade duration is largely constant, peak angular velocity and succession into either direction are variable and depend on the visual surroundings. Saccade rate and amplitude also vary with arena layout and are correlated with the 'time-to-contact' to the arena wall. We provide evidence that both saccade and velocity control rely to a large extent on the intersaccadic optic flow generated in eye regions looking well in front of the fly, rather than in the lateral visual field, where the optic flow at least during forward flight tends to be strongest.},
author = {Kern, R. and Boeddeker, N. and Dittmar, L. and Egelhaaf, M.},
doi = {10.1242/jeb.061713},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Kern et al. - 2012 - Blowfly flight characteristics are shaped by environmental features and controlled by optic flow information.pdf:pdf},
isbn = {0022-0949},
issn = {0022-0949},
journal = {Journal of Experimental Biology},
keywords = {accepted 23 march 2012,active vision,flight control,insect,obstacle avoidance,received 16 june 2011,saccade,translation velocity},
number = {14},
pages = {2501--2514},
pmid = {22723490},
title = {{Blowfly flight characteristics are shaped by environmental features and controlled by optic flow information}},
volume = {215},
year = {2012}
}
@article{Srinivasan2011,
author = {Srinivasan, Mandyam V},
doi = {10.1152/physrev.00005.2010},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Srinivasan - 2011 - Honeybees as a Model for the Study of Visually Guided Flight, Navigation, and Biologically Inspired Robotics.pdf:pdf},
issn = {0031-9333},
journal = {Physiological Reviews},
keywords = {biology,survey},
mendeley-tags = {biology,survey},
month = {apr},
number = {2},
pages = {413--460},
title = {{Honeybees as a Model for the Study of Visually Guided Flight, Navigation, and Biologically Inspired Robotics}},
url = {http://physrev.physiology.org/cgi/doi/10.1152/physrev.00005.2010},
volume = {91},
year = {2011}
}
@article{Yu2010,
abstract = {This paper presents a vision-based navigation frame mapping and path planning technique for collision avoidance for Miniature Air Vehicles. A depth map that represents the range and bearing to obstacles is obtained by computer vision. Based on the depth map, an extended Kalman Filter is used to estimate the range and bearing. Using this information, a map, constructed in polar coordinates, is created in the navigation frame of the MAV. The Rapidly-exploring Random Tree algorithm is employed to find a collision-free path in the navigation frame. The proposed algorithm was successfully implemented in both simulation and flight tests. ?? 2010 Elsevier Ltd.},
author = {Yu, Huili and Beard, Randy and Byrne, Jeffrey},
doi = {10.1016/j.conengprac.2010.02.001},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Yu, Beard, Byrne - 2010 - Vision-based navigation frame mapping and planning for collision avoidance for miniature air vehicles.pdf:pdf},
isbn = {9781424445240},
issn = {09670661},
journal = {Control Engineering Practice},
keywords = {Collision avoidance,Computer vision,Mapping and path planning,Miniature air vehicle},
number = {7},
pages = {824--836},
publisher = {Elsevier},
title = {{Vision-based navigation frame mapping and planning for collision avoidance for miniature air vehicles}},
url = {http://dx.doi.org/10.1016/j.conengprac.2010.02.001},
volume = {18},
year = {2010}
}
@article{Smith2008,
author = {Smith, Lincoln and Philippides, Andrew and Graham, Paul and Husbands, Phil},
doi = {10.1007/978-3-540-69134-1_18},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Smith et al. - 2008 - Linked local visual navigation and robustness to motor noise and route displacement.pdf:pdf},
isbn = {3540691332},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {179--188},
title = {{Linked local visual navigation and robustness to motor noise and route displacement}},
volume = {5040 LNAI},
year = {2008}
}
@article{Ramisa2011,
author = {Ramisa, Arnau and Goldhoorn, Alex and Aldavert, David and Toledo, Ricardo and Lopez, Ramon},
doi = {10.1007/s10846-011-9552},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Ramisa et al. - 2011 - Combining Invariant Features and the ALV Homing Method for Autonomous Robot Navigation Based on Panoramas.pdf:pdf},
journal = {Journal of Intelligent {\&} Robotic Systems},
keywords = {biologically inspired methods,local features,visual homing},
number = {3},
pages = {625--649},
title = {{Combining Invariant Features and the ALV Homing Method for Autonomous Robot Navigation Based on Panoramas}},
volume = {64},
year = {2011}
}
@article{Alcantarilla2013,
abstract = {We propose a novel and fast multiscale feature detection and description approach that exploits the benefits of nonlinear scale spaces. Previous attempts to detect and describe features in nonlinear scale spaces are highly time consuming due to the computational burden of creating the nonlinear scale space. In this paper we propose to use recent numerical schemes called Fast Explicit Diffusion (FED) embedded in a pyramidal framework to dramatically speed-up feature detection in nonlinear scale spaces. In addition, we introduce a Modified-Local Difference Binary (M-LDB) descriptor that is highly efficient, exploits gradient information from the nonlinear scale space, is scale and rotation invariant and has low storage requirements. We present an extensive evaluation that shows the excellent compromise between speed and performance of our approach compared to state-of-the-art methods such as BRISK, ORB, SURF, SIFT and KAZE.},
author = {Alcantarilla, Pablo Fern{\'{a}}ndez and Nuevo, Jes{\'{u}}s and Bartoli, Adrien},
doi = {10.5244/C.27.13},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Alcantarilla, Nuevo, Bartoli - 2013 - Fast Explicit Diffusion for Accelerated Features in Nonlinear Scale Spaces.pdf:pdf},
isbn = {1-901725-49-9},
journal = {British Machine Vision Conference},
pages = {13.1--13.11},
title = {{Fast Explicit Diffusion for Accelerated Features in Nonlinear Scale Spaces}},
url = {http://www.robesafe.com/personal/pablo.alcantarilla/kaze.html},
year = {2013}
}
@article{TomasCardosoRezioMartins2017a,
author = {{Tom{\'{a}}s Cardoso R{\'{e}}zio Martins}, Diogo and van Hecke, Kevin and de Croon, Guido},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Tom{\'{a}}s Cardoso R{\'{e}}zio Martins, van Hecke, de Croon - 2017 - Fusion of stereo and monocular depth estimates in a self-supervised learning.pdf:pdf},
title = {{Fusion of stereo and monocular depth estimates in a self-supervised learning context}},
year = {2017}
}
@inproceedings{Mayer2016,
author = {Mayer, Nikolaus and Ilg, Eddy and Hausser, Philip and Fischer, Philipp and Cremers, Daniel and Dosovitskiy, Alexey and Brox, Thomas},
booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Mayer et al. - 2016 - A Large Dataset to Train Convolutional Networks for Disparity, Optical Flow, and Scene Flow Estimation.pdf:pdf},
title = {{A Large Dataset to Train Convolutional Networks for Disparity, Optical Flow, and Scene Flow Estimation}},
year = {2016}
}
@article{Argyros2005,
abstract = {We propose a novel, vision-based method for robot homing, the problem of computing a route so that a robot can return to its initial “home” position after the execution of an arbitrary “prior” path. The method assumes that the robot tracks visual features in panoramic views of the environment that it acquires as it moves. By exploiting only angular information regarding the tracked features, a local control strategy moves the robot between two positions, provided that there are at least three features that can be matched in the panoramas acquired at these positions. The strategy is successful when certain geometric constraints on the configuration of the two positions relative to the features are fulfilled. In order to achieve long-range homing, the features' trajectories are organized in a visual memory during the execution of the “prior” path. When homing is initiated, the robot selects Milestone Positions (MPs) on the “prior” path by exploiting information in its visual memory. The MP selection process aims at picking positions that guarantee the success of the local control strategy between two consecutive MPs. The sequential visit of successive MPs successfully guides the robot even if the visual context in the “home” position is radically different from the visual context at the position where homing was initiated. Experimental results from a prototype implementation of the method demonstrate that homing can be achieved with high accuracy, independent of the distance traveled by the robot. The contribution of this work is that it shows how a complex navigational task such as homing can be accomplished efficiently, robustly and in real-time by exploiting primitive visual cues. Such cues carry implicit information regarding the 3D structure of the environment. Thus, the computation of explicit range information and the existence of a geometric map are not required.},
author = {Argyros, Antonis A. and Bekris, Kostas E. and Orphanoudakis, Stelios C. and Kavraki, Lydia E.},
doi = {10.1007/s10514-005-0603-7},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Argyros et al. - 2005 - Robot homing by exploiting panoramic vision.pdf:pdf},
isbn = {09295593},
issn = {09295593},
journal = {Autonomous Robots},
keywords = {Omni-directional vision,Panoramic cameras,Robot homing,Vision-based robot navigation},
number = {1},
pages = {7--25},
title = {{Robot homing by exploiting panoramic vision}},
volume = {19},
year = {2005}
}
@inproceedings{Camus1996,
author = {Camus, T. and Coombs, D. and Herman, M. and {Tsai-Hong Hong}},
booktitle = {Proceedings of 13th International Conference on Pattern Recognition},
doi = {10.1109/ICPR.1996.546964},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Camus et al. - 1996 - Real-time single-workstation obstacle avoidance using only wide-field flow divergence.pdf:pdf},
isbn = {0-8186-7282-X},
issn = {08866236},
keywords = {experiment,nieuwe referenties,obstacle detection,optical flow,reactive obstacle avoidance,scenario: room},
mendeley-tags = {experiment,nieuwe referenties,obstacle detection,optical flow,reactive obstacle avoidance,scenario: room},
month = {jun},
number = {6},
pages = {323--330 vol.3},
publisher = {IEEE},
title = {{Real-time single-workstation obstacle avoidance using only wide-field flow divergence}},
url = {http://doi.wiley.com/10.1002/2014GB005021 http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=546964},
volume = {29},
year = {1996}
}
@article{Neumann2001,
abstract = {Flying insects use highly efficient visual strategies to control their self-motion in three-dimensional space. We present a biologically inspired, minimalistic model for visual flight control in an autonomous agent. Large, specialized receptive fields exploit the distribution of local intensities and local motion in an omnidirectional field of view, extracting the information required for attitude control, course stabilization, obstacle avoidance, and altitude control. In open-loop simulations, recordings from each control mechanism robustly indicate the sign of attitude angles, self rotation, obstacle dircetion and altitude deviation, respectively. Closed-loop experiments show that these signals are sufficient for three-dimensional flight stabilization with six degrees of freedom.},
author = {Neumann, Tr and B{\"{u}}lthoff, Hh},
doi = {10.1007/3-540-44811-X_71},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Neumann, B{\"{u}}lthoff - 2001 - Insect inspired visual control of translatory flight.pdf:pdf},
isbn = {3-540-44811-X},
issn = {16113349},
journal = {Advances in Artificial Life},
pages = {627--636},
title = {{Insect inspired visual control of translatory flight}},
url = {http://link.springer.com/chapter/10.1007/3-540-44811-X{\_}71},
year = {2001}
}
@article{Fu2012,
abstract = {This paper proposes an image sequence-based navigation method under the teaching-replay framework for robots in piecewise linear routes.Waypoints used by the robot contain either the positions with large heading changes or selected midway positions between junctions. The robot applies local visual homing to move between consecutive waypoints. The arrival at a waypoint is determined by minimizing the average vertical displacements of feature correspondences. The performance of the proposed approach is supported by extensive experiments in hallway and office environments. While the homing speed of robots using other approaches is constrained by the speed in the teaching phase, our robot is not bounded by such limit and can travel much faster without compromising the homing accuracy.},
author = {Fu, Yu and Hsiang, Tien-Ruey and Chung, Sheng-Luen},
doi = {10.1017/S0263574712000434},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Fu, Hsiang, Chung - 2012 - Multi-waypoint visual homing in piecewise linear trajectory.pdf:pdf},
issn = {0263-5747},
journal = {Robotica},
keywords = {multi-waypoint visual homing,piecewise},
number = {03},
pages = {479--491},
title = {{Multi-waypoint visual homing in piecewise linear trajectory}},
url = {http://www.journals.cambridge.org/abstract{\_}S0263574712000434},
volume = {31},
year = {2012}
}
@article{Milford2010,
abstract = {The challenge of persistent navigation and mapping is to develop an autonomous robot system that can simultaneously localize, map and navigate over the lifetime of the robot with little or no human inter-vention. Most solutions to the simultaneous localization and mapping (SLAM) problem aim to produce highly accurate maps of areas that are assumed to be static. In contrast, solutions for persistent navi-gation and mapping must produce reliable goal-directed navigation outcomes in an environment that is assumed to be in constant flux. We investigate the persistent navigation and mapping problem in the context of an autonomous robot that performs mock deliveries in a working office environment over a two-week period. The so-lution was based on the biologically inspired visual SLAM system, RatSLAM. RatSLAM performed SLAM continuously while interacting with global and local navigation systems, and a task selection module that selected between exploration, delivery, and recharging modes. The robot performed 1,143 delivery tasks to 11 different locations with only one delivery failure (from which it recovered), traveled a total distance of more than 40 km over 37 hours of active operation, and recharged autonomously a total of 23 times.},
author = {Milford, Michael and Wyeth, Gordon},
doi = {10.1177/0278364909340592},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Milford, Wyeth - 2010 - Persistent Navigation and Mapping using a Biologically Inspired SLAM System.pdf:pdf},
issn = {0278-3649},
journal = {The International Journal of Robotics Research},
keywords = {biology,highlight,slam},
mendeley-tags = {biology,highlight,slam},
month = {aug},
number = {9},
pages = {1131--1153},
title = {{Persistent Navigation and Mapping using a Biologically Inspired SLAM System}},
url = {http://ijr.sagepub.com/cgi/doi/10.1177/0278364909340592},
volume = {29},
year = {2010}
}
@article{Piasco2016,
abstract = {This paper considers collaborative stereo-vision as a mean of localization for a fleet of micro-air vehicles (MAV) equipped with monocular cameras, inertial measurement units and sonar sensors. A sensor fusion scheme using an extended Kalman filter is designed to estimate the positions and orienta- tions of all the vehicles from these distributed measurements. The estimation is completed by a formation control to maximize the overlapping fields of view of the vehicles. Experimental tests for the complete perception and control loop have been performed on multiple MAVs with centralized processing on a ROS ground station.},
author = {Piasco, Nathan and Marzat, Julien and Sanfourche, Martial},
doi = {10.1109/ICRA.2016.7487251},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Piasco, Marzat, Sanfourche - 2016 - Collaborative localization and formation flying using distributed stereo-vision.pdf:pdf},
isbn = {9781467380263},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
keywords = {collaborative localization,formation control,micro-air vehicles,stereo-vision},
pages = {1202--1207},
title = {{Collaborative localization and formation flying using distributed stereo-vision}},
volume = {2016-June},
year = {2016}
}
@inproceedings{Moore2014,
abstract = {— This study describes the design and implementa-tion of several bioinspired algorithms for providing guidance to an ultra-lightweight micro-aerial vehicle (MAV) using a 2.6 g omnidirectional vision sensor. Using this visual guidance sys-tem we demonstrate autonomous speed control, centring, and heading stabilisation on board a 30 g MAV flying in a corridor-like environment. In addition to the computation of wide-field optic flow, the comparatively high-resolution omnidirectional imagery provided by this sensor also offers the potential for image-based algorithms such as landmark recognition to be implemented in the future.},
annote = {Lightweight omnicamera, tip van Kimberly},
author = {Moore, Richard J D and Dantu, Karthik and Barrows, Geoffrey L and Nagpal, Radhika},
booktitle = {Robotics and Automation (ICRA), 2014 IEEE International Conference on},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Moore et al. - 2014 - Autonomous MAV guidance with a lightweight omnidirectional vision sensor.pdf:pdf},
keywords = {optical flow},
mendeley-tags = {optical flow},
pages = {3856----3861},
title = {{Autonomous MAV guidance with a lightweight omnidirectional vision sensor}},
year = {2014}
}
@inproceedings{Engel2014,
abstract = {We propose a direct (feature-less) monocular SLAM algorithm which, in contrast to current state-of-the-art regarding direct meth- ods, allows to build large-scale, consistent maps of the environment. Along with highly accurate pose estimation based on direct image alignment, the 3D environment is reconstructed in real-time as pose-graph of keyframes with associated semi-dense depth maps. These are obtained by filtering over a large number of pixelwise small-baseline stereo comparisons. The explicitly scale-drift aware formulation allows the approach to operate on challenging sequences including large variations in scene scale. Major enablers are two key novelties: (1) a novel direct tracking method which operates on sim(3), thereby explicitly detecting scale-drift, and (2) an elegant probabilistic solution to include the effect of noisy depth values into tracking. The resulting direct monocular SLAM system runs in real-time on a CPU.},
author = {Engel, Jakob and Sch{\"{o}}ps, Thomas and Cremers, Daniel},
booktitle = {European Conference on Computer Vision},
doi = {10.1007/978-3-319-10605-2_54},
file = {:home/tom/Documents/phd/mendeley{\_}pdf//Engel, Sch{\"{o}}ps, Cremers - 2014 - LSD-SLAM Large-Scale Direct monocular SLAM.pdf:pdf},
isbn = {9783319106045},
issn = {16113349},
keywords = {monocular},
mendeley-tags = {monocular},
pages = {834--849},
publisher = {Springer},
title = {{LSD-SLAM: Large-Scale Direct monocular SLAM}},
year = {2014}
}
@article{Alcantarilla2012,
author = {Alcantarilla, P F and Bartoli, A and Davison, A J},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Alcantarilla, Bartoli, Davison - 2012 - KAZE Features.pdf:pdf},
journal = {Eur. Conf. on Computer Vision (ECCV)},
pages = {214--227},
title = {{KAZE Features}},
year = {2012}
}
@article{Williams2011,
abstract = {A Feature and Pose Constrained Extended Kalman Filter (FPC-EKF) is developed for highly dynamic computationally constrained micro aerial vehicles. Vehicle localization is achieved using only a low performance inertial measurement unit and a single camera. The FPC-EKF framework augments the vehicle{\&}apos;s state with both previous vehicle poses and critical environmental features, including vertical edges. This filter framework efficiently incorporates measurements from hundreds of opportunistic visual features to constrain the motion estimate, while allowing navigating and sustained tracking with respect to a few persistent features. In addition, vertical features in the environment are opportunistically used to provide global attitude references. Accurate pose estimation is demonstrated on a sequence including fast traversing, where visual features enter and exit the fleld-of-view quickly, as well as hover and ingress maneuvers where drift free navigation is achieved with respect to the environment. View full abstract},
author = {Williams, Brian and Hudson, Nicolas and Tweddle, Brent and Brockers, Roland and Matthies, Larry},
doi = {10.1109/ICRA.2011.5979997},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Williams et al. - 2011 - Feature and pose constrained visual aided inertial navigation for computationally constrained aerial vehicles.pdf:pdf},
isbn = {9781612843865},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
pages = {431--438},
pmid = {5979997},
title = {{Feature and pose constrained visual aided inertial navigation for computationally constrained aerial vehicles}},
year = {2011}
}
@inproceedings{Angeli2006,
author = {Angeli, Adrien and Filliat, David and Doncieux, S and Meyer, J.-A.},
booktitle = {European Micro Aerial Vehicles (EMAV)},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Angeli et al. - 2006 - 2D Simultaneous Localization And Mapping for Micro Aerial Vehicles.pdf:pdf},
title = {{2D Simultaneous Localization And Mapping for Micro Aerial Vehicles}},
year = {2006}
}
@article{Hrabar2011,
abstract = {We present a goal-directed 3D reactive obstacle avoidance algorithm specifically designed for Rotorcraft Unmanned Aerial Vehicles (RUAVs) that fly point-to-point type trajectories. The algorithm detects potential collisions within a cylindrical Safety Volume projected ahead of the UAV. This is done in a 3D occupancy map representation of the environment. An expanding elliptical search is performed to find an Escape Point; a waypoint which offers a collision free route past obstacles and towards a goal waypoint. An efficient occupied voxel checking technique is employed which approximates the Safety Volume by a series of spheres, and uses an approximate nearest neighbour search in a Bkd-tree representation of the occupied voxels. Tests show the algorithm can typically find an Escape Point in under 100 ms using onboard UAV processing for a cluttered environment with 20 000 occupied voxels. Successful collision avoidance results are presented from simulation experiments and from flights with an autonomous helicopter equipped with stereo and laser range sensors.},
author = {Hrabar, Stefan},
doi = {10.1109/IROS.2011.6048312},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Hrabar - 2011 - Reactive obstacle avoidance for rotorcraft UAVs.pdf:pdf},
isbn = {9781612844541},
issn = {2153-0858},
journal = {IEEE International Conference on Intelligent Robots and Systems},
pages = {4967--4974},
title = {{Reactive obstacle avoidance for rotorcraft UAVs}},
year = {2011}
}
@article{Oord2016,
abstract = {This paper introduces WaveNet, a deep neural network for generating raw audio waveforms. The model is fully probabilistic and autoregressive, with the predictive distribution for each audio sample conditioned on all previous ones; nonetheless we show that it can be efficiently trained on data with tens of thousands of samples per second of audio. When applied to text-to-speech, it yields state-of-the-art performance, with human listeners rating it as significantly more natural sounding than the best parametric and concatenative systems for both English and Mandarin. A single WaveNet can capture the characteristics of many different speakers with equal fidelity, and can switch between them by conditioning on the speaker identity. When trained to model music, we find that it generates novel and often highly realistic musical fragments. We also show that it can be employed as a discriminative model, returning promising results for phoneme recognition.},
archivePrefix = {arXiv},
arxivId = {1609.03499},
author = {van den Oord, Aaron and Dieleman, Sander and Zen, Heiga and Simonyan, Karen and Vinyals, Oriol and Graves, Alex and Kalchbrenner, Nal and Senior, Andrew and Kavukcuoglu, Koray},
doi = {10.1109/ICASSP.2009.4960364},
eprint = {1609.03499},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Oord et al. - 2016 - WaveNet A Generative Model for Raw Audio.pdf:pdf},
isbn = {9783901882760},
issn = {0899-7667},
pages = {1--15},
pmid = {18785855},
title = {{WaveNet: A Generative Model for Raw Audio}},
url = {http://arxiv.org/abs/1609.03499},
year = {2016}
}
@article{Gonz??lez2016,
abstract = {—Intelligent vehicles have increased their capabilities for highly and, even fully, automated driving under controlled environments. Scene information is received using onboard sen-sors and communication network systems, i.e., infrastructure and other vehicles. Considering the available information, different motion planning and control techniques have been implemented to autonomously driving on complex environments. The main goal is focused on executing strategies to improve safety, comfort, and energy optimization. However, research challenges such as navi-gation in urban dynamic environments with obstacle avoidance capabilities, i.e., vulnerable road users (VRU) and vehicles, and cooperative maneuvers among automated and semi-automated vehicles still need further efforts for a real environment implemen-tation. This paper presents a review of motion planning techniques implemented in the intelligent vehicles literature. A description of the technique used by research teams, their contributions in motion planning, and a comparison among these techniques is also presented. Relevant works in the overtaking and obstacle avoidance maneuvers are presented, allowing the understanding of the gaps and challenges to be addressed in the next years. Finally, an overview of future research direction and applications is given.},
author = {Gonz??lez, David and P??rez, Joshu?? and Milan??s, Vicente and Nashashibi, Fawzi},
doi = {10.1109/TITS.2015.2498841},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Gonzlez et al. - 2016 - A Review of Motion Planning Techniques for Automated Vehicles.pdf:pdf},
isbn = {1524-9050},
issn = {15249050},
journal = {IEEE Transactions on Intelligent Transportation Systems},
keywords = {Motion planning,automated vehicles,intelligent transportation systems,motion planning,path planning,survey},
mendeley-tags = {motion planning,survey},
number = {4},
pages = {1135--1145},
title = {{A Review of Motion Planning Techniques for Automated Vehicles}},
volume = {17},
year = {2016}
}
@article{Leishman2014a,
abstract = {Results are presented that quantify how velocity and attitude estimates can benefit from an improvement to the traditional quadrotor dynamic model. The improved model allows accelerometer measurements, which are routinely available at high rates, to reduce an estimator's dependence on complex exteroceptive measurements, such as those obtained by an onboard camera or laser range finder.},
author = {Leishman, Robert C. and MacDonald, John C. and Beard, Randal W. and McLain, Timothy W.},
doi = {10.1109/MCS.2013.2287362},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Leishman et al. - 2014 - Quadrotors and accelerometers State estimation with an improved dynamic model.pdf:pdf},
isbn = {1066-033X VO - 34},
issn = {1066033X},
journal = {IEEE Control Systems},
number = {1},
pages = {28--41},
title = {{Quadrotors and accelerometers: State estimation with an improved dynamic model}},
volume = {34},
year = {2014}
}
@inproceedings{Eigen2014,
author = {Eigen, David and Puhrsch, Christian and Fergus, Rob},
booktitle = {Advances in Neural Information Processing Systems 27},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Eigen, Puhrsch, Fergus - 2014 - Depth Map Prediction from a Single Image using a Multi-Scale Deep Network.pdf:pdf},
pages = {2366--2374},
publisher = {Curran Associates, Inc.},
title = {{Depth Map Prediction from a Single Image using a Multi-Scale Deep Network}},
url = {http://papers.nips.cc/paper/5539-depth-map-prediction-from-a-single-image-using-a-multi-scale-deep-network.pdf},
year = {2014}
}
@article{Bajracharya2012,
annote = {Boston Dynamics LS3},
author = {Bajracharya, Max and Ma, Jeremy and Howard, Andrew and Matthies, Larry},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Bajracharya et al. - 2012 - Real-time 3D Stereo Mapping in Complex Dynamic Environments.pdf:pdf},
journal = {International Conference on Robotics and Automation-Semantic Mapping, Perception, and Exploration (SPME) Workshop},
keywords = {delete},
mendeley-tags = {delete},
number = {c},
title = {{Real-time 3D Stereo Mapping in Complex Dynamic Environments}},
year = {2012}
}
@article{Tomatis2003,
abstract = {In this paper the metric and topological paradigms are integrated in a hybrid system for both localization and map building. A global topological map connects local metric maps, allowing a compact environment model, which does not require global metric consistency and permits both precision and robustness. Furthermore, the approach handles loops in the environment during automatic mapping by means of the information of the multimodal topological localization. The system uses a 360?? laser scanner to extract corners and openings for the topological approach and lines for the metric method. This hybrid approach has been tested in a 50 m ?? 25 m portion of the institute building with the fully autonomous robot Donald Duck. Experiments are of four types: maps created by a complete exploration of the environment are compared to estimate their quality; test missions are randomly generated in order to evaluate the efficiency of the approach for both the localization and relocation; the fourth type of experiments shows the practicability of the approach for closing the loop. ?? 2003 Elsevier Science B.V. All rights reserved.},
author = {Tomatis, Nicola and Nourbakhsh, Illah and Siegwart, Roland},
doi = {10.1016/S0921-8890(03)00006-X},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Tomatis, Nourbakhsh, Siegwart - 2003 - Hybrid simultaneous localization and map building A natural integration of topological and metric.pdf:pdf},
isbn = {0921-8890},
issn = {09218890},
journal = {Robotics and Autonomous Systems},
keywords = {Hybrid (metric-topological),Kalman filtering,Mobile robot navigation,POMDP},
number = {1},
pages = {3--14},
title = {{Hybrid simultaneous localization and map building: A natural integration of topological and metric}},
volume = {44},
year = {2003}
}
@article{Ruffier2003,
abstract = {In the framework of our research on biologically inspired microrobotics, we have developed a visually based autopilot for micro air vehicles (MAV), which we have called OCTAVE (optical altitude control system for autonomous vehicles). Here, we show the feasibility of a joint altitude and speed control system based on a low complexity optronic velocity sensor that estimates the optic flow in the downward direction. This velocity sensor draws on electrophysiological findings of on the fly elementary motion detectors (EMDs) obtained at our laboratory. We built an elementary, 100-gram tethered helicopter system that carries out terrain following above a randomly textured ground. The overall processing system is light enough to be mounted on-board MAVs with an avionic payload of only some grams.},
author = {Ruffier, F. and Viollet, S. and Amic, S. and Franceschini, N.},
doi = {10.1109/ISCAS.2003.1205152},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Ruffier et al. - 2003 - Bio-inspired optical flow circuits for the visual guidance of micro air vehicles.pdf:pdf},
isbn = {0-7803-7761-3},
journal = {Proceedings of the 2003 International Symposium on Circuits and Systems, 2003. ISCAS '03.},
keywords = {optical flow},
mendeley-tags = {optical flow},
pages = {846--849},
title = {{Bio-inspired optical flow circuits for the visual guidance of micro air vehicles}},
volume = {3},
year = {2003}
}
@article{Artieda2009,
abstract = {The aim of the paper is to present, test and discuss the implementation of Visual SLAM techniques to images taken from Unmanned Aerial Vehicles (UAVs) outdoors, in partially structured environments. Every issue of the whole process is discussed in order to obtain more accurate localization and mapping from UAVs flights. Firstly, the issues related to the visual features of objects in the scene, their distance to the UAV, and the related image acquisition system and their calibration are evaluated for improving the whole process. Other important, considered issues are related to the image processing techniques, such as interest point detection, the matching procedure and the scaling factor. The whole system has been tested using the COLIBRI mini UAV in partially structured environments. The results that have been obtained for localization, tested against the GPS information of the flights, show that Visual SLAM delivers reliable localization and mapping that makes it suitable for some outdoors applications when flying UAVs.},
author = {Artieda, Jorge and Sebastian, Jos{\'{e}} M. and Campoy, Pascual and Correa, Juan F. and Mondrag{\'{o}}n, Iv{\'{a}}n F. and Mart{\'{i}}nez, Carol and Olivares, Miguel},
doi = {10.1007/s10846-008-9304-8},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Artieda et al. - 2009 - Visual 3-D SLAM from UAVs.pdf:pdf},
isbn = {0921-0296},
issn = {09210296},
journal = {Journal of Intelligent and Robotic Systems: Theory and Applications},
keywords = {3D SLAM,Computer vision,Unmanned aerial vehicles (UAV),Visual SLAM,monocular,slam,survey},
mendeley-tags = {monocular,slam,survey},
number = {4-5},
pages = {299--321},
title = {{Visual 3-D SLAM from UAVs}},
volume = {55},
year = {2009}
}
@article{Olah2017,
author = {Olah, Chris and Mordvintsev, Alexander and Schubert, Ludwig},
doi = {10.23915/distill.00007},
journal = {Distill},
number = {https://distill.pub/2017/feature-visualization},
title = {{Feature Visualization}},
url = {https://distill.pub/2017/feature-visualization},
year = {2017}
}
@article{Li2013,
author = {Li, Mingyang and Mourikis, Anastasios I.},
doi = {10.1177/0278364913481251},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Li, Mourikis - 2013 - High-precision, consistent EKF-based visual-inertial odometry.pdf:pdf},
journal = {The International Journal of Robotics Research},
keywords = {extended kalman filter consistency,vision-aided inertial navigation,visual-inertial odometry,visual-inertial slam},
number = {6},
pages = {690--711},
title = {{High-precision, consistent EKF-based visual-inertial odometry}},
volume = {32},
year = {2013}
}
@article{VanBeers1999,
author = {van Beers, Robert J. and Sittig, Anne C. and van der Gon, Jan J. Denier},
doi = {10.1152/jn.1999.81.3.1355},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/van Beers, Sittig, Gon - 1999 - Integration of Proprioceptive and Visual Position-Information An Experimentally Supported Model.pdf:pdf},
issn = {0022-3077},
journal = {Journal of Neurophysiology},
month = {mar},
number = {3},
pages = {1355--1364},
title = {{Integration of Proprioceptive and Visual Position-Information: An Experimentally Supported Model}},
url = {http://www.physiology.org/doi/10.1152/jn.1999.81.3.1355},
volume = {81},
year = {1999}
}
@article{Ross2012,
abstract = {Autonomous navigation for large Unmanned Aerial Vehicles (UAVs) is fairly straight-forward, as expensive sensors and monitoring devices can be employed. In contrast, obstacle avoidance remains a challenging task for Micro Aerial Vehicles (MAVs) which operate at low altitude in cluttered environments. Unlike large vehicles, MAVs can only carry very light sensors, such as cameras, making autonomous navigation through obstacles much more challenging. In this paper, we describe a system that navigates a small quadrotor helicopter autonomously at low altitude through natural forest environments. Using only a single cheap camera to perceive the environment, we are able to maintain a constant velocity of up to 1.5m/s. Given a small set of human pilot demonstrations, we use recent state-of-the-art imitation learning techniques to train a controller that can avoid trees by adapting the MAVs heading. We demonstrate the performance of our system in a more controlled environment indoors, and in real natural forest environments outdoors.},
archivePrefix = {arXiv},
arxivId = {1211.1690},
author = {Ross, Stephane and Melik-Barkhudarov, Narek and Shankar, Kumar Shaurya and Wendel, Andreas and Dey, Debadeepta and Bagnell, J. Andrew and Hebert, Martial},
doi = {10.1002/2014GB005021},
eprint = {1211.1690},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Ross et al. - 2012 - Learning Monocular Reactive UAV Control in Cluttered Natural Environments.pdf:pdf},
isbn = {9781617796029},
journal = {Bulletin of the Astronomical Institutes of Czecheslovakia},
keywords = {experiment,imitation learning,low complexity,monocular,obstacle avoidance,obstacle detection,optical flow,reactive obstacle avoidance,scenario: forest},
mendeley-tags = {experiment,imitation learning,low complexity,monocular,obstacle avoidance,obstacle detection,optical flow,reactive obstacle avoidance,scenario: forest},
month = {nov},
number = {6},
title = {{Learning Monocular Reactive UAV Control in Cluttered Natural Environments}},
url = {http://arxiv.org/abs/1211.1690},
volume = {29},
year = {2012}
}
@article{Bradley,
abstract = {— This document presents an overview of tech-niques for an important class of manipulation problems in mobile robotics, odometry calibration and error modeling for wheeled vehicles.},
author = {Bradley, David M},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Bradley - Unknown - Odometry Calibration and Error Modeling.pdf:pdf},
title = {{Odometry: Calibration and Error Modeling}}
}
@inproceedings{Razavian2014,
author = {Razavian, Ali Sharif and Azizpour, Hossein and Sullivan, Josephine and Carlsson, Stefan},
booktitle = {CVPR2014 Workshop},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Razavian et al. - 2014 - CNN Features off-the-shelf an Astounding Baseline for Recognition.pdf:pdf},
title = {{CNN Features off-the-shelf: an Astounding Baseline for Recognition}},
year = {2014}
}
@article{Stentz1994,
abstract = {The task of planning trajectories for a mobile robot has received considerable attention in the research literature. Most of the work assumes the robot has a complete and accurate model of its environment before it begins to move; less attention has been paid to the problem of partially known environments. This situation occurs for an exploratory robot or one that must move to a goal location without the benefit of a floorplan or terrain map. Existing approaches plan an initial path based on known information and then modify the plan locally or replan the entire path as the robot discovers obstacles with its sensors, sacrificing optimality or computational efficiency respectively. This paper introduces a new algorithm, D*, capable of planning paths in unknown, partially known, and changing environments in an efficient, optimal, and complete manner},
author = {Stentz, A.},
doi = {10.1109/ROBOT.1994.351061},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Stentz - 1994 - Optimal and efficient path planning for partially-known environments.pdf:pdf},
isbn = {0-8186-5330-2},
issn = {10504729},
journal = {Proceedings of the 1994 IEEE International Conference on Robotics and Automation},
keywords = {Cost function,D* algorithm,Mobile robots,Motion planning,Orbital robotics,Path planning,Regions,Robot sensing systems,State estimation,State-space methods,Trajectory,changing environments,computational complexity,efficient path planning,exploratory robot,mobile robot,mobile robots,motion planning,optimal path planning,optimisation,partially-known environments,path planning,planning trajectories,unknown environments},
mendeley-tags = {motion planning},
pages = {3310--3317},
pmid = {18344057},
title = {{Optimal and efficient path planning for partially-known environments}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=351061},
year = {1994}
}
@article{Minguez2005,
abstract = {This paper addresses the obstacle avoidance problem in difficult scenarios that usually are dense, complex and cluttered. The proposal is a method called the obstacle-restriction. At each iteration of the control cycle, this method addresses the obstacle avoidance in two steps. First there is procedure to compute instantaneous subgoals in the obstacle structure (obtained by the sensors). The second step associates a motion restriction to each obstacle, which are managed next to compute the most promising motion direction. The advantage of this technique is that it avoids common limitations of previous obstacle avoidance methods, improving their navigation performance in difficult scenarios. Furthermore, we obtain similar results to the recent methods that achieve navigation in troublesome scenarios. However, the new method improves their behavior in open spaces. The performance of this method is illustrated with experimental results obtained with a robotic wheelchair vehicle.},
author = {Minguez, Javier},
doi = {10.1109/IROS.2005.1545546},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Minguez - 2005 - The Obstacle-Restriction Method (ORM) for robot obstacle avoidance in difficult environments.pdf:pdf},
isbn = {0780389123},
journal = {2005 IEEE/RSJ International Conference on Intelligent Robots and Systems, IROS},
number = {Section III},
pages = {3706--3712},
title = {{The Obstacle-Restriction Method (ORM) for robot obstacle avoidance in difficult environments}},
year = {2005}
}
@article{Matas2004,
author = {Matas, Jiri and Chum, Ondrej and Urban, Martin and Pajdla, Tom{\'{a}}s},
doi = {10.1016/j.imavis.2004.02.006},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Matas et al. - 2004 - Robust Wide Baseline Stereo from Maximally Stable Extremal Regions.pdf:pdf},
journal = {Image and vision computing},
number = {10},
pages = {761--767},
title = {{Robust Wide Baseline Stereo from Maximally Stable Extremal Regions}},
volume = {22},
year = {2004}
}
@article{Alcantarilla2010,
abstract = {One of the main drawbacks of standard visual EKF-SLAM techniques is the assumption of a general camera motion model. Usually this motion model has been implemented in the literature as a constant linear and angular velocity model. Because of this, most approaches cannot deal with sudden camera movements, causing them to lose accurate camera pose and leading to a corrupted 3D scene map. In this work we propose increasing the robustness of EKF-SLAM techniques by replacing this general motion model with a visual odometry prior, which provides a real-time relative pose prior by tracking many hundreds of features from frame to frame. We perform fast pose estimation using the two-stage RANSAC-based approach from [1]: a two-point algorithm for rotation followed by a one-point algorithm for translation. Then we integrate the estimated relative pose into the prediction step of the EKF. In the measurement update step, we only incorporate a much smaller number of landmarks into the 3D map to maintain real-time operation. Incorporating the visual odometry prior in the EKF process yields better and more robust localization and mapping results when compared to the constant linear and angular velocity model case. Our experimental results, using a handheld stereo camera as the only sensor, clearly show the benefits of our method against the standard constant velocity model.},
author = {Alcantarilla, Pablo F. and Bergasa, Luis M. and Dellaert, Frank},
doi = {10.1109/ROBOT.2010.5509272},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Alcantarilla, Bergasa, Dellaert - 2010 - Visual odometry priors for robust EKF-SLAM.pdf:pdf},
isbn = {9781424450381},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
pages = {3501--3506},
title = {{Visual odometry priors for robust EKF-SLAM}},
year = {2010}
}
@article{Garratt2012,
abstract = {The problem of developing a reliable system for sensing and controlling the hover of a Micro Air Vehicle (MAV) using visual snapshots is considered. The current problem is part of a larger project, which is developing an autonomous MAV, controlled by vision only information. A new algorithm is proposed that uses a stored image of the ground, a snapshot taken of the ground directly under the MAV, as a visual anchor point. The absolute translation of the aircraft and its velocity are then calculated by comparing the subsequent frames with the stored image and fed into the position controller. In order to increase the performance, several issues, such as effects of scale uncertainty on the closed loop stability of the platform are investigated. For controller design and testing purposes, we analytically derive a complete model of a small size helicopter with no stabilizing bar (flybar). The simulation results for 2D and 3D snapshots confirm the effectiveness of the proposed algorithm.},
author = {Garratt, Matthew a. and Lambert, Andrew J. and Teimoori, Hamid},
doi = {10.1007/s10514-012-9310-3},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Garratt, Lambert, Teimoori - 2012 - Design of a 3D snapshot based visual flight control system using a single camera in hover.pdf:pdf},
issn = {0929-5593},
journal = {Autonomous Robots},
keywords = {mav,micro air vehicle,visual guidance},
number = {C},
pages = {2005--2008},
title = {{Design of a 3D snapshot based visual flight control system using a single camera in hover}},
year = {2012}
}
@article{Lambrinos2000,
abstract = {The ability to navigate in a complex environment is crucial for both animals and robots. Many animals use a combination of different strategies to return to significant locations in their environment. For example, the desert ant Cataglyphis is able to explore its desert habitat for hundreds of meters while foraging and return back to its nest precisely and on a straight line. The three main strategies that Cataglyphis is using to accomplish this task are path integration, visual piloting and systematic search. In this study, we use a synthetic methodology to gain additional insights into the navigation behavior of Cataglyphis. Inspired by the insect's navigation system we have developed mechanisms for path integration and visual piloting that were successfully employed on the mobile robot Sahabot 2. On the one hand, the results obtained from these experiments provide support for the underlying biological models. On the other hand, by taking the parsimonious navigation strategies of insects as a guideline, computationally cheap navigation methods for mobile robots are derived from the insights gained in the experiments.},
author = {Lambrinos, Dimitrios and M{\"{o}}ller, Ralf and Labhart, Thomas and Pfeifer, Rolf and Wehner, R{\"{u}}diger},
doi = {10.1016/S0921-8890(99)00064-0},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Lambrinos et al. - 2000 - A mobile robot employing insect strategies for navigation.pdf:pdf},
issn = {09218890},
journal = {Robotics and Autonomous Systems},
keywords = {highlight,insect navigation,path integration,polarization vision,robot navigation,visual landmark navigation},
mendeley-tags = {highlight},
number = {1},
pages = {39--64},
title = {{A mobile robot employing insect strategies for navigation}},
volume = {30},
year = {2000}
}
@article{Gaspar2000,
abstract = {Proposes a method for the visual-based navigation of a mobile$\backslash$nrobot in indoor environments, using a single omnidirectional$\backslash$n(catadioptric) camera. The geometry of the catadioptric sensor and the$\backslash$nmethod used to obtain a bird's eye (orthographic) view of the ground$\backslash$nplane are presented. This representation significantly simplifies the$\backslash$nsolution to navigation problems, by eliminating any perspective effects.$\backslash$nThe nature of each navigation task is taken into account when designing$\backslash$nthe required navigation skills and environmental representations. We$\backslash$npropose two main navigation modalities: topological navigation and$\backslash$nvisual path following. Topological navigation is used for traveling long$\backslash$ndistances and does not require knowledge of the exact position of the$\backslash$nrobot but rather, a qualitative position on the topological map. The$\backslash$nnavigation process combines appearance based methods and visual servoing$\backslash$nupon some environmental features. Visual path following is required for$\backslash$nlocal, very precise navigation, e.g., door traversal, docking. The robot$\backslash$nis controlled to follow a prespecified path accurately, by tracking$\backslash$nvisual landmarks in bird's eye views of the ground plane. By clearly$\backslash$nseparating the nature of these navigation tasks, a simple and yet$\backslash$npowerful navigation system is obtained},
author = {Gaspar, Jos{\'{e}} and Winters, Niall and Santos-Victor, Jos{\'{e}}},
doi = {10.1109/70.897802},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Gaspar, Winters, Santos-Victor - 2000 - Vision-based navigation and environmental representations with an omnidirectional camera.pdf:pdf},
isbn = {9781905209101},
issn = {1042296X},
journal = {IEEE Transactions on Robotics and Automation},
keywords = {appearance-based navigation,topological slam},
mendeley-tags = {appearance-based navigation,topological slam},
number = {6},
pages = {890--898},
title = {{Vision-based navigation and environmental representations with an omnidirectional camera}},
volume = {16},
year = {2000}
}
@inproceedings{Stelzer2014,
author = {Stelzer, Annett and Mair, Elmar and Suppa, Michael},
booktitle = {2014 IEEE International Conference on Robotics and Biomimetics (ROBIO 2014)},
doi = {10.1109/ROBIO.2014.7090653},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Stelzer, Mair, Suppa - 2014 - Trail-Map A scalable landmark data structure for biologically inspired range-free navigation.pdf:pdf},
isbn = {978-1-4799-7397-2},
keywords = {highlight},
mendeley-tags = {highlight},
month = {dec},
pages = {2138--2145},
publisher = {IEEE},
title = {{Trail-Map: A scalable landmark data structure for biologically inspired range-free navigation}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7090653},
year = {2014}
}
@inproceedings{Zeiler2013,
abstract = {Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we address both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. We also perform an ablation study to discover the performance contribution from different model layers. This enables us to find model architectures that outperform Krizhevsky $\backslash$etal on the ImageNet classification benchmark. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.},
address = {Cham},
author = {Zeiler, Matthew D. and Fergus, Rob},
booktitle = {Computer Vision -- ECCV 2014},
editor = {Fleet, David and Pajdla, Tomas and Schiele, Bernt and Tuytelaars, Tinne},
file = {:home/tom/Documents/phd/mendeley{\_}pdf//Zeiler, Fergus - 2014 - Visualizing and Understanding Convolutional Networks.pdf:pdf},
pages = {818--833},
publisher = {Springer International Publishing},
title = {{Visualizing and Understanding Convolutional Networks}},
year = {2014}
}
@article{Weiss2011,
abstract = {It is anticipatedthat theMars ScienceLaboratory rover, namedCuriosity,will traverse 10–20 kmon the surface of Mars during its primary mission. In preparation for this traverse, Earth-based tests were performed using Mars weight vehicles. These vehicles were driven over Mars analog bedrock, cohesive soil, and cohesionless sand at various slopes. Vehicle slip was characterized on each of these terrains versus slope for direct upslope driving. Results show that slopes up to 22 degrees are traversable on smooth bedrock and that slopes up to 28 degrees are traversable on some cohesive soils. In cohesionless sand, results show a sharp transition between moderate slip on 10 degree slopes and vehicle embedding at 17 degrees. For cohesionless sand, data are also presented showing the relationship between vehicle slip and wheel sinkage. Side by side testing of the Mars Exploration Rover test vehicle and the Mars Science Laboratory test vehicle show how increased wheel diameter leads to better slope climbing ability in sand for vehicles with nearly identical ground pressure. Lastly, preliminary data from Curiosity's initial driving on Mars are presented and compared to the Earth-based testing, showing good agreement for the driving done during the first 250 Martian days.},
archivePrefix = {arXiv},
arxivId = {10.1.1.91.5767},
author = {Weiss, Stephan and Scaramuzza, Davide and Siegwart, Roland},
doi = {10.1002/rob.20412},
eprint = {10.1.1.91.5767},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Weiss, Scaramuzza, Siegwart - 2011 - Monocular-SLAM-based navigation for autonomous micro helicopters in GPS-denied environments.pdf:pdf},
isbn = {9783902661623},
issn = {15564959},
journal = {Journal of Field Robotics},
keywords = {Backstepping,Non-linear control,Path-following},
month = {nov},
number = {6},
pages = {854--874},
pmid = {22164016},
title = {{Monocular-SLAM-based navigation for autonomous micro helicopters in GPS-denied environments}},
url = {http://doi.wiley.com/10.1002/rob.20412},
volume = {28},
year = {2011}
}
@article{Sanfourche2012,
author = {Sanfourche, M and Delaune, J and {Le Besnerais}, G and {De Plinval}, H and Israel, J and Cornic, P. and Treil, A and Watanabe, Y and Plyer, A},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Sanfourche et al. - 2012 - Perception for UAV Vision-Based Navigation and Environment Modeling.pdf:pdf},
journal = {AerospaceLab},
number = {4},
pages = {1--19},
title = {{Perception for UAV: Vision-Based Navigation and Environment Modeling.}},
year = {2012}
}
@article{DeWagter2014,
abstract = {Autonomous flight of Flapping Wing Micro Air Vehicles (FWMAVs) is a major challenge in the field of robotics, due to their light weight and the flapping-induced body motions. In this article, we present the first FWMAV with onboard vision processing for autonomous flight in generic environments. In particular, we introduce the DelFly ‘Explorer', a 20-gram FWMAV equipped with a 0.98-gram autopilot and a 4.0-gram onboard stereo vision system. We explain the design choices that permit carrying the extended payload, while retaining the DelFly's hover capabilities. In addition, we introduce a novel stereo vision algorithm, LongSeq, designed specifically to cope with the flapping motion and the desire to attain a computational effort tuned to the frame rate. The onboard stereo vision system is illustrated in the context of an obstacle avoidance task in an environment with sparse obstacles.},
author = {{De Wagter}, C. and Tijmons, S. and Remes, B. D W and {De Croon}, G. C H E},
doi = {10.1109/ICRA.2014.6907589},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/De Wagter et al. - 2014 - Autonomous flight of a 20-gram Flapping Wing MAV with a 4-gram onboard stereo vision system.pdf:pdf},
isbn = {9781479936847},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
pages = {4982--4987},
title = {{Autonomous flight of a 20-gram Flapping Wing MAV with a 4-gram onboard stereo vision system}},
year = {2014}
}
@inproceedings{Wang2005,
annote = {combinatie color histogram en bag of words},
author = {Wang, Junqui and Zha, Hongbin and Cipolla, Roberto},
booktitle = {IEEE International Conference on Image Processing 2005},
doi = {10.1109/ICIP.2005.1530627},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Wang, Zha, Cipolla - 2005 - Combining interest points and edges for content-based image retrieval.pdf:pdf},
isbn = {0-7803-9134-9},
keywords = {bag-of-words},
mendeley-tags = {bag-of-words},
pages = {III--1256},
publisher = {IEEE},
title = {{Combining interest points and edges for content-based image retrieval}},
url = {http://ieeexplore.ieee.org/document/1530627/},
year = {2005}
}
@article{Labayrade,
abstract = {— Many roads are not totaly planar and often present hills and valleys because of environment topogra-phy. Nevertheless, the majority of existing techniques for road obstacle detection using stereovision assumes that the road is planar. This can cause several issues : imprecision as regards the real position of obstacles as well as false obstacle detection or obstacle detection failures. In order to increase the reliability of the obstacle detection process, this paper proposes an original, fast and robust method for detecting the obstacles without using the flat-earth geometry assump-tion; this method is able to cope with uphill and downhill gradients as well as dynamic pitching of the vehicle. Our ap-proach is based on the construction and investigation of the " v-disparity " 1 image which provides a good representation of the geometric content of the road scene. The advantage of this image is that it provides semi-global matching and is able to perform robust obstacle detection even in the case of partial occlusion or errors committed during the matching process. Furthermore, this detection is performed without any explicit extraction of coherent structures such as road edges or lane-markings in the stereo image pair. This paper begins by explaining the construction of the " v-disparity " image and by describing its main properties. On the basis of this image, we then describe a robust method for road obstacle detection in the context of flat and non flat road geometry, including estimation of the relative height and pitching of the stereo sensor with respect to the road surface. The longitudinal profile of the road is estimated and the ob-jects located above the road surface are then extracted as potential obstacles; subsequently, the accurate detection of road obstacles, in particular the position of tyre-road con-tact points is computed in a precise manner. The whole process is performed at frame rate with a current-day PC. Our experimental findings and comparisons with the results obtained using a flat geometry hypothesis show the benefits of our approach. Future work will be concerned with the construction of a 3D road model and the test of the system for Stop'n'Go applications.},
author = {Labayrade, Raphael and Aubert, Didier and Tarel, Jean-Philippe},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Labayrade, Aubert, Tarel - Unknown - Real Time Obstacle Detection in Stereovision on Non Flat Road Geometry Through V-disparity Repres.pdf:pdf},
keywords = {image space,low complexity,obstacle detection,stereo vision,uv disparity},
mendeley-tags = {image space,low complexity,obstacle detection,stereo vision,uv disparity},
title = {{Real Time Obstacle Detection in Stereovision on Non Flat Road Geometry Through " V-disparity " Representation}}
}
@inproceedings{Matsumoto1996,
abstract = {Previous work in vision-based mobile robotics have lacked models of the route which can be utilized for (l)locaEization, (2)steering angle determination, and (3)obstacle detection, simultaneously. In this paper, we propose a new visual rep-resentation of the route, the " View-Sequenced Route Rep-resentation (VSRR). " The VSRR is a non-metrical model of the route, which contains a sequence of front view images along a route memorized in the recording run. In the au-tonomous run, the three types of recognition described above are achieved in real-time b y matching between the current view image and the memorized view sequence using a cor-relation technique. W e also developed a n easy procedure for acquiring V S R R s , and a quick control procedure using VS-RRs. VSRR ' s are especially useful for representing routes i n corridors. Results of autonomous navigation using a two-wheeled robot in a real corridor are also presented. 1 Introduction A model of the route for a mobile robot should be described in a way that makes the comparison of stored information and the current sensory information easy. This enables recognition in a small fixed amount of time possible even in complicated environments. Conventional research on mobile robots have fo-cused on deliberative approaches using geometric models(e.g.[l]). As extracting geometric information of the environment from the camera is both time-consuming and intolerant of noise, these past few years a lot researchers used methods which build models of the route using real images. Meng[2] and Pomerleau[3] applied neural networks, which learn the relation between input view image and steering angle, to drive systems for both indoor and outdoor use. Crespi[4] applied memory-based ap-proach to indoor navigation, which also learns the re-lation between the input view image and the lateral position in the corridor. The data learned in the train-ing phases are models of the road, but they are only used for following and not for identifying the robot's position along a route. Panoramic representation[5], which is obtained b:y scanning side views along a route, can be used for lo-calization. However, since the camera is facing side-ways, guidance and obstacle detection is not possible. In this paper, we propose a new model of the route, called the " View-Sequenced Route Representa-tion (VSRR), " which is especially useful for corridor environmerits. This representation is obtained by memorizing front view images in reduced resolution along a route dur-ing the recording run. The stored information is rich enough for recognition in the autonomous run. Meth-ods of (l)localization, (2)steering angle determination and (3)obslacle detection using VSRR are presented. At the end, experimental results of autonomous nav-igation of EL two-wheeled robot in a real corridor are shown.},
author = {Matsumoto, Yoshio and Inaba, Masayuki and Inoue, Hirochika},
booktitle = {Proceedings of IEEE International Conference on Robotics and Automation},
doi = {10.1109/ROBOT.1996.503577},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Matsumoto, Inaba, Inoue - 1996 - Visual navigation using view-sequenced route representation.pdf:pdf},
isbn = {0-7803-2988-0},
keywords = {appearance-based navigation,scene familiarity},
mendeley-tags = {appearance-based navigation,scene familiarity},
pages = {83--88},
publisher = {IEEE},
title = {{Visual navigation using view-sequenced route representation}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=503577},
volume = {1},
year = {1996}
}
@inproceedings{Saha2014,
author = {Saha, Suman and Natraj, Ashutosh and Waharte, Sonia},
booktitle = {IEEE International Conference on Aerospace Electronics and Remote Sensing Technology (ICARES) A},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Saha, Natraj, Waharte - 2014 - A Real-Time Monocular Vision-based Frontal Obstacle Detection and Avoidance for Low Cost UAVs in GPS Deni.pdf:pdf},
isbn = {9781479961887},
pages = {189--195},
title = {{A Real-Time Monocular Vision-based Frontal Obstacle Detection and Avoidance for Low Cost UAVs in GPS Denied Environment}},
year = {2014}
}
@article{Schmid2014,
author = {Schmid, Korbinian and Lutz, Philipp and Tomi{\'{c}}, Teodor and Mair, Elmar and Hirschm{\"{u}}ller, Heiko},
doi = {10.1002/rob.21506},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Schmid et al. - 2014 - Autonomous Vision-based Micro Air Vehicle for Indoor and Outdoor Navigation.pdf:pdf},
journal = {Journal of Field Robotics},
number = {4},
pages = {537--570},
title = {{Autonomous Vision-based Micro Air Vehicle for Indoor and Outdoor Navigation}},
volume = {31},
year = {2014}
}
@article{Nguyen2006,
abstract = {Today, lightweight SLAM algorithms are needed in many embedded robotic systems. In this paper the orthogonal SLAM (OrthoSLAM ) algorithm is presented and empirically validated. The algorithm has constant time complexity in the state estimation and is capable to run real-time. The main contribution resides in the idea of reducing the complexity by means of an assumption on the environment. This is done by mapping only lines that are parallel or perpendicular to each other which represent the main structure of most indoor environments. The combination of this assumption with a Kalman filter and a relative map approach is able to map our laboratory hallway with the size of 80 m times 50 m and a trajectory of more than 500 m. The precision of the resulting map is similar to the measurements done by hand which are used as the ground-truth},
author = {Nguyen, Viet and Harati, Ahad and Martinelli, Agostino and Siegwart, Roland and Tomatis, Nicola},
doi = {10.1109/IROS.2006.282527},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Nguyen et al. - 2006 - Orthogonal SLAM A step toward lightweight indoor autonomous navigation.pdf:pdf},
isbn = {142440259X},
journal = {IEEE International Conference on Intelligent Robots and Systems},
keywords = {Indoor environment,Lightweight SLAM,Mobile robotics,Orthogonality,highlight,low complexity,slam},
mendeley-tags = {highlight,low complexity,slam},
pages = {5007--5012},
title = {{Orthogonal SLAM: A step toward lightweight indoor autonomous navigation}},
year = {2006}
}
@article{Muller2009,
abstract = {Weighted averaging is said to be optimal when the weights assigned to the cues minimize the variance of the final estimate. Since the variance of this optimal percept only depends on the variances of the individual cues, irrespective of their values, judgments about a cue conflict stimulus should have the same variance as ones about a cue consistent stimulus. We tested this counter-intuitive prediction with a slant matching experiment using monocular and binocular slant cues. We found that the slant was indeed matched with about the same variance when the cues indicated slants that differed by 15° as when they indicated the same slant. {\textcopyright} 2008 Elsevier Ltd. All rights reserved.},
author = {Muller, Chris M.P. and Brenner, Eli and Smeets, Jeroen B.J.},
doi = {10.1016/j.visres.2008.10.006},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Muller, Brenner, Smeets - 2009 - Testing a counter-intuitive prediction of optimal cue combination.pdf:pdf},
isbn = {0042-6989},
issn = {00426989},
journal = {Vision Research},
keywords = {Binocular,Monocular,Optimal cue combination,Slant},
number = {1},
pages = {134--139},
pmid = {18983869},
publisher = {Elsevier Ltd},
title = {{Testing a counter-intuitive prediction of optimal cue combination}},
url = {http://dx.doi.org/10.1016/j.visres.2008.10.006},
volume = {49},
year = {2009}
}
@article{Garcia-Fidalgo2015,
abstract = {• A comprehensive survey on vision-based topological mapping and localization methods. • Classification according to image descriptions: global, local, BoW and combinations. • Advantages and disadvantages of each approach are discussed. a b s t r a c t Topological maps model the environment as a graph, where nodes are distinctive places of the environ-ment and edges indicate topological relationships between them. They represent an interesting alter-native to the classic metric maps, due to their simplicity and storage needs, what has made topological mapping and localization an active research area. The different solutions that have been proposed during years have been designed around several kinds of sensors. However, in the last decades, vision approaches have emerged because of the technology improvements and the amount of useful information that a cam-era can provide. In this paper, we review the main solutions presented in the last fifteen years, and classify them in accordance to the kind of image descriptor employed. Advantages and disadvantages of each ap-proach are thoroughly reviewed and discussed.},
annote = {Focus on loop closure detection.},
author = {Garcia-Fidalgo, Emilio and Ortiz, Alberto},
doi = {10.1016/j.robot.2014.11.009},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Garcia-Fidalgo, Ortiz - 2015 - Vision-based topological mapping and localization methods A survey.pdf:pdf},
issn = {09218890},
journal = {Robotics and Autonomous Systems},
keywords = {highlight,slam,survey},
mendeley-tags = {highlight,slam,survey},
month = {feb},
pages = {1--20},
title = {{Vision-based topological mapping and localization methods: A survey}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0921889014002619},
volume = {64},
year = {2015}
}
@inproceedings{Nourani-Vatani2010,
abstract = {— A method for detecting changes in the environ-ment using only vision sensors is presented. We demonstrate that optical flow can be used to detect these changes at key locations in outdoor scenarios in difficult and varying lighting conditions. These key locations are used as nodes in a topological mapping and localization framework. To close the loop we employ a bag-of-words methodology. We show that bag-of-words methods can be used in real-time on a standard computer to detect loop closures in sparse topological maps. Experimental results from field trials using our quad-rotor UAV demonstrate the capability of the proposed scene change detection method.},
author = {Nourani-Vatani, Navid and Pradalier, Cedric},
booktitle = {2010 IEEE/RSJ International Conference on Intelligent Robots and Systems},
doi = {10.1109/IROS.2010.5652556},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Nourani-Vatani, Pradalier - 2010 - Scene change detection for vision-based topological mapping and localization.pdf:pdf},
isbn = {978-1-4244-6674-0},
keywords = {slam,topological slam},
mendeley-tags = {slam,topological slam},
month = {oct},
pages = {3792--3797},
publisher = {IEEE},
title = {{Scene change detection for vision-based topological mapping and localization}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5652556},
year = {2010}
}
@inproceedings{Denuelle2015a,
abstract = {—This paper describes a novel view-based method using panoramic images to perform local homing in outdoor environments. This holistic algorithm makes uses of difference images (in relation to a reference snapshot) to build an image reference frame centred at the home position. The currently experienced view at any local position is then projected onto this reference frame to determine the image coordinates and the homing vector. We present here results obtained in a simulated environment as well as from static outdoor tests. The biologically inspired algorithm described in this study is a feasible alternative to the local homing schemes that strongly rely on odometry or landmark extraction, making it therefore well suited for implementation onboard unmanned aerial vehicles.},
author = {Denuelle, Aymeric and Thurrowgood, Saul and Kendoul, Farid and Srinivasan, Mandyam V},
booktitle = {2015 6th International Conference on Automation, Robotics and Applications (ICARA)},
doi = {10.1109/ICARA.2015.7081189},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Denuelle et al. - 2015 - A view-based method for local homing of unmanned rotorcraft.pdf:pdf},
isbn = {978-1-4799-6466-6},
keywords = {highlight,visual homing},
mendeley-tags = {highlight,visual homing},
month = {feb},
pages = {443--449},
publisher = {IEEE},
title = {{A view-based method for local homing of unmanned rotorcraft}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7081189},
year = {2015}
}
@article{Neumann2002,
abstract = {Most flying insects extract information about their spatial orientation and self-motion from visual cues such as global patterns of light intensity or optic flow. We present an insect-inspired neuronal filter model and show how optimal receptive fields for the detec- tion of flight-relevant input patterns can be derived di- rectly from the local receptor signals during typical flight behavior. Using a least squares principle, the re- ceptive fields are optimally adapted to all behaviorally relevant, invariant properties of the agent and the en- vironment. In closed-loop simulations in a highly re- alistic virtual environment we show that four indepen- dent, purely reactive mechanisms based on optimized receptive fields for attitude control, course stabiliza- tion, obstacle avoidance and altitude control, are suf- ficient for a fully autonomous and robust flight stabi- lization with all six degrees of freedom.},
author = {Neumann, Titus R. and B{\"{u}}lthoff, Heinrich H.},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Neumann, B{\"{u}}lthoff - 2002 - Behavior-oriented vision for biomimetic flight control.pdf:pdf},
journal = {Proceedings of the EPSRC/BBSRC {\ldots}},
keywords = {low complexity,optical flow},
mendeley-tags = {low complexity,optical flow},
number = {August},
pages = {196--203},
title = {{Behavior-oriented vision for biomimetic flight control}},
url = {http://www.kyb.mpg.de/fileadmin/user{\_}upload/files/publications/pdfs/pdf1825.pdf},
volume = {203},
year = {2002}
}
@article{Godard2017,
author = {Godard, Cl{\'{e}}ment and {Mac Aodha}, Oisin and Brostow, Gabriel J.},
file = {:home/tom/Documents/phd/mendeley{\_}pdf//Godard, Mac Aodha, Brostow - 2017 - Unsupervised Monocular Depth Estimation with Left-Right Consistency.pdf:pdf},
journal = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
title = {{Unsupervised Monocular Depth Estimation with Left-Right Consistency}},
year = {2017}
}
@article{Ranganathan2002,
abstract = {This article describes a navigation system for a mobile robot which must execute motions in a building; the robot is equipped with a belt of ultrasonic sensors and with a camera. The environment is represented by a topological model based on a Generalized Voronoi Graph (GVG) and by a set of visual landmarks. Typically, the topological graph describes the free space in which the robot must navigate; a node is associated to an intersection between corridors, or to a crossing towards another topological area (an open space: rooms, hallways, ...); an edge corresponds to a corridor or to a path in an open space. Landmarks correspond to static, rectangular and planar objects (e.g. doors, windows, posters, ...) located on the walls. The landmarks are only located with respect to the topological graph: some of them are associated to nodes, other to edges. The paper is focused on the preliminary exploration task, i.e. the incremental construction of the topological model. The navigation task is based on this model: the robot self-localization is only expressed with respect to the graph. ?? 2002 Published by Elsevier Science B.V.},
author = {Ranganathan, P. and Hayet, J. B. and Devy, M. and Hutchinson, S. and Lerasle, F.},
doi = {10.1016/S0921-8890(02)00276-2},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Ranganathan et al. - 2002 - Topological navigation and qualitative localization for indoor environment using multi-sensory perception.pdf:pdf},
issn = {09218890},
journal = {Robotics and Autonomous Systems},
keywords = {Generalized Voronoi Graph,Topological navigation,Visual landmarks,slam,topological slam},
mendeley-tags = {slam,topological slam},
number = {2-3},
pages = {137--144},
title = {{Topological navigation and qualitative localization for indoor environment using multi-sensory perception}},
volume = {41},
year = {2002}
}
@article{Welchman2016,
abstract = {Human perception is remarkably flexible: We experience vivid three-dimensional (3D) structure under diverse conditions, from the seemingly random magic-eye stereograms to the aesthetically beautiful, but obviously flat, canvases of the Old Masters. How does the brain achieve this apparently effortless robustness? Using brain imaging we are beginning to discover how different parts of the visual cortex support 3D perception by tracing different computations in the dorsal and ventral pathways. This review concentrates on studies of binocular disparity and its combination with other depth cues. This work suggests that the dorsal visual cortex is strongly engaged by 3D information and is involved in integrating signals to represent the structure of viewed surfaces. The ventral cortex may store representations of object configurations and the features required for task performance. These differences can be broadly understood in terms of the different computational demands of reducing estimator variance versus increasing the separation between exemplars.},
author = {Welchman, Andrew E.},
doi = {10.1146/annurev-vision-111815-114605},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Welchman - 2016 - The Human Brain in Depth How We See in 3D.pdf:pdf},
issn = {2374-4642},
journal = {Annual Review of Vision Science},
keywords = {binocular vision,cue fusion,depth cues,human brain imaging,stereopsis},
number = {1},
pages = {345--376},
title = {{The Human Brain in Depth: How We See in 3D}},
volume = {2},
year = {2016}
}
@article{Ulrich1998,
author = {Ulrich, I. and Borenstein, J.},
doi = {10.1109/ROBOT.1998.677362},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Ulrich, Borenstein - 1998 - VFH reliable obstacle avoidance for fast mobile robots.pdf:pdf},
isbn = {0-7803-4300-X},
issn = {1050-4729},
journal = {Proceedings. 1998 IEEE International Conference on Robotics and Automation (Cat. No.98CH36146)},
keywords = {obstacle avoidance,polar map,reactive obstacle avoidance},
mendeley-tags = {obstacle avoidance,polar map,reactive obstacle avoidance},
number = {May},
pages = {1572--1577},
pmid = {677362},
title = {{VFH+: reliable obstacle avoidance for fast mobile robots}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=677362},
volume = {2},
year = {1998}
}
@article{Baumann1997,
abstract = {An important task of vision is the segregation of figure and ground in situations of spatial occlusion. Psychophysical evidence suggests that the depth order at contours is defined early in visual processing. We have analysed this process in the visual cortex of the alert monkey. The animals were trained on a visual fixation task which reinforced foveal viewing. During periods of active visual fixation, we recorded the responses of single neurons in striate and prestriate cortex (areas V1, V2, and V3/V3A). The stimuli mimicked situations of spatial occlusion, usually a uniform light (or dark) rectangle overlaying a grating texture of opposite contrast. The direction of figure and ground at the borders of these rectangles was defined by the direction of the terminating grating lines (occlusion cues). Neuronal responses were analysed with respect to figure-ground direction and contrast polarity at such contours. Striate neurons often failed to respond to such stimuli, or were selective for contrast polarity; others were non-selective. Some neurons preferred a certain combination of figure-ground direction and contrast polarity. These neurons were rare both in striate and prestriate cortex. The majority of neurons signalled figure-ground direction independent of contrast polarity. These neurons were only found in prestriate cortex. We explain these responses in terms of a model which also explains neuronal signals of illusory contours. These results suggest that occlusion cues are used at an early level of processing to segregate figure and ground at contours.},
author = {Baumann, R. and van der Zwan, R. and Peterhans, E.},
doi = {10.1111/j.1460-9568.1997.tb01484.x},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Baumann, van der Zwan, Peterhans - 1997 - Figure-ground segregation at contours A neural mechanism in the visual cortex of the alert mon.pdf:pdf},
isbn = {0953-816X (Print)},
issn = {0953816X},
journal = {European Journal of Neuroscience},
keywords = {Alert monkey,Figure-ground segregation,Neurophysiology,Visual cortex},
number = {6},
pages = {1290--1303},
pmid = {9215713},
title = {{Figure-ground segregation at contours: A neural mechanism in the visual cortex of the alert monkey}},
volume = {9},
year = {1997}
}
@inproceedings{Sarfraz2008,
author = {Sarfraz, M Saquib and Hellwich, Olaf},
booktitle = {International Conference on Computer Vision Theory and Applications},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Sarfraz, Hellwich - 2008 - Head pose estimation in face recognition across pose scenarios.pdf:pdf},
keywords = {-,estimation procedure to be,face recognition,facial pose,front-end pose classification,head,local energy models,local features,pose classification,pose estimation,shape description,used in face recognition,we present a robust},
pages = {235--242},
title = {{Head pose estimation in face recognition across pose scenarios}},
year = {2008}
}
@article{Goedeme2005,
abstract = {Upcoming fast vision techniques for finding image correspondences enable reliable real-time visual homing, i.e. the guidance of a mobile robot from a arbitrary start pose towards a goal pose defined by an image taken there. Two approaches emerge in the field that differ in the fact that the structure of the scene is estimated or not. In this paper, we compare these two approaches for the general case and especially for our application, being automatic wheelchair navigation.},
author = {Goedeme, Toon and Tuytelaars, T and {Van Gool}, L and Vanhooydonck, D and Demeester, E and Nuttin, M},
doi = {10.1109/CIRA.2005.1554294},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Goedeme et al. - 2005 - Is structure needed for omnidirectional visual homing.pdf:pdf},
isbn = {0-7803-9355-4},
journal = {Computational Intelligence in Robotics and Automation, 2005. CIRA 2005. Proceedings. 2005 IEEE International Symposium on},
keywords = {Biomedical optical imaging,Biosensors,Image sensors,Mechanical sensors,Optical sensors,Optical variables control,Robot kinematics,Robot sensing systems,automatic wheelchair navigation,computer vision,image correspondences,mobile robot,mobile robots,navigation,omnidirectional images,omnidirectional visual homing,real-time visual homing,robot navigation,robot vision,visual homing},
pages = {303--308},
title = {{Is structure needed for omnidirectional visual homing?}},
url = {http://ieeexplore.ieee.org/ielx5/10413/33075/01554294.pdf?tp={\&}arnumber=1554294{\&}isnumber=33075},
year = {2005}
}
@inproceedings{Ranjan2016,
abstract = {We learn to compute optical flow by combining a classical coarse-to-fine flow approach with deep learning. Specifically we adopt a spatial-pyramid formulation to deal with large motions. According to standard practice, we warp one image of a pair at each pyramid level by the current flow estimate and compute an update to the flow field. Instead of the standard minimization of an objective function at each pyramid level, we train a deep neural network at each level to compute the flow update. Unlike the recent FlowNet approach, the networks do not need to deal with large motions; these are dealt with by the pyramid. This has several advantages. First the network is much simpler and much smaller; our Spatial Pyramid Network (SPyNet) is 96{\%} smaller than FlowNet in terms of model parameters. Because it is so small, the method could be made to run on a cell phone or other embedded system. Second, since the flow is assumed to be small at each level ({\$}{\textless} 1{\$} pixel), a convolutional approach applied to pairs of warped images is appropriate. Third, unlike FlowNet, the filters that we learn appear similar to classical spatio-temporal filters, possibly giving insight into how to improve the method further. Our results are more accurate than FlowNet in most cases and suggest a new direction of combining classical flow methods with deep learning.},
archivePrefix = {arXiv},
arxivId = {1611.00850},
author = {Ranjan, Anurag and Black, Michael J.},
booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2017.291},
eprint = {1611.00850},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Ranjan, Black - 2017 - Optical Flow Estimation using a Spatial Pyramid Network.pdf:pdf},
isbn = {9781538604571},
publisher = {IEEE},
title = {{Optical Flow Estimation using a Spatial Pyramid Network}},
url = {http://arxiv.org/abs/1611.00850},
year = {2017}
}
@article{Thrun1998,
abstract = {Autonomous robots must be able to learn and maintain models of their environments. Research on mobile robot navigation has produced two major paradigms for mapping indoor environments: grid-based and topological. While grid-based methods produce accurate metric maps, their com- plexity often prohibits efficient planning and problem solving in large-scale indoor environments. Topological maps, on the other hand, can be used much more efficiently, yet accurate and con- sistent topological maps are often difficult to learn and maintain in large-scale environments, particularly if momentary sensor data is highly ambiguous. This paper describes an approach that integrates both paradigms: grid-based and topoIogica1. Grid-based maps are learned using artificial neural networks and naive Bayesian integration. Topological maps are generated on top of the grid-based maps, by partitioning the latter into coherent regions. By combining both paradigms, the approach presented here gains advantages from both worlds: accuracy/consistency and effi- ciency. The paper gives results for autonomous exploration, mapping and operation of a mobile robot in populated multi-room environments. @},
author = {Thrun, Sebastian},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Thrun - 1998 - Learning metric-topological maps for indoor mobile robot navigation.pdf:pdf},
journal = {Artificial Intelligence},
keywords = {aeronautical systems center,air force materiel,autonomous robots,exploration,mobile robots,neural networks,occupancy grids,path,planning,robot mapping,the national science,the wright laboratory,topological maps,under award iri-9313367},
pages = {21--71},
title = {{Learning metric-topological maps for indoor mobile robot navigation}},
volume = {99},
year = {1998}
}
@article{Jacobsen2016,
abstract = {Learning powerful feature representations with CNNs is hard when training data are limited. Pre-training is one way to overcome this, but it requires large datasets sufficiently similar to the target domain. Another option is to design priors into the model, which can range from tuned hyperparameters to fully engineered representations like Scattering Networks. We combine these ideas into structured receptive field networks, a model which has a fixed filter basis and yet retains the flexibility of CNNs. This flexibility is achieved by expressing receptive fields in CNNs as a weighted sum over a fixed basis which is similar in spirit to Scattering Networks. The key difference is that we learn arbitrary effective filter sets from the basis rather than modeling the filters. This approach explicitly connects classical multiscale image analysis with general CNNs. With structured receptive field networks, we improve considerably over unstructured CNNs for small and medium dataset scenarios as well as over Scattering for large datasets. We validate our findings on ILSVRC2012, Cifar-10, Cifar-100 and MNIST. As a realistic small dataset example, we show state-of-the-art classification results on popular 3D MRI brain-disease datasets where pre-training is difficult due to a lack of large public datasets in a similar domain.},
archivePrefix = {arXiv},
arxivId = {1605.02971},
author = {Jacobsen, J{\"{o}}rn-Henrik and van Gemert, Jan and Lou, Zhongyu and Smeulders, Arnold W. M.},
doi = {10.1109/CVPR.2016.286},
eprint = {1605.02971},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Jacobsen et al. - 2016 - Structured Receptive Fields in CNNs.pdf:pdf},
isbn = {978-1-4673-8851-1},
issn = {10636919},
pages = {2610--2619},
title = {{Structured Receptive Fields in CNNs}},
url = {http://arxiv.org/abs/1605.02971},
year = {2016}
}
@inproceedings{Basiri2013,
abstract = {Employing a group of independently controlled flying micro air vehicles (MAVs) for aerial coverage missions, instead of a single flying robot, increases the robustness and efficiency of the missions. Designing a group of MAVs requires addressing new challenges, such as inter-robot collision avoidance and formation control, where individual's knowledge about the relative location of their local group members is essential. A relative positioning system for a MAV needs to satisfy severe constraints in terms of size, weight, processing power, power consumption, three-dimensional coverage and price. In this paper we present an on-board audio based system that is capable of providing individuals with relative positioning information of their neighbouring sound emitting MAVs. We propose a method based on coherence testing among signals of a small onboard microphone array to obtain relative bearing measurements, and a particle filter estimator to fuse these measurements with information about the motion of robots throughout time to obtain the desired relative location estimates. A method based on frac- tional Fourier transform (FrFT) is used to identify and extract sounds of simultaneous chirping robots in the neighbourhood. Furthermore, we evaluate our proposed method in a real world experiment with three simultaneously flying micro air vehicles},
address = {Berlin, Germany},
author = {Basiri, Meysam and Schill, Felix and Floreano, Dario and Lima, Pedro},
booktitle = {Robotics: Science and Systems IX},
doi = {10.15607/RSS.2013.IX.002},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Basiri et al. - 2013 - Audio-based Relative Positioning System for Multiple Micro Air Vehicle Systems.pdf:pdf},
isbn = {9789810739379},
month = {jun},
number = {266470},
publisher = {Robotics: Science and Systems Foundation},
title = {{Audio-based Relative Positioning System for Multiple Micro Air Vehicle Systems}},
url = {http://www.roboticsproceedings.org/rss09/p02.pdf},
year = {2013}
}
@article{Binding2006,
abstract = {In this paper, we present a method that uses panoramic images to perform local homing. Our method is different from others in that it does not extract any features from the images and only performs simple image processing operations. Furthermore, it uses a method borrowed from computer graphics to simulate the effect in the images of translations of the robot to compute local motion parameters.},
author = {Binding, D and Labrosse, F},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Binding, Labrosse - 2006 - Visual local navigation using warped panoramic images.pdf:pdf},
journal = {Proceedings of Towards Autonomous Robotic Systems},
pages = {19--26},
title = {{Visual local navigation using warped panoramic images}},
volume = {26},
year = {2006}
}
@article{Gaffin2016,
author = {Gaffin, Douglas D. and Brayfield, Brad P.},
doi = {10.1371/journal.pone.0153706},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Gaffin, Brayfield - 2016 - Autonomous Visual Navigation of an Indoor Environment Using a Parsimonious, Insect Inspired Familiarity Algor.pdf:pdf},
issn = {1932-6203},
journal = {Plos One},
keywords = {biology,scene familiarity},
mendeley-tags = {biology,scene familiarity},
number = {4},
pages = {e0153706},
title = {{Autonomous Visual Navigation of an Indoor Environment Using a Parsimonious, Insect Inspired Familiarity Algorithm}},
url = {http://dx.plos.org/10.1371/journal.pone.0153706},
volume = {11},
year = {2016}
}
@article{Geiger2011a,
abstract = {Accurate 3d perception from video sequences is a core subject in computer vision and robotics, since it forms the basis of subsequent scene analysis. In practice however, online requirements often severely limit the utilizable camera resolution and hence also reconstruction accuracy. Furthermore, real-time systems often rely on heavy parallelism which can prevent applications in mobile devices or driver assistance systems, especially in cases where FPGAs cannot be employed. This paper proposes a novel approach to build 3d maps from high-resolution stereo sequences in real-time. Inspired by recent progress in stereo matching, we propose a sparse feature matcher in conjunction with an efficient and robust visual odometry algorithm. Our reconstruction pipeline combines both techniques with efficient stereo matching and a multi-view linking scheme for generating consistent 3d point clouds. In our experiments we show that the proposed odometry method achieves state-of-the-art accuracy. Including feature matching, the visual odometry part of our algorithm runs at 25 frames per second, while - at the same time - we obtain new depth maps at 3-4 fps, sufficient for online 3d reconstructions.},
author = {Geiger, Andreas and Ziegler, Julius and Stiller, Christoph},
doi = {10.1109/IVS.2011.5940405},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Geiger, Ziegler, Stiller - 2011 - StereoScan Dense 3d reconstruction in real-time.pdf:pdf},
isbn = {9781457708909},
issn = {1931-0587},
journal = {IEEE Intelligent Vehicles Symposium, Proceedings},
number = {Iv},
pages = {963--968},
pmid = {24344074},
title = {{StereoScan: Dense 3d reconstruction in real-time}},
year = {2011}
}
@inproceedings{Alcantarilla2016,
author = {{F. Alcantarilla}, Pablo and Stent, Simon and Ros, German and Arroyo, Roberto and Gherardi, Riccardo},
booktitle = {Robotics: Science and Systems XII},
doi = {10.15607/RSS.2016.XII.044},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/F. Alcantarilla et al. - 2016 - Street-View Change Detection with Deconvolutional Networks.pdf:pdf},
isbn = {9780992374723},
keywords = {pascual},
mendeley-tags = {pascual},
month = {jun},
publisher = {Robotics: Science and Systems Foundation},
title = {{Street-View Change Detection with Deconvolutional Networks}},
url = {http://www.roboticsproceedings.org/rss12/p44.pdf},
year = {2016}
}
@article{Dey2016,
abstract = {Cameras provide a rich source of information while being passive, cheap and lightweight for small and medium Unmanned Aerial Vehicles (UAVs). In this work we present the first implementation of receding horizon control, which is widely used in ground vehicles, with monocular vision as the only sensing mode for autonomous UAV flight in dense clutter. We make it feasible on UAVs via a number of contributions: novel coupling of perception and control via relevant and diverse, multiple interpretations of the scene around the robot, leveraging recent advances in machine learning to showcase anytime budgeted cost-sensitive feature selection, and fast non-linear regression for monocular depth prediction. We empirically demonstrate the efficacy of our novel pipeline via real world experiments of more than 2 kms through dense trees with a quadrotor built from off-the-shelf parts. Moreover our pipeline is designed to combine information from other modalities like stereo and lidar as well if available.},
archivePrefix = {arXiv},
arxivId = {1411.6326},
author = {Dey, Debadeepta and Shankar, Kumar Shaurya and Zeng, Sam and Mehta, Rupesh and Agcayazi, M. Talha and Eriksen, Christopher and Daftry, Shreyansh and Hebert, Martial and Bagnell, J. Andrew},
doi = {10.1007/978-3-319-27702-8_26},
eprint = {1411.6326},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Dey et al. - 2016 - Vision and learning for deliberative monocular cluttered flight.pdf:pdf},
isbn = {9783319277004},
issn = {1610742X},
journal = {Springer Tracts in Advanced Robotics},
pages = {391--409},
title = {{Vision and learning for deliberative monocular cluttered flight}},
volume = {113},
year = {2016}
}
@inproceedings{Angeli2008a,
abstract = {— In robotic applications of visual simultaneous localization and mapping, loopclosure detection and global localization are two issues that require the capacity to recognize a previously visited place from current camera measurements. We present an online method that makes it possible to detect when an image comes from an already perceived scene using local shape information. Our approach extends the bag of visual words method used in image recognition to incremental conditions and relies on Bayesian filtering to estimate loopclosure probability. We demonstrate the efficiency of our solution by realtime loopclosure detection under strong perceptual aliasing conditions in an indoor image sequence taken with a handheld camera.},
author = {Angeli, Adrien and Doncieux, Stephane and Meyer, JeanArcady and Filliat, David},
booktitle = {2008 IEEE International Conference on Robotics and Automation},
doi = {10.1109/ROBOT.2008.4543475},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Angeli et al. - 2008 - Real-time visual loop-closure detection.pdf:pdf},
isbn = {978-1-4244-1646-2},
keywords = {bag-of-words},
mendeley-tags = {bag-of-words},
pages = {1842--1847},
publisher = {IEEE},
title = {{Real-time visual loop-closure detection}},
url = {http://ieeexplore.ieee.org/document/4543475/},
year = {2008}
}
@article{Talukder2003,
abstract = {Dynamic scene perception is currently limited to detection of moving objects from a static platform or scenes with flat backgrounds. We discuss novel real-time methods to segment moving objects in the motion field formed by a moving camera/robotic platform with mostly translational motion. Our solution does not explicitly require any egomotion knowledge, thereby making the solution applicable to mobile outdoor robot problems where no IMU information is available. We address two problems in dynamic scene perception on the move, first using only 2D monocular grayscale images, and second where 3D range information from stereo is also available. Our solution involves real-time optical flow computations, followed by optical flow field preprocessing to highlight moving object boundaries. In the case where range data from stereo is computed, a 3D optical flow field is estimated by combining range information with 2D optical flow estimates, followed by a similar 3D flow field preprocessing step. A segmentation of the flow field using fast flood filling then identifies every moving object in the scene with a unique label. This novel algorithm is expected to be the critical first step in robust recognition of moving vehicles and people from mobile outdoor robots, and therefore offers a robust solution to the problem of dynamic scene perception in the presence of certain kinds of robot motion. It is envisioned that our algorithm will benefit robot scene perception in urban environments for scientific, commercial and defense applications. Results of our real-time algorithm on a mobile robot in a scene with a single moving vehicle are presented.},
author = {Talukder, Ashit and Goldberg, S. and Matthies, L. and Ansar, A.},
doi = {10.1109/IROS.2003.1248826},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Talukder et al. - 2003 - Real-time detection of moving objects in a dynamic scene from moving robotic vehicles.pdf:pdf},
isbn = {0-7803-7860-1},
journal = {Proceedings 2003 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2003) (Cat. No.03CH37453)},
keywords = {computer vision,dynamic scenes,moving,object detection,optical flow,robotics,scene understanding,segmentation},
number = {818},
pages = {0--5},
title = {{Real-time detection of moving objects in a dynamic scene from moving robotic vehicles}},
volume = {2},
year = {2003}
}
@article{Weiss2013,
abstract = {In this paper, we describe a novel approach in fusing optical flow with inertial cues (3D acceleration and 3D angular velocities) in order to navigate a Micro Aerial Vehicle (MAV) drift free in 4DoF and metric velocity. Our approach only requires two consecutive images with a minimum of three feature matches. It does not require any (point) map nor any type of feature history. Thus it is an inherently failsafe approach that is immune to map and feature-track failures. With these minimal requirements we show in real experiments that the system is able to navigate drift free in all angles including yaw, in one metric position axis, and in 3D metric velocity. Furthermore, it is a power-on-and-go system able to online self- calibrate the inertial biases, the visual scale and the full 6DoF extrinsic transformation parameters between camera and IMU.},
author = {Weiss, Stephan and Brockers, Roland and Matthies, Larry},
doi = {10.1109/IROS.2013.6696955},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Weiss, Brockers, Matthies - 2013 - 4DoF drift free navigation using inertial cues and optical flow.pdf:pdf},
isbn = {9781467363587},
issn = {21530858},
journal = {IEEE International Conference on Intelligent Robots and Systems},
keywords = {monocular,optical flow,state estimation},
mendeley-tags = {monocular,optical flow,state estimation},
pages = {4180--4186},
title = {{4DoF drift free navigation using inertial cues and optical flow}},
year = {2013}
}
@inproceedings{Nous2016,
author = {Nous, Clint and Meertens, Roland and de Wagter, Christophe and de Croon, Guido},
booktitle = {IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Nous et al. - 2016 - Performance Evaluation in Obstacle Avoidance.pdf:pdf},
isbn = {9781509037629},
pages = {3614--3619},
title = {{Performance Evaluation in Obstacle Avoidance}},
year = {2016}
}
@article{Davison2007,
abstract = {We present a real-time algorithm which can recover the 3D trajectory of a monocular camera, moving rapidly through a previously unknown scene. Our system, which we dub MonoSLAM, is the first successful application of the SLAM methodology from mobile robotics to the "pure vision" domain of a single uncontrolled camera, achieving real time but drift-free performance inaccessible to Structure from Motion approaches. The core of the approach is the online creation of a sparse but persistent map of natural landmarks within a probabilistic framework. Our key novel contributions include an active approach to mapping and measurement, the use of a general motion model for smooth camera movement, and solutions for monocular feature initialization and feature orientation estimation. Together, these add up to an extremely efficient and robust algorithm which runs at 30 Hz with standard PC and camera hardware. This work extends the range of robotic systems in which SLAM can be usefully applied, but also opens up new areas. We present applications of MonoSLAM to real-time 3D localization and mapping for a high-performance full-size humanoid robot and live augmented reality with a hand-held camera.},
archivePrefix = {arXiv},
arxivId = {there is not},
author = {Davison, Andrew J. and Reid, Ian D. and Molton, Nicholas D. and Stasse, Olivier},
doi = {10.1109/TPAMI.2007.1049},
eprint = {there is not},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Davison et al. - 2007 - MonoSLAM Real-time single camera SLAM.pdf:pdf},
isbn = {0162-8828},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {3D/stereo scene analysis,Autonomous vehicles,Tracking,monocular,slam},
mendeley-tags = {monocular,slam},
number = {6},
pages = {1052--1067},
pmid = {17431302},
title = {{MonoSLAM: Real-time single camera SLAM}},
volume = {29},
year = {2007}
}
@article{Fajen2003,
abstract = {Abstract. Using a biologically-inspired model, we show how successful route selection through a cluttered environment can emerge from on-line steering dynamics, without explicit path planning. The model is derived from experiments on human walking performed in the Virtual Environment Navigation Lab (VENLab) at Brown. We find that goals and obstacles behave as attractors and repellors of heading, the direction of locomotion, for an observer moving at a constant speed. The influence of a goal on turning rate increases with its angle from the heading and decreases exponentially with its distance; the influence of an obstacle decreases exponentially with angle and distance. Linearly combining goal and obstacle terms allows us to simulate paths through arbitrarily complex scenes, based on information about obstacles in view near the heading direction and a few meters ahead. We simulated the model on a variety of scene configurations and observed generally efficient routes, and verified this behavior on a mobile robot. Discussion focuses on comparisons between dynamical models and other approaches, including potential field models and explicit path planning. Effective route selection can thus be performed on-line, in simple environments as a consequence of elementary behaviors for steering and obstacle avoidance.},
annote = {Human inspired reactive obstacle avoidance},
author = {Fajen, Brett R. and William and Warren, H. and Temizer, Selim and Leslie and Kaelbling, Pack},
doi = {citeulike-article-id:4165026},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Fajen et al. - 2003 - A dynamical model of visually-guided steering, obstacle avoidance, and route selection.pdf:pdf},
isbn = {0920-5691},
issn = {1939-1277},
journal = {International Journal of Computer Vision},
keywords = {actions with complex,chang-,have a remarkable abil-,humans and other animals,ity to coordinate their,obstacle avoidance,optic flow,path planning,reactive obstacle avoidance,robot navigation,visual control of locomotion},
mendeley-tags = {obstacle avoidance,reactive obstacle avoidance},
pages = {13--34},
pmid = {19803653},
title = {{A dynamical model of visually-guided steering, obstacle avoidance, and route selection}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=95CADD775414B8E8C4A75506D5677998?doi=10.1.1.89.4417{\&}rep=rep1{\&}type=pdf{\%}5Cnhttp://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.89.4417},
volume = {54},
year = {2003}
}
@inproceedings{Loizou2007,
author = {Loizou, Savvas G and Kumar, Vijay},
booktitle = {Decision and Control, 2007 46th IEEE Conference on},
doi = {10.1109/CDC.2007.4435005},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Loizou, Kumar - 2007 - Biologically Inspired Bearing-Only Navigation and Tracking.pdf:pdf},
isbn = {1424414989},
pages = {1386--1391},
title = {{Biologically Inspired Bearing-Only Navigation and Tracking}},
year = {2007}
}
@inproceedings{Vardy2008,
author = {Vardy, Andrew},
booktitle = {Electrical and Computer Engineering, 2008. CCECE 2008. Canadian Conference on},
doi = {10.1109/CCECE.2008.4564808},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Vardy - 2008 - A simple visual compass with learned pixel weights.pdf:pdf},
isbn = {978-1-4244-1642-4},
pages = {1581--1586},
publisher = {IEEE},
title = {{A simple visual compass with learned pixel weights}},
year = {2008}
}
@article{Baddeley2012,
abstract = {In this paper we propose a model of visually guided route navigation in ants that captures the known properties of real behaviour whilst retaining mechanistic simplicity and thus biological plausibility. For an ant, the coupling of movement and viewing direction means that a familiar view specifies a familiar direction of movement. Since the views experienced along a habitual route will be more familiar, route navigation can be re-cast as a search for familiar views. This search can be performed with a simple scanning routine, a behaviour that ants have been observed to perform. We test this proposed route navigation strategy in simulation, by learning a series of routes through visually cluttered environments consisting of objects that are only distinguishable as silhouettes against the sky. In the first instance we determine view familiarity by exhaustive comparison with the set of views experienced during training. In further experiments we train an artificial neural network to perform familiarity discrimination using the training views. Our results indicate that, not only is the approach successful, but also that the routes that are learnt show many of the characteristics of the routes of desert ants. As such, we believe the model represents the only detailed and complete model of insect route guidance to date. What is more, the model provides a general demonstration that visually guided routes can be produced with parsimonious mechanisms that do not specify when or what to learn, nor separate routes into sequences of waypoints.},
annote = {Heading control met familiarity van visual panoramas.

Infomax om geen individuele afbeeldingen op te hoeven slaan.},
author = {Baddeley, Bart and Graham, Paul and Husbands, Philip and Philippides, Andrew},
doi = {10.1371/journal.pcbi.1002336},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Baddeley et al. - 2012 - A model of ant route navigation driven by scene familiarity.pdf:pdf},
isbn = {1553-7358 (Electronic)$\backslash$n1553-734X (Linking)},
issn = {1553734X},
journal = {PLoS Computational Biology},
keywords = {biology,highlight,homing,scene familiarity},
mendeley-tags = {biology,highlight,homing,scene familiarity},
number = {1},
pmid = {22241975},
title = {{A model of ant route navigation driven by scene familiarity}},
volume = {8},
year = {2012}
}
@article{Yagi2005,
abstract = {A route navigation method for a mobile robot with an omnidirectional image sensor is described. The route is memorized from a series of consecutive omnidirectional images of the horizon when the robot moves to its goal. While the robot is navigating to the goal point, input is matched against the memorized spatio-temporal route pattern by using dual active contour models and the exact robot position and orientation is estimated from the converged shape of the active contour models.},
author = {Yagi, Yasushi and Imai, Kousuke and Tsuji, Kentaro and Yachida, Masahiko},
doi = {10.1109/TPAMI.2005.11},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Yagi et al. - 2005 - Iconic memory-based omnidirectional route panorama navigation.pdf:pdf},
isbn = {0162-8828},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Active contour model,Localization,Navigation,Omnidirectional vision,Route panorama},
number = {1},
pages = {78--87},
pmid = {15628270},
title = {{Iconic memory-based omnidirectional route panorama navigation}},
volume = {27},
year = {2005}
}
@inproceedings{Vardy2003,
author = {Vardy, Andrew and Oppacher, Franz},
booktitle = {ECAL 2003: Advances in Artificial Life},
doi = {10.1007/978-3-540-39432-7_94},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Vardy, Oppacher - 2003 - Low-Level Visual Homing.pdf:pdf},
pages = {875--884},
title = {{Low-Level Visual Homing}},
url = {http://link.springer.com/10.1007/978-3-540-39432-7{\_}94},
year = {2003}
}
@article{Bills2011,
abstract = {We consider the problem of autonomously flying Miniature Aerial Vehicles (MAVs) in indoor environments such as home and office buildings. The primary long range sensor in these MAVs is a miniature camera. While previous approaches first try to build a 3D model in order to do planning and control, our method neither attempts to build nor requires a 3D model. Instead, our method first classifies the type of indoor environment the MAV is in, and then uses vision algorithms based on perspective cues to estimate the desired direction to fly. We test our method on two MAV platforms: a co-axial miniature helicopter and a toy quadrotor. Our experiments show that our vision algorithms are quite reliable, and they enable our MAVs to fly in a variety of corridors and staircases.},
annote = {Classifies corridors, stairs and uses perspective cues to navigate. Other types of rooms use close range sensors for very simple wall avoidance.},
author = {Bills, Cooper and Chen, Joyce and Saxena, Ashutosh},
doi = {10.1109/ICRA.2011.5980136},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Labayrade, Aubert, Tarel - 2015 - Real time obstacle detection in stereovision on non flat road geometry through v-disparity representat.pdf:pdf},
isbn = {9781612843865},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
keywords = {experiment,exploration,highlight,low complexity,monocular,nieuwe referenties,scenario: corridor,video},
mendeley-tags = {experiment,exploration,highlight,low complexity,monocular,nieuwe referenties,scenario: corridor,video},
pages = {5776--5783},
title = {{Autonomous MAV flight in indoor environments using single image perspective cues}},
year = {2011}
}
@inproceedings{Pizzoli2014,
author = {Pizzoli, Matia and Forster, Christian and Scaramuzza, Davide},
booktitle = {Robotics and Automation (ICRA), 2014 IEEE International Conference on},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Pizzoli, Forster, Scaramuzza - 2014 - REMODE Probabilistic, Monocular Dense Reconstruction in Real Time.pdf:pdf},
isbn = {9781479936854},
pages = {2609--2616},
publisher = {IEEE},
title = {{REMODE: Probabilistic, Monocular Dense Reconstruction in Real Time}},
year = {2014}
}
@article{Idris1997,
abstract = {Visual database systems require efficient indexing to facilitate fast access to the images and video sequences in the database. Recently, several content-based indexing methods for image and video based on spatial relationships, color, texture, shape, sketch, object motion, and camera parameters have been reported in the literature. The goal of this paper is to provide a critical survey of existing literature on content-based indexing techniques and to point out the relative advantages and disadvantages of each approach.},
author = {Idris, F and Panchanathan, S},
doi = {10.1006/jvci.1997.0355},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Idris, Panchanathan - 1997 - Review of image and video indexing techniques.pdf:pdf},
isbn = {1047-3203},
issn = {10473203},
journal = {Journal of Visual Communication and Image Representation},
number = {2},
pages = {146--166},
title = {{Review of image and video indexing techniques}},
url = {http://www.sciencedirect.com/science/article/B6WMK-45KKSGK-5/2/df25df5374b5ce44616de5550980b9d2},
volume = {8},
year = {1997}
}
@article{Wang2011a,
abstract = {This paper presents a novel method for feature description based on intensity order. Specifically, a Local Intensity Order Pattern(LIOP) is proposed to encode the local ordinal information of each pixel and the overall ordinal information is used to divide the local patch into subregions which are used for accumulating the LIOPs respectively. Therefore, both local and overall intensity ordinal information of the local patch are captured by the proposed LIOP descriptor so as to make it a highly discriminative descriptor. It is shown that the proposed descriptor is not only invariant to monotonic intensity changes and image rotation but also robust to many other geometric and photometric transformations such as viewpoint change, image blur and JEPG compression. The proposed descriptor has been evaluated on the standard Oxford dataset and four additional image pairs with complex illumination changes. The experimental results show that the proposed descriptor obtains a significant improvement over the existing state-of-the-art descriptors.},
author = {Wang, Zhenhua and Fan, Bin and Wu, Fuchao},
doi = {10.1109/ICCV.2011.6126294},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Wang, Fan, Wu - 2011 - Local intensity order pattern for feature description.pdf:pdf},
isbn = {9781457711015},
issn = {1550-5499},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
pages = {603--610},
title = {{Local intensity order pattern for feature description}},
year = {2011}
}
@article{Younes2016,
archivePrefix = {arXiv},
arxivId = {1607.00470v1},
author = {Younes, Georges and Asmar, Daniel and Shammas, Elie},
eprint = {1607.00470v1},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Younes, Asmar, Shammas - 2016 - A survey on non-filter-based monocular Visual SLAM systems.pdf:pdf},
journal = {arXiv preprint arXiv:1607.00470},
keywords = {monocular,non-filter based,visual slam},
title = {{A survey on non-filter-based monocular Visual SLAM systems}},
year = {2016}
}
@article{Laina2016,
abstract = {This paper addresses the problem of estimating the depth map of a scene given a single RGB image. We propose a fully convolutional architecture, encompassing residual learning, to model the ambiguous mapping between monocular images and depth maps. In order to improve the output resolution, we present a novel way to efficiently learn feature map up-sampling within the network. For optimization, we introduce the reverse Huber loss that is particularly suited for the task at hand and driven by the value distributions commonly present in depth maps. Our model is composed of a single architecture that is trained end-to-end and does not rely on post-processing techniques, such as CRFs or other additional refinement steps. As a result, it runs in real-time on images or videos. In the evaluation, we show that the proposed model contains fewer parameters and requires fewer training data than the current state of the art, while outperforming all approaches on depth estimation. Code and models are publicly available.},
archivePrefix = {arXiv},
arxivId = {1606.00373},
author = {Laina, Iro and Rupprecht, Christian and Belagiannis, Vasileios and Tombari, Federico and Navab, Nassir},
doi = {10.1109/3DV.2016.32},
eprint = {1606.00373},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Laina et al. - 2016 - Deeper depth prediction with fully convolutional residual networks.pdf:pdf},
isbn = {9781509054077},
journal = {Proceedings - 2016 4th International Conference on 3D Vision, 3DV 2016},
keywords = {CNN,Depth prediction},
pages = {239--248},
title = {{Deeper depth prediction with fully convolutional residual networks}},
year = {2016}
}
@article{Meyer2003,
abstract = {This article reviews map-learning and path-planning strategies within the context of map-based navigation in mobile robots. Concerning map-learning, it distinguishes metric maps from topological maps and describes procedures that help maintain the coherency of these maps. Concerning path-planning, it distinguishes continuous from discretized spaces and describes procedures applicable when the execution of a plan fails. It insists on the need for an integrated conception of such procedures, which must be tightly tailored to the specific robot that is used, notably to the capacities and limitations of its sensory-motor equipment, and to the specific environment that is experienced. A hierarchy of navigation strategies is outlined in the discussion, together with the sort of adaptive capacities each affords to cope with unexpected obstacles or dangers encountered on an animat or robot's way to its goal. ?? 2003 Elsevier B.V. All rights reserved.},
author = {Meyer, Jean Arcady and Filliat, David},
doi = {10.1016/S1389-0417(03)00007-X},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Meyer, Filliat - 2003 - Map-based navigation in mobile robots II. A review of map-learning and path-planning strategies.pdf:pdf},
isbn = {1389-0417},
issn = {13890417},
journal = {Cognitive Systems Research},
keywords = {Autonomous mobile robot,Map-based navigation,Map-learning,Path-planning},
number = {4},
pages = {283--317},
title = {{Map-based navigation in mobile robots: II. A review of map-learning and path-planning strategies}},
volume = {4},
year = {2003}
}
@inproceedings{Albaker2009,
author = {Albaker, B. M. and Rahim, N. A.},
booktitle = {Technical Postgraduates (TECHPOS), 2009 International Conference for},
doi = {10.1109/TECHPOS.2009.5412074},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Albaker, Rahim - 2009 - A Survey of Collision Avoidance Approaches for Unmanned Aerial Vehicles.pdf:pdf},
title = {{A Survey of Collision Avoidance Approaches for Unmanned Aerial Vehicles}},
year = {2009}
}
@article{Engel2014a,
abstract = {We present a complete solution for the visual navigation of a small-scale, low-cost quadrocopter in unknown environments. Our approach relies solely on a monocular camera as the main sensor, and therefore does not need external tracking aids such as GPS or visual markers. Costly computations are carried out on an external laptop that communicates over wireless LAN with the quadrocopter. Our approach consists of three components: a monocular SLAM system, an extended Kalman filter for data fusion, and a PID controller. In this paper, we (1) propose a simple, yet effective method to compensate for large delays in the control loop using an accurate model of the quadrocopter's flight dynamics, and (2) present a novel, closed-form method to estimate the scale of a monocular SLAM system from additional metric sensors. We extensively evaluated our system in terms of pose estimation accuracy, flight accuracy, and flight agility using an external motion capture system. Furthermore, we compared the convergence and accuracy of our scale estimation method for an ultrasound altimeter and an air pressure sensor with filtering-based approaches. The complete system is available as open-source in ROS. This software can be used directly with a low-cost, off-the-shelf Parrot AR.Drone quadrocopter, and hence serves as an ideal basis for follow-up research projects. {\textcopyright} 2014 Elsevier B.V. All rights reserved.},
author = {Engel, Jakob and Sturm, J{\"{u}}rgen and Cremers, Daniel},
doi = {10.1016/j.robot.2014.03.012},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Engel, Sturm, Cremers - 2014 - Scale-aware navigation of a low-cost quadrocopter with a monocular camera.pdf:pdf},
isbn = {9781467317375},
issn = {09218890},
journal = {Robotics and Autonomous Systems},
keywords = {AR.Drone,Monocular SLAM,Quadrocopter,Scale estimation,Visual SLAM,Visual navigation,monocular,scenario: room,slam},
mendeley-tags = {monocular,scenario: room,slam},
number = {11},
pages = {1646--1656},
publisher = {Elsevier B.V.},
title = {{Scale-aware navigation of a low-cost quadrocopter with a monocular camera}},
url = {http://dx.doi.org/10.1016/j.robot.2014.03.012},
volume = {62},
year = {2014}
}
@article{Shell2007,
abstract = {This article describes how to use the IEEEtran class with LATEX to produce high quality typeset papers that are suitable for submission to the Institute of Electrical and Electronics Engineers (IEEE). IEEEtran can produce conference, journal and technical note (correspondence) papers with a suitable choice of class options. This document was produced using IEEEtran in journal mode.},
archivePrefix = {arXiv},
arxivId = {suresh govindarajan},
author = {Shell, Michael},
doi = {10.1016/j.neuron.2008.07.028},
eprint = {suresh govindarajan},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Shell - 2007 - How to Use the IEEEtran LaTEX Class.pdf:pdf},
isbn = {1097-4199 (Electronic)$\backslash$r0896-6273 (Linking)},
issn = {1097-4199},
journal = {Journal of LaTeX class files},
keywords = {Class, IEEE, IEEEtran, LaTeX, Typesetting, TeX},
number = {4},
pages = {1--26},
pmid = {18701059},
title = {{How to Use the IEEEtran LaTEX Class}},
url = {http://www.opticsinfobase.org/jocn/submit/templates/jocn/IEEETran.zip},
volume = {6},
year = {2007}
}
@article{Angeli2009,
abstract = {Visual localization and mapping for mobile robots has been achieved with a large variety of methods. Among them, topological navigation using vision has the advantage of offering a scalable representation, and of relying on a common and affordable sensor. In previous work, we developed such an incremental and real-time topological mapping and localization solution, without using any metrical information, and by relying on a Bayesian visual loop-closure detection algorithm. In this paper, we propose an extension of this work by integrating metrical information from robot odometry in the topological map, so as to obtain a globally consistent environment model. Also, we demonstrate the performance of our system on the global localization task, where the robot has to determine its position in a map acquired beforehand.},
author = {Angeli, Adrien and Doncieux, St́ephane and Meyer, Jean Arcady and Filliat, David},
doi = {10.1109/ROBOT.2009.5152501},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Angeli et al. - 2009 - Visual topological slam and global localization.pdf:pdf},
isbn = {9781424427895},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
pages = {4300--4305},
title = {{Visual topological slam and global localization}},
year = {2009}
}
@article{DeBoer,
abstract = {—In recent years optical-flow-aided position measurement solutions have been used in both commercial and academic applications. These systems are used for navigating unmanned aerial vehicles (UAVs) in GPS-deprived environments. Movement in sequential images is detected and converted to real world position change. Multiple approaches have been suggested, ranging from using an optical mouse sensor to the use of a stereo camera setup. Our research focuses on single camera solutions. Previous research has used a variety of optical flow algorithms for single camera solutions. This paper presents a comparison on three algorithms to check if using different algorithms yield different results in terms of quality and CPU time. This paper also provides insight into the general theory behind using single camera optical flow for UAV navigation. The compared algorithms are the Lucas-Kanade method, Gunnar Farne ack's algorithm and Block Matching. A testing framework and custom indoor and outdoor datasets were created to measure algorithms flow estimation quality and computation time. Characteristic differences were found between the performance of the algorithms in terms of both computation time and quality. Choosing between algorithms therefore can increase flow estimation quality or reduce CPU time usage. Also different winners per test set were found in terms of estimation quality.},
author = {{De Boer}, Jasper and Kalksma, Mathieu},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/De Boer, Kalksma - Unknown - Choosing between optical flow algorithms for UAV position change measurement.pdf:pdf},
keywords = {Block Matching,Gunnar Farne ack method,Index Terms—UAV navigation,Lucas-Kanade method,optical flow},
title = {{Choosing between optical flow algorithms for UAV position change measurement}}
}
@inproceedings{Jones1997,
author = {Jones, S.D. and Andresen, Claus and Crowley, J.L.},
booktitle = {Proceedings of the 1997 IEEE/RSJ International Conference on Intelligent Robot and Systems. Innovative Robotics for Real-World Applications. IROS '97},
doi = {10.1109/IROS.1997.655066},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Jones, Andresen, Crowley - 1997 - Appearance based process for visual navigation.pdf:pdf},
isbn = {0-7803-4119-8},
pages = {551--557},
publisher = {IEEE},
title = {{Appearance based process for visual navigation}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=655066},
volume = {2},
year = {1997}
}
@misc{Longuet-Higgins1980,
abstract = {It is shown that from a monocular view of a rigid, textured, curved surface it is possible, in principle, to determine the gradient of the surface at any point, and the motion of the eye relative to it, from the velocity field of the changing retinal image, and its first and second spatial derivatives. The relevant equations are redundant, thus providing a test of the rigidity assumption. They involve, among other observable quantities, the components of shear of the retinal velocity field, suggesting that the visual system may possess specialized channels for computing these components.},
author = {Longuet-Higgins, H. C. and Prazdny, K},
booktitle = {Proceedings of the Royal Society of London, B: Biological Sciences},
doi = {10.1098/rspb.1980.0057},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Longuet-Higgins, Prazdny - 1980 - The interpretation of a moving retinal image.pdf:pdf},
isbn = {0080-4649 (Print)$\backslash$r0080-4649 (Linking)},
issn = {0962-8452},
keywords = {optical flow},
mendeley-tags = {optical flow},
number = {1173},
pages = {385--397},
pmid = {6106198},
title = {{The interpretation of a moving retinal image}},
volume = {208},
year = {1980}
}
@article{Mejias2006,
abstract = {It is anticipatedthat theMars ScienceLaboratory rover, namedCuriosity,will traverse 10–20 kmon the surface of Mars during its primary mission. In preparation for this traverse, Earth-based tests were performed using Mars weight vehicles. These vehicles were driven over Mars analog bedrock, cohesive soil, and cohesionless sand at various slopes. Vehicle slip was characterized on each of these terrains versus slope for direct upslope driving. Results show that slopes up to 22 degrees are traversable on smooth bedrock and that slopes up to 28 degrees are traversable on some cohesive soils. In cohesionless sand, results show a sharp transition between moderate slip on 10 degree slopes and vehicle embedding at 17 degrees. For cohesionless sand, data are also presented showing the relationship between vehicle slip and wheel sinkage. Side by side testing of the Mars Exploration Rover test vehicle and the Mars Science Laboratory test vehicle show how increased wheel diameter leads to better slope climbing ability in sand for vehicles with nearly identical ground pressure. Lastly, preliminary data from Curiosity's initial driving on Mars are presented and compared to the Earth-based testing, showing good agreement for the driving done during the first 250 Martian days.},
archivePrefix = {arXiv},
arxivId = {10.1.1.91.5767},
author = {Mej{\'{i}}as, Luis and Saripalli, Srikanth and Campoy, Pascual and Sukhatme, Gaurav S.},
doi = {10.1002/rob.20115},
eprint = {10.1.1.91.5767},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Mej{\'{i}}as et al. - 2006 - Visual servoing of an autonomous helicopter in urban areas using feature tracking.pdf:pdf},
isbn = {9783902661623},
issn = {1556-4959},
journal = {Journal of Field Robotics},
keywords = {Backstepping,Non-linear control,Path-following},
month = {mar},
number = {3-4},
pages = {185--199},
pmid = {22164016},
title = {{Visual servoing of an autonomous helicopter in urban areas using feature tracking}},
url = {http://doi.wiley.com/10.1002/rob.20115},
volume = {23},
year = {2006}
}
@article{Ho2007,
author = {Ho, Kin Leong and Newman, Paul},
doi = {10.1007/s11263-006-0020-1},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Ho, Newman - 2007 - Detecting Loop Closure with Scene Sequences.pdf:pdf},
issn = {0920-5691},
journal = {International Journal of Computer Vision},
keywords = {loop closing,mobile robotics,multi-robot navigation,scene appearance and navigation,slam},
month = {jul},
number = {3},
pages = {261--286},
title = {{Detecting Loop Closure with Scene Sequences}},
url = {http://link.springer.com/10.1007/s11263-006-0020-1},
volume = {74},
year = {2007}
}
@article{Humenberger2010a,
author = {Humenberger, Martin and Zinner, Christian and Weber, Michael and Kubinger, Wilfried and Vincze, Markus},
doi = {10.1016/j.cviu.2010.03.012},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Humenberger et al. - 2010 - A fast stereo matching algorithm suitable for embedded real-time systems.pdf:pdf},
issn = {1077-3142},
journal = {Computer Vision and Image Understanding},
number = {11},
pages = {1180--1202},
publisher = {Elsevier Inc.},
title = {{A fast stereo matching algorithm suitable for embedded real-time systems}},
url = {http://dx.doi.org/10.1016/j.cviu.2010.03.012},
volume = {114},
year = {2010}
}
@article{Borenstein1989,
author = {Borenstein, J and Koren, Y},
doi = {10.1109/21.44033},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Borenstein, Koren - 1989 - Real-time obstacle avoidance for fast mobile robots.pdf:pdf},
issn = {0018-9472},
journal = {IEEE Transactions on Systems, Man, and Cybernetics},
month = {sep},
number = {5},
pages = {1179--1187},
title = {{Real-time obstacle avoidance for fast mobile robots}},
url = {http://ieeexplore.ieee.org/document/44033/},
volume = {19},
year = {1989}
}
@article{Iandola2016,
abstract = {Recent research on deep neural networks has focused primarily on improving accuracy. For a given accuracy level, it is typically possible to identify multiple DNN architectures that achieve that accuracy level. With equivalent accuracy, smaller DNN architectures offer at least three advantages: (1) Smaller DNNs require less communication across servers during distributed training. (2) Smaller DNNs require less bandwidth to export a new model from the cloud to an autonomous car. (3) Smaller DNNs are more feasible to deploy on FPGAs and other hardware with limited memory. To provide all of these advantages, we propose a small DNN architecture called SqueezeNet. SqueezeNet achieves AlexNet-level accuracy on ImageNet with 50x fewer parameters. Additionally, with model compression techniques we are able to compress SqueezeNet to less than 0.5MB (510x smaller than AlexNet). The SqueezeNet architecture is available for download here: https://github.com/DeepScale/SqueezeNet},
archivePrefix = {arXiv},
arxivId = {1602.07360},
author = {Iandola, Forrest N. and Han, Song and Moskewicz, Matthew W. and Ashraf, Khalid and Dally, William J. and Keutzer, Kurt},
doi = {10.1007/978-3-319-24553-9},
eprint = {1602.07360},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Iandola et al. - 2016 - SqueezeNet AlexNet-level accuracy with 50x fewer parameters and 0.5MB model size.pdf:pdf},
isbn = {978-3-319-24552-2},
issn = {0302-9743},
pages = {1--13},
pmid = {23285570},
title = {{SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and {\textless}0.5MB model size}},
url = {http://arxiv.org/abs/1602.07360},
year = {2016}
}
@article{Smolyanskiy2018,
abstract = {We revisit the problem of visual depth estimation in the context of autonomous vehicles. Despite the progress on monocular depth estimation in recent years, we show that the gap between monocular and stereo depth accuracy remains large---a particularly relevant result due to the prevalent reliance upon monocular cameras by vehicles that are expected to be self-driving. We argue that the challenges of removing this gap are significant, owing to fundamental limitations of monocular vision. As a result, we focus our efforts on depth estimation by stereo. We propose a novel semi-supervised learning approach to training a deep stereo neural network, along with a novel architecture containing a machine-learned argmax layer and a custom runtime (that will be shared publicly) that enables a smaller version of our stereo DNN to run on an embedded GPU. Competitive results are shown on the KITTI 2015 stereo dataset. We also evaluate the recent progress of stereo algorithms by measuring the impact upon accuracy of various design criteria.},
archivePrefix = {arXiv},
arxivId = {1803.09719},
author = {Smolyanskiy, Nikolai and Kamenev, Alexey and Birchfield, Stan},
eprint = {1803.09719},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Smolyanskiy, Kamenev, Birchfield - 2018 - On the Importance of Stereo for Accurate Depth Estimation An Efficient Semi-Supervised Deep Ne.pdf:pdf},
journal = {arXiv preprint arXiv:1803.09719},
keywords = {on-board},
mendeley-tags = {on-board},
title = {{On the Importance of Stereo for Accurate Depth Estimation: An Efficient Semi-Supervised Deep Neural Network Approach}},
year = {2018}
}
@article{Talukder2004,
abstract = {Dynamic scene perception is very important for autonomous vehicles operating around other moving vehicles and humans. Most work on real-time camera based object tracking from moving platforms has used sparse features or assumed flat scene structures. We have recently extended a real-time, dense stereo system to include real-time, dense optical flow, enabling more comprehensive dynamic scene analysis. We describe algorithms to robustly estimate 6-DOF robot egomotion in the presence of moving objects using dense flow and dense stereo. We then use dense stereo and egomotion estimates to identify other moving objects while the robot itself is moving. We present results showing accurate egomotion estimation and detection of moving people and vehicles under general 6-DOF motion of the robot and independently moving objects. The system runs at 18.3 Hz on a 1.4 GHz Pentium M laptop, computing 160{\&}times;120 disparity maps and optical flow fields, egomotion, and moving object segmentation. We believe this is a significant step toward general unconstrained dynamic scene analysis for mobile robots, as well as for improved position estimation where GPS is unavailable.},
annote = {Segmentation of moving obstacles by comparing flow (optical/stereo) vs expectations.},
author = {Talukder, a. and Matthies, L.},
doi = {10.1109/IROS.2004.1389993},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Talukder, Matthies - 2004 - Real-time detection of moving objects from moving vehicles using dense stereo and optical flow.pdf:pdf},
isbn = {0-7803-8463-6},
journal = {2004 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) (IEEE Cat. No.04CH37566)},
keywords = {dynamic scene analysis,egomotion,low complexity,moving object detection,moving obstacles,object tracking,obstacle detection,optical flow,stereo vision},
mendeley-tags = {low complexity,moving obstacles,obstacle detection,optical flow,stereo vision},
pages = {3718--3725},
title = {{Real-time detection of moving objects from moving vehicles using dense stereo and optical flow}},
volume = {4},
year = {2004}
}
@article{Weiss2011b,
abstract = {Recent development showed that Micro Aerial Vehicles (MAVs) are nowadays capable of autonomously take off at one point and land at another using only one single camera as exteroceptive sensor. During the flight and landing phase the MAV and user have, however, little knowledge about the whole terrain and potential obstacles. In this paper we show a new solution for a real-time dense 3D terrain reconstruction. This can be used for efficient unmanned MAV terrain exploration and yields a solid base for standard autonomous obstacle avoidance algorithms and path planners. Our approach is based on a textured 3D mesh on sparse 3D point features of the scene. We use the same feature points to localize and control the vehicle in the 3D space as we do for building the 3D terrain reconstruction mesh. This enables us to reconstruct the terrain without significant additional cost and thus in real-time. Experiments show that the MAV is easily guided through an unknown, GPS denied environment. Obstacles are recognized in the iteratively built 3D terrain reconstruction and are thus well avoided.},
author = {Weiss, Stephan and Achtelik, Markus and Kneip, Laurent and Scaramuzza, Davide and Siegwart, Roland},
doi = {10.1007/s10846-010-9491-y},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Weiss et al. - 2011 - Intuitive 3D maps for MAV terrain exploration and obstacle avoidance.pdf:pdf},
isbn = {0921-0296},
issn = {09210296},
journal = {Journal of Intelligent and Robotic Systems: Theory and Applications},
keywords = {3D map generation,MAV navigation,Mesh map,Obstacle avoidance,Terrain exploration},
number = {1-4},
pages = {473--493},
title = {{Intuitive 3D maps for MAV terrain exploration and obstacle avoidance}},
volume = {61},
year = {2011}
}
@article{Morerio2017,
abstract = {Dropout is a very effective way of regularizing neural networks. Stochastically "dropping out" units with a certain probability discourages over-specific co-adaptations of feature detectors, preventing overfitting and improving network generalization. Besides, Dropout can be interpreted as an approximate model aggregation technique, where an exponential number of smaller networks are averaged in order to get a more powerful ensemble. In this paper, we show that using a fixed dropout probability during training is a suboptimal choice. We thus propose a time scheduling for the probability of retaining neurons in the network. This induces an adaptive regularization scheme that smoothly increases the difficulty of the optimization problem. This idea of "starting easy" and adaptively increasing the difficulty of the learning problem has its roots in curriculum learning and allows one to train better models. Indeed, we prove that our optimization strategy implements a very general curriculum scheme, by gradually adding noise to both the input and intermediate feature representations within the network architecture. Experiments on seven image classification datasets and different network architectures show that our method, named Curriculum Dropout, frequently yields to better generalization and, at worst, performs just as well as the standard Dropout method.},
archivePrefix = {arXiv},
arxivId = {1703.06229},
author = {Morerio, Pietro and Cavazza, Jacopo and Volpi, Riccardo and Vidal, Rene and Murino, Vittorio},
doi = {10.1109/ICCV.2017.383},
eprint = {1703.06229},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Morerio et al. - 2017 - Curriculum Dropout.pdf:pdf},
isbn = {9781538610329},
issn = {15505499},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
pages = {3564--3572},
title = {{Curriculum Dropout}},
volume = {2017-Octob},
year = {2017}
}
@article{Becerra2014,
abstract = {Many applications of wheeled mobile robots demand a good solution for the autonomous mobility problem, i.e., the navigation with large displacement. A promising approach to solve this problem is the following of a visual path extracted from a visual memory. In this paper, we propose an image-based control scheme for driving wheeled mobile robots along visual paths. Our approach is based on the feedback of information given by geometric constraints: the epipolar geometry or the trifocal tensor. The proposed control law only requires one measurement easily computed from the image data through the geometric constraint. The proposed approach has two main advantages: explicit pose parameters decomposition is not required and the rotational velocity is smooth or eventually piece-wise constant avoiding discontinuities that generally appear in previous works when the target image changes. The translational velocity is adapted as demanded for the path and the resultant motion is independent of this velocity. Furthermore, our approach is valid for all cameras with approximated central projection, including conventional, catadioptric and some fisheye cameras. Simulations and real-world experiments illustrate the validity of the proposal.},
author = {Becerra, H M and Sagues, C and Mezouar, Y and Hayet, J B},
doi = {DOI 10.1007/s10514-014-9382-3},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Becerra et al. - 2014 - Visual navigation of wheeled mobile robots using direct feedback of a geometric constraint.pdf:pdf},
isbn = {0929-5593},
issn = {0929-5593},
journal = {Autonomous Robots},
keywords = {epipolar geometry,localization,memory,trifocal tensor,vision,visual memory,visual navigation,visual path following},
number = {2},
pages = {137--156},
title = {{Visual navigation of wheeled mobile robots using direct feedback of a geometric constraint}},
volume = {37},
year = {2014}
}
@article{Alvarez2016,
author = {Alvarez, H and Paz, L M and Sturm, J and Cremers, D},
doi = {10.1007/978-3-319-23778-7},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Alvarez et al. - 2016 - Collision Avoidance for Quadrotors with a Monocular Camera.pdf:pdf},
isbn = {9783319237787},
journal = {Springer Tracts in Advanced Robotics},
number = {Experimental Robotics},
pages = {195--209},
title = {{Collision Avoidance for Quadrotors with a Monocular Camera}},
volume = {109},
year = {2016}
}
@article{Lehman2018,
abstract = {Biological evolution provides a creative fount of complex and subtle adaptations, often surprising the scientists who discover them. However, because evolution is an algorithmic process that transcends the substrate in which it occurs, evolution's creativity is not limited to nature. Indeed, many researchers in the field of digital evolution have observed their evolving algorithms and organisms subverting their intentions, exposing unrecognized bugs in their code, producing unexpected adaptations, or exhibiting outcomes uncannily convergent with ones in nature. Such stories routinely reveal creativity by evolution in these digital worlds, but they rarely fit into the standard scientific narrative. Instead they are often treated as mere obstacles to be overcome, rather than results that warrant study in their own right. The stories themselves are traded among researchers through oral tradition, but that mode of information transmission is inefficient and prone to error and outright loss. Moreover, the fact that these stories tend to be shared only among practitioners means that many natural scientists do not realize how interesting and lifelike digital organisms are and how natural their evolution can be. To our knowledge, no collection of such anecdotes has been published before. This paper is the crowd-sourced product of researchers in the fields of artificial life and evolutionary computation who have provided first-hand accounts of such cases. It thus serves as a written, fact-checked collection of scientifically important and even entertaining stories. In doing so we also present here substantial evidence that the existence and importance of evolutionary surprises extends beyond the natural world, and may indeed be a universal property of all complex evolving systems.},
archivePrefix = {arXiv},
arxivId = {1803.03453},
author = {Lehman, Joel and Clune, Jeff and Misevic, Dusan and Adami, Christoph and Beaulieu, Julie and Bentley, Peter J. and Bernard, Samuel and Belson, Guillaume and Bryson, David M. and Cheney, Nick and Cully, Antoine and Donciuex, Stephane and Dyer, Fred C. and Ellefsen, Kai Olav and Feldt, Robert and Fischer, Stephan and Forrest, Stephanie and Fr{\'{e}}noy, Antoine and Gagne{\'{e}}, Christian and Goff, Leni Le and Grabowski, Laura M. and Hodjat, Babak and Keller, Laurent and Knibbe, Carole and Krcah, Peter and Lenski, Richard E. and Lipson, Hod and MacCurdy, Robert and Maestre, Carlos and Miikkulainen, Risto and Mitri, Sara and Moriarty, David E. and Mouret, Jean-Baptiste and Nguyen, Anh and Ofria, Charles and Parizeau, Marc and Parsons, David and Pennock, Robert T. and Punch, William F. and Ray, Thomas S. and Schoenauer, Marc and Shulte, Eric and Sims, Karl and Stanley, Kenneth O. and Taddei, Fran{\c{c}}ois and Tarapore, Danesh and Thibault, Simon and Weimer, Westley and Watson, Richard and Yosinksi, Jason},
eprint = {1803.03453},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Lehman et al. - 2018 - The Surprising Creativity of Digital Evolution A Collection of Anecdotes from the Evolutionary Computation and Ar.pdf:pdf},
pages = {1--31},
title = {{The Surprising Creativity of Digital Evolution: A Collection of Anecdotes from the Evolutionary Computation and Artificial Life Research Communities}},
url = {http://arxiv.org/abs/1803.03453},
year = {2018}
}
@article{Wallach1982,
abstract = {It is proposed that some distance cues are learned when a perceptual parameter that varies with observation distance is regularly associated with objects whose distances are perceived because another distance cue operates. If that is the way distance cues can come into existence, it may be possible to identify a parameter that varies with distance but is not a known distance cue and to show that it functions as one. The slope of regard with which an object on the ground is viewed is such a potential distance cue. Its angle varies approximately with the reciprocal of distance. An experiment was done that showed that this slope angle functions as a distance cue. Subjects who looked through a device that altered slope angles gave estimates of the dimensions of an object on the ground. Perceived sizes, which vary inversely with distance, were found to be altered in accordance with the altered slope angle.},
author = {Wallach, Hans and O'Leary, Ann},
doi = {10.3758/BF03206214},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Wallach, O'Leary - 1982 - Slope of regard as a distance cue.pdf:pdf},
issn = {00315117},
journal = {Perception {\&} Psychophysics},
number = {2},
pages = {145--148},
pmid = {7079094},
title = {{Slope of regard as a distance cue}},
volume = {31},
year = {1982}
}
@article{Sturm2009,
author = {Sturm, J and Visser, A},
doi = {10.1016/j.robot.2008.10.002},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Sturm, Visser - 2009 - An appearance-based visual compass for mobile robots.pdf:pdf},
issn = {0921-8890},
journal = {Robotics and Autonomous Systems},
number = {5},
pages = {536--545},
publisher = {Elsevier B.V.},
title = {{An appearance-based visual compass for mobile robots}},
url = {http://dx.doi.org/10.1016/j.robot.2008.10.002},
volume = {57},
year = {2009}
}
@inproceedings{Booij2007,
abstract = {— Vision systems are used more and more in 'per-sonal' robots interacting with humans, since semantic infor-mation about objects and places can be derived from the rich sensory information. Visual information is also used for building appearance based topological maps, which can be used for localization. In this paper we describe a system capable of using this appearance based topological map for navigation. The system is made robust by using the epipolar geometry and a planar floor constraint in computing the necessary heading information. Using this method the robot is able to drive robustly in a large environment. We tested the method on real data under varying environment conditions and compared performance with a human-controlled robot.},
author = {Booij, O and Terwijn, B and Zivkovic, Z and Krose, B.},
booktitle = {Proceedings 2007 IEEE International Conference on Robotics and Automation},
doi = {10.1109/ROBOT.2007.364081},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Booij et al. - 2007 - Navigation using an appearance based topological map.pdf:pdf},
isbn = {1-4244-0602-1},
issn = {1050-4729},
month = {apr},
pages = {3927--3932},
publisher = {IEEE},
title = {{Navigation using an appearance based topological map}},
url = {http://ieeexplore.ieee.org/document/4209699/},
year = {2007}
}
@article{Fu2012a,
abstract = {This paper proposes a fast image sequence-based navigation approach for a flat route represented in sparse waypoints. Instead of purely optimizing the length of the path, this paper aims to speed up the navigation by lengthening the distance between consecutive waypoints. When local visual homing in a variable velocity is applied for robot navigation between two waypoints, the robot's speed changes according to the distance between waypoints. Because long distance implies large scale difference between the robot's view and the waypoint image, log-polar transform is introduced to find a correspondence between images and infer a less accurate motion vector. In order to maintain the navigation accuracy, our prior work on local visual homing with SIFT feature matching is adopted when the robot is relatively close to the waypoint. Experiments support the proposed navigation approach in a multiple-waypoint route. Compared to other prior work on visual homing with SIFT feature matching, the proposed navigation approach requires fewer waypoints and the navigation speed is improved without compromising the accuracy in navigation. ?? 2012 Elsevier B.V. All rights reserved.},
author = {Fu, Yu and Hsiang, Tien Ruey},
doi = {10.1016/j.imavis.2012.02.006},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Fu, Hsiang - 2012 - A fast robot homing approach using sparse image waypoints.pdf:pdf},
issn = {02628856},
journal = {Image and Vision Computing},
keywords = {Image sequence-based navigation,Log-polar transform,Sparse waypoints},
number = {2},
pages = {109--121},
title = {{A fast robot homing approach using sparse image waypoints}},
volume = {30},
year = {2012}
}
@article{Wedel2011,
abstract = {Building upon recent developments in optical flow and stereo matching estimation, we propose a variational framework for the estimation of stereoscopic scene flow, i.e., the motion of points in the three-dimensional world from stereo image sequences. The proposed algorithm takes into account image pairs from two consecutive times and computes both depth and a 3D motion vector associated with each point in the image. In contrast to previous works, we partially decouple the depth estimation from the motion estimation, which has many practical advantages. The variational formulation is quite flexible and can handle both sparse or dense disparity maps. The proposed method is very efficient; with the depth map being computed on an FPGA, and the scene flow computed on the GPU, the proposed algorithm runs at frame rates of 20 frames per second on QVGA images (320{\{}$\backslash$texttimes{\}}240 pixels). Furthermore, we present solutions to two important problems in scene flow estimation: violations of intensity consistency between input images, and the uncertainty measures for the scene flow result.},
author = {Wedel, Andreas and Brox, Thomas and Vaudrey, Tobi and Rabe, Clemens and Franke, Uwe and Cremers, Daniel},
doi = {10.1007/s11263-010-0404-0},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Wedel et al. - 2011 - Stereoscopic Scene Flow Computation for 3D Motion Understanding.pdf:pdf},
issn = {1573-1405},
journal = {International Journal of Computer Vision},
number = {1},
pages = {29--51},
title = {{Stereoscopic Scene Flow Computation for 3D Motion Understanding}},
url = {https://doi.org/10.1007/s11263-010-0404-0},
volume = {95},
year = {2011}
}
@article{Campoy2009,
abstract = {Computer vision is much more than a technique to sense and recover environmental information from an UAV. It should play a main role regarding UAVs' functionality because of the big amount of information that can be extracted, its possible uses and applications, and its natural connection to human driven tasks, taking into account that vision is our main interface to world understanding. Our current research's focus lays on the development of techniques that allow UAVs to maneuver in spaces using visual information as their main input source. This task involves the creation of techniques that allow an UAV to maneuver towards features of interest whenever a GPS signal is not reliable or sufficient, e.g. when signal dropouts occur (which usually happens in urban areas, when flying through terrestrial urban canyons or when operating on remote planetary bodies), or when tracking or inspecting visual targets—including moving ones—without knowing their exact UMT coordinates. This paper also investigates visual servoing control techniques that use velocity and position of suitable image features to compute the references for flight control. This paper aims to give a global view of the main aspects related to the research field of computer vision for UAVs, clustered in four main active research lines: visual servoing and control, stereo-based visual navigation, image processing algorithms for detection and tracking, and visual SLAM. Finally, the results of applying these techniques in several applications are presented and discussed: this study will encompass power line inspection, mobile target tracking, stereo distance estimation, mapping and positioning.},
author = {Campoy, Pascual and Correa, Juan F. and Mondrag??n, Ivan and Mart??nez, Carol and Olivares, Miguel and Mej??as, Luis and Artieda, Jorge},
doi = {10.1007/s10846-008-9256-z},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Campoy et al. - 2009 - Computer vision onboard UAVs for civilian tasks.pdf:pdf},
isbn = {9781402091},
issn = {09210296},
journal = {Journal of Intelligent and Robotic Systems: Theory and Applications},
keywords = {Feature detection,Image processing,SLAM,Tracking,UAV,Visual servoing,monocular,slam},
mendeley-tags = {monocular,slam},
number = {1-3 SPEC. ISS.},
pages = {105--135},
title = {{Computer vision onboard UAVs for civilian tasks}},
volume = {54},
year = {2009}
}
@inproceedings{Labayrade2015,
abstract = {Soil is the largest organic carbon (C) pool of terrestrial ecosystems, and C loss from soil accounts for a large proportion of land-atmosphere C exchange. Therefore, a small change in soil organic C (SOC) can affect atmospheric carbon dioxide (CO2) concentration and climate change. In the past decades, a wide variety of studies have been conducted to quantify global SOC stocks and soil C exchange with the atmosphere through site measurements, inventories, and empirical/process-based modeling. However, these estimates are highly uncertain, and identifying major driving forces controlling soil C dynamics remains a key research challenge. This study has compiled century-long (1901–2010) estimates of SOC storage and heterotrophic respiration (Rh) from 10 terrestrial biosphere models (TBMs) in the Multi-scale Synthesis and Terrestrial Model Intercomparison Project and two observation-based data sets. The 10 TBM ensemble shows that global SOC estimate ranges from 425 to 2111 Pg C (1 Pg = 1015 g) with a median value of 1158 Pg C in 2010. The models estimate a broad range of Rh from 35 to 69 Pg C yr−1 with a median value of 51 Pg C yr−1 during 2001–2010. The largest uncertainty in SOC stocks exists in the 40–65°N latitude whereas the largest cross-model divergence in Rh are in the tropics. The modeled SOC change during 1901–2010 ranges from −70 Pg C to 86 Pg C, but in some models the SOC change has a different sign from the change of total C stock, implying very different contribution of vegetation and soil pools in determining the terrestrial C budget among models. The model ensemble-estimated mean residence time of SOC shows a reduction of 3.4 years over the past century, which accelerate C cycling through the land biosphere. All the models agreed that climate and land use changes decreased SOC stocks, while elevated atmospheric CO2 and nitrogen deposition over intact ecosystems increased SOC stocks—even though the responses varied significantly among models. Model representations of temperature and moisture sensitivity, nutrient limitation, and land use partially explain the divergent estimates of global SOC stocks and soil C fluxes in this study. In addition, a major source of systematic error in model estimations relates to nonmodeled SOC storage in wetlands and peatlands, as well as to old C storage in deep soil layers.},
author = {Labayrade, R. and Aubert, D. and Tarel, J.-P.},
booktitle = {Intelligent Vehicle Symposium, 2002. IEEE},
doi = {10.1109/IVS.2002.1188024},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Labayrade, Aubert, Tarel - 2015 - Real time obstacle detection in stereovision on non flat road geometry through v-disparity representat.pdf:pdf},
isbn = {0-7803-7346-4},
issn = {19449224},
keywords = {belowground processes,heterotrophic respiration (Rh),mean residence time (MRT),soil carbon dynamics model,soil organic carbon (SOC),uncertainty},
number = {6},
pages = {646--651},
publisher = {IEEE},
title = {{Real time obstacle detection in stereovision on non flat road geometry through "v-disparity" representation}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1188024},
volume = {2},
year = {2015}
}
@article{Hou2015,
abstract = {Deep convolutional neural networks (CNN) have recently been shown in many computer vision and pattern recog- nition applications to outperform by a significant margin state- of-the-art solutions that use traditional hand-crafted features. However, this impressive performance is yet to be fully exploited in robotics. In this paper, we focus one specific problem that can benefit from the recent development of the CNN technology, i.e., we focus on using a pre-trained CNN model as a method of generating an image representation appropriate for visual loop closure detection in SLAM (simultaneous localization and mapping). We perform a comprehensive evaluation of the outputs at the intermediate layers of a CNN as image descriptors, in comparison with state-of-the-art image descriptors, in terms of their ability to match images for detecting loop closures. The main conclusions of our study include: (a) CNN-based image representations perform comparably to state-of-the-art hand- crafted competitors in environments without significant lighting change, (b) they outperform state-of-the-art competitors when lighting changes significantly, and (c) they are also significantly faster to extract than the state-of-the-art hand-crafted features even on a conventional CPU and are two orders of magnitude faster on an entry-level GPU.},
archivePrefix = {arXiv},
arxivId = {1504.05241},
author = {Hou, Yi and Zhang, Hong and Zhou, Shilin},
eprint = {1504.05241},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Hou, Zhang, Zhou - 2015 - Convolutional Neural Network-Based Image Representation for Visual Loop Closure Detection.pdf:pdf},
month = {apr},
title = {{Convolutional Neural Network-Based Image Representation for Visual Loop Closure Detection}},
url = {http://arxiv.org/abs/1504.05241},
year = {2015}
}
@article{Furgale2010,
author = {Furgale, Paul and Barfoot, Timothy D},
doi = {10.1002/rob.20342},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Furgale, Barfoot - 2010 - Visual teach and repeat for long-range rover autonomy.pdf:pdf},
journal = {Journal of Field Robotics},
number = {5},
pages = {534--560},
title = {{Visual teach and repeat for long-range rover autonomy}},
volume = {27},
year = {2010}
}
@phdthesis{VanHecke2015,
author = {van Hecke, Kevin},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/van Hecke - 2015 - Persistent self-supervised learning principle study and demonstration on flying robots.pdf:pdf},
school = {Delft University of Technology},
title = {{Persistent self-supervised learning principle: study and demonstration on flying robots}},
year = {2015}
}
@article{Ibragimov2017,
author = {Ibragimov, Ilmir Z and Afanasyev, Ilya M},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Ibragimov, Afanasyev - 2017 - Comparison of ROS-based Visual SLAM methods in homogeneous indoor environment.pdf:pdf},
isbn = {9781538630891},
keywords = {dpptam,kinect,lidar,orb-slam,ros,rtab-map,visual odometry,visual slam,zed camera},
title = {{Comparison of ROS-based Visual SLAM methods in homogeneous indoor environment}},
year = {2017}
}
@article{Reichel1990,
abstract = {A relative depth judgement task was used to distinguish perceived reversals in depth due to image orientation from spontaneous reversals such as those observed with a Necker cube. Results demonstrate that inversion effects due to image orientation can occur for several different types of pictorial representation and that many of these effects are incompatible with traditional explanations involving a perceptual bias for overhead illumination. When this bias was neutralized by placing the light source at the point of observation, the effects of image orientation were just as large as with overhead illumination. Similar results were also obtained for surfaces depicted with texture or motion in which all relevant shading information was eliminated. These results can be explained by a perceptual bias for backward slanting surfaces, but additional evidence suggests that this bias can be attenuated by the presence of smooth occlusion contours.},
author = {Reichel, Francene D. and Todd, James T.},
doi = {10.1037/0096-1523.16.3.653},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Reichel, Todd - 1990 - Perceived Depth Inversion of Smoothly Curved Surfaces Due to Image Orientation.pdf:pdf},
isbn = {0096-1523 (Print)$\backslash$r0096-1523 (Linking)},
issn = {00961523},
journal = {Journal of Experimental Psychology: Human Perception and Performance},
number = {3},
pages = {653--664},
pmid = {2144578},
title = {{Perceived Depth Inversion of Smoothly Curved Surfaces Due to Image Orientation}},
volume = {16},
year = {1990}
}
@inbook{Wedel2008,
abstract = {This paper presents a technique for estimating the three-dimensional velocity vector field that describes the motion of each visible scene point (scene flow). The technique presented uses two consecutive image pairs from a stereo sequence. The main contribution is to decouple the position and velocity estimation steps, and to estimate dense velocities using a variational approach. We enforce the scene flow to yield consistent displacement vectors in the left and right images. The decoupling strategy has two main advantages: Firstly, we are independent in choosing a disparity estimation technique, which can yield either sparse or dense correspondences, and secondly, we can achieve frame rates of 5 fps on standard consumer hardware. The approach provides dense velocity estimates with accurate results at distances up to 50 meters.},
address = {Berlin, Heidelberg},
author = {Wedel, Andreas and Rabe, Clemens and Vaudrey, Tobi and Brox, Thomas and Franke, Uwe and Cremers, Daniel},
booktitle = {Computer Vision -- ECCV 2008: 10th European Conference on Computer Vision, Marseille, France, October 12-18, 2008, Proceedings, Part I},
doi = {10.1007/978-3-540-88682-2_56},
editor = {Forsyth, David and Torr, Philip and Zisserman, Andrew},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Wedel et al. - 2008 - Efficient Dense Scene Flow from Sparse or Dense Stereo Data.pdf:pdf},
isbn = {978-3-540-88682-2},
pages = {739--751},
publisher = {Springer Berlin Heidelberg},
title = {{Efficient Dense Scene Flow from Sparse or Dense Stereo Data}},
url = {https://doi.org/10.1007/978-3-540-88682-2{\_}56},
year = {2008}
}
@article{Fischler1981,
abstract = {A new paradigm, Random Sample Consensus (RANSAC), for fitting a model to experimental data is introduced. RANSAC is capable of interpreting/smoothing data containing a significant percentage of gross errors, and is thus ideally suited for applications in automated image analysis where interpretation is based on the data provided by error-prone feature detectors. A major portion of this paper describes the application of RANSAC to the Location Determination Problem (LDP): Given an image depicting a set of landmarks with known locations, determine that point in space from which the image was obtained. In response to a RANSAC requirement, new results are derived on the minimum number of landmarks needed to obtain a solution, and algorithms are presented for computing these minimum-landmark solutions in closed form. These results provide the basis for an automatic system that can solve the LDP under difficult viewing},
author = {Fischler, Martin a. and Bolles, Robert C.},
doi = {10.1145/358669.358692},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Fischler, Bolles - 1981 - Random sample consensus a paradigm for model fitting with applications to image analysis and automated cartogr.pdf:pdf},
isbn = {0934613338},
issn = {00010782},
journal = {Communications of the ACM},
keywords = {0,1,2,3,5,60,61,71,8,analysis,and phrases,automated cartography,camera calibration,cr categories,determination,image matching,location,model fitting,scene},
number = {6},
pages = {381--395},
title = {{Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography}},
url = {http://portal.acm.org/citation.cfm?doid=358669.358692},
volume = {24},
year = {1981}
}
@inproceedings{Dunkley2014,
author = {Dunkley, Oliver and Engel, Jakob and Cremers, Daniel},
booktitle = {IROS2014 aerial open source robotics workshop},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Dunkley, Engel, Cremers - 2014 - Visual-Inertial Navigation for a Camera-Equipped 25 g Nano-Quadrotor.pdf:pdf},
pages = {2},
title = {{Visual-Inertial Navigation for a Camera-Equipped 25 g Nano-Quadrotor}},
year = {2014}
}
@article{Hubner2007,
abstract = {Most recent robotic systems, capable of exploring unknown environments, use topological structures (graphs) as a spatial representation. Localization can be done by deriving an estimate of the global pose from landmark information. In this case navigation is tightly coupled to metric knowledge, and hence the derived control method is mainly pose-based. Alternative to continuous metric localization, it is also possible to base localization methods on weaker constraints, e.g. the similarity between images capturing the appearance of places or landmarks. In this case navigation can be controlled by a homing algorithm. Similarity based localization can be scaled to continuous metric localization by adding additional constraints, such as alignment of depth estimates. We present a method to scale a similarity based navigation system (the view-graph-model) to continuous metric localization. Instead of changing the landmark model, we embed the graph into the three dimensional pose space. Therefore, recalibration of the path integrator is only possible at discrete locations in the environment. The navigation behavior of the robot is controlled by a homing algorithm which combines three local navigation capabilities, obstacle avoidance, path integration, and scene based homing. This homing scheme allows automated adaptation to the environment. It is further used to compensate for path integration errors, and therefore allows to derive globally consistent pose estimates based on "weak" metric knowledge, i.e. knowledge solely derived from odometry. The system performance is tested with a robotic setup and with a simulated agent which explores a large, open, and cluttered environment.},
author = {H{\"{u}}bner, Wolfgang and Mallot, Hanspeter A.},
doi = {10.1007/s10514-007-9040-0},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/H{\"{u}}bner, Mallot - 2007 - Metric embedding of view-graphs A vision and odometry-based approach to cognitive mapping.pdf:pdf},
isbn = {0929-5593},
issn = {09295593},
journal = {Autonomous Robots},
keywords = {Autonomous map building,Cognitive mapping,Simultaneous localization and mapping},
number = {3},
pages = {183--196},
title = {{Metric embedding of view-graphs : A vision and odometry-based approach to cognitive mapping}},
volume = {23},
year = {2007}
}
@article{Kloet2017,
author = {Kloet, N and Watkins, S and Clothier, R},
doi = {10.1177/1756829316681868},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Kloet, Watkins, Clothier - 2017 - Acoustic signature measurement of small multi-rotor unmanned aircraft systems.pdf:pdf},
journal = {International Journal of Micro Air Vehicles},
keywords = {14 january 2016,26 october 2015,accepted,acoustic,date received,noise,quadcopter,uas,unmanned aircraft system},
number = {1},
pages = {3--14},
title = {{Acoustic signature measurement of small multi-rotor unmanned aircraft systems}},
volume = {9},
year = {2017}
}
@incollection{Trzcinski2012,
abstract = {... 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 True Po sitiv e Ra te ... Efficient Discriminative Projections for Compact Binary Descriptors ... Moreover, there is a trade-off between the accuracy and efficiency of the dic- tionaries: The slower one yields the best recognition performance, and vice versa. ... $\backslash$n},
author = {Trzcinski, Tomasz and Lepetit, Vincent},
booktitle = {European Conference on Computer Vision},
doi = {10.1007/978-3-642-33718-5_17},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Trzcinski, Lepetit - 2012 - Efficient Discriminative Projections for Compact Binary Descriptors.pdf:pdf},
isbn = {9783642337178},
issn = {03029743},
keywords = {D-BRIEF},
mendeley-tags = {D-BRIEF},
pages = {228--242},
publisher = {Springer},
title = {{Efficient Discriminative Projections for Compact Binary Descriptors}},
url = {http://link.springer.com/10.1007/978-3-642-33718-5{\_}17},
year = {2012}
}
@article{Engel2017,
author = {Engel, Jakob and Koltun, Vladlen and Cremers, Daniel},
doi = {10.1109/TPAMI.2017.2658577},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Engel, Koltun, Cremers - 2017 - Direct Sparse Odometry.pdf:pdf},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
title = {{Direct Sparse Odometry}},
year = {2017}
}
@phdthesis{Tapus2005a,
author = {Tapus, Adriana},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Tapus - 2005 - Topological SLAM - Simultaneous Localization and Mapping with Fingerprints of Places.pdf:pdf},
keywords = {slam},
mendeley-tags = {slam},
school = {{\'{E}}COLE POLYTECHNIQUE F{\'{E}}D{\'{E}}RALE DE LAUSANNE},
title = {{Topological SLAM - Simultaneous Localization and Mapping with Fingerprints of Places}},
volume = {3357},
year = {2005}
}
@article{Dubey2018,
abstract = {— Agile MAVs are required to operate in cluttered, unstructured environments at high speeds and low altitudes for efficient data gathering. Given the payload constraints and long range sensing requirements, cameras are the preferred sensing modality for MAVs. The computation burden of using cameras for obstacle sensing has forced the state of the art methods to construct world representations on a per frame basis, leading to myopic decision making. In this paper we propose a long range perception and planning approach using cameras. By utilizing FPGA hardware for disparity calculation and image space to represent obstacles, our approach and system design allows for construction of long term world representation whilst accounting for highly non-linear noise models in real time. We demonstrate these obstacle avoidance capabilities on a quad-rotor flying through dense foliage at speeds of up to 4 m/s for a total of 1.6 hours of autonomous flights. The presented approach enables high speed navigation at low altitudes for MAVs for terrestrial scouting.},
author = {Dubey, Geetesh and Madaan, Ratnesh and Scherer, Sebastian},
doi = {10.1109/IROS.2018.8593499},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Dubey, Madaan, Scherer - 2018 - DROAN - Disparity-Space Representation for Obstacle Avoidance Enabling Wire Mapping Avoidance.pdf:pdf},
isbn = {9781538680940},
issn = {21530866},
journal = {IEEE International Conference on Intelligent Robots and Systems},
pages = {6311--6318},
title = {{DROAN - Disparity-Space Representation for Obstacle Avoidance: Enabling Wire Mapping Avoidance}},
year = {2018}
}
@article{Carloni2013,
author = {Carloni, Raffaella and Lippiello, Vincenzo and D'Auria, Massimo and Fumagalli, Matteo and Mersha, Abeje Y. and Stramigioli, Stefano and Siciliano, Bruno},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Carloni et al. - 2013 - Robot Vision Obstacle-Avoidance Techniques for Unmanned Aerial Vehicles.pdf:pdf},
journal = {IEEE ROBOTICS {\&} AUTOMATION MAGAZINE},
number = {December},
pages = {22--31},
title = {{Robot Vision: Obstacle-Avoidance Techniques for Unmanned Aerial Vehicles}},
year = {2013}
}
@article{Yang2014,
author = {Yang, Xin and Cheng, Kwang-Ting},
doi = {10.1109/TPAMI.2013.150},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Yang, Cheng - 2014 - Local Difference Binary for Ultrafast and Distinctive Feature Description.pdf:pdf},
issn = {0162-8828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
month = {jan},
number = {1},
pages = {188--194},
title = {{Local Difference Binary for Ultrafast and Distinctive Feature Description}},
url = {http://ieeexplore.ieee.org/document/6579616/},
volume = {36},
year = {2014}
}
@article{Zampoglou2006,
author = {Zampoglou, Markos and Szenher, Matthew and Webb, Barbara},
doi = {10.1177/1059712306072338},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Zampoglou, Szenher, Webb - 2006 - Adaptation of Controllers for Image-Based Homing 1.pdf:pdf},
journal = {Adaptive Behavior},
keywords = {homing,image difference,taxis},
number = {4},
pages = {381--399},
title = {{Adaptation of Controllers for Image-Based Homing 1}},
volume = {14},
year = {2006}
}
@article{Garm2007,
abstract = {Box jellyfish, cubomedusae, possess an impressive total of 24 eyes of four morphologically different types. Two of these eye types, called the upper and lower lens eyes, are camera-type eyes with spherical fish-like lenses. Compared with other cnidarians, cubomedusae also have an elaborate behavioral repertoire, which seems to be predominantly visually guided. Still, positive phototaxis is the only behavior described so far that is likely to be correlated with the eyes. We have explored the obstacle avoidance response of the Caribbean species Tripedalia cystophora and the Australian species Chiropsella bronzie in a flow chamber. Our results show that obstacle avoidance is visually guided. Avoidance behavior is triggered when the obstacle takes up a certain angle in the visual field. The results do not allow conclusions on whether color vision is involved but the strength of the response had a tendency to follow the intensity contrast between the obstacle and the surroundings (chamber walls). In the flow chamber Tripedalia cystophora displayed a stronger obstacle avoidance response than Chiropsella bronzie since they had less contact with the obstacles. This seems to follow differences in their habitats.},
author = {Garm, A. and O'Connor, M. and Parkefelt, L. and Nilsson, D.-E.},
doi = {10.1242/jeb.004044},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Garm et al. - 2007 - Visually guided obstacle avoidance in the box jellyfish Tripedalia cystophora and Chiropsella bronzie.pdf:pdf},
isbn = {0022-0949 (Print)},
issn = {0022-0949},
journal = {The Journal of Experimental Biology},
keywords = {behavior,biology,box jellyfish,cnidaria,eyes,obstacle avoidance},
mendeley-tags = {biology},
pages = {3616--3623},
pmid = {17921163},
title = {{Visually guided obstacle avoidance in the box jellyfish Tripedalia cystophora and Chiropsella bronzie}},
volume = {210},
year = {2007}
}
@article{Gillner2008,
annote = {Hierarchy of navigation tasks},
author = {Gillner, Sabine and Mallot, H.},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Gillner, Mallot - 2008 - These Maps Are Made for Walking - Task Hierarchy of Spatial Cognition.pdf:pdf},
journal = {Robotics and cognitive approaches to spatial mapping},
keywords = {highlight},
mendeley-tags = {highlight},
pages = {181--201},
title = {{These Maps Are Made for Walking - Task Hierarchy of Spatial Cognition}},
url = {http://www.springerlink.com/index/4341767569h03046.pdf},
year = {2008}
}
@article{Vardy2006,
abstract = {A biologically-inspired approach to robot route following is presented. Snapshot images of a robot's environment are captured while learning a route. Later, when retracing the route, the robot uses visual homing to move between positions where snapshot images had been captured. This general approach was inspired by experiments on route following in wood ants. The impact of odometric error and another key parameter is studied in relation to the number of snapshots captured by the learning algorithm. Tests in a photo-realistic simulated environment reveal that route following can succeed even on relatively sparse paths. A major change in illumination reduces, but does eliminate, the robot's ability to retrace a route.},
author = {Vardy, Andrew},
doi = {10.1109/ROBIO.2006.340381},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Vardy - 2006 - Long-range visual homing.pdf:pdf},
isbn = {1424405718},
journal = {2006 IEEE International Conference on Robotics and Biomimetics, ROBIO 2006},
keywords = {Insect navigation,Robot navigation,Route following,Visual homing},
pages = {220--226},
title = {{Long-range visual homing}},
year = {2006}
}
@article{Magree2014,
author = {Magree, Daniel and Mooney, John G. and Johnson, Eric N.},
doi = {10.1007/s10846-013-9967-7},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Magree, Mooney, Johnson - 2014 - Monocular Visual Mapping for Obstacle Avoidance on UAVs.pdf:pdf},
journal = {J Intell Robot Syst},
keywords = {extended,kalman filter,monocualar vision,obstacle avoidance,terrain mapping,visual estimation},
pages = {17--26},
title = {{Monocular Visual Mapping for Obstacle Avoidance on UAVs}},
volume = {74},
year = {2014}
}
@book{Thrun2006,
author = {Thrun, Sebastian and Burgard, Wolfram and Fox, Dieter},
publisher = {The MIT Press},
title = {{Probabilistic Robotics}},
year = {2006}
}
@article{Tijmons2017,
archivePrefix = {arXiv},
arxivId = {arXiv:1604.00833v2},
author = {Tijmons, S. and de Croon, G.C.H.E. and Remes, B.D.W. and {De Wagter}, C. and Mulder, M.},
eprint = {arXiv:1604.00833v2},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Tijmons et al. - 2017 - Obstacle Avoidance Strategy using Onboard Stereo Vision on a Flapping Wing MAV.pdf:pdf},
journal = {IEEE Transactions on Robotics},
number = {4},
pages = {858--874},
title = {{Obstacle Avoidance Strategy using Onboard Stereo Vision on a Flapping Wing MAV}},
volume = {33},
year = {2017}
}
@article{Winters1999,
author = {Winters, Niall and Santos-Victor, Jos{\'{e}}},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Winters, Santos-Victor - 1999 - Omni-directional visual navigation.pdf:pdf},
journal = {Proceedings of the 7th International Symposium on Intelligent Robotics Systems},
keywords = {appearance-based navigation,topological slam},
mendeley-tags = {appearance-based navigation,topological slam},
pages = {109--118},
title = {{Omni-directional visual navigation}},
year = {1999}
}
@article{Saxe2011,
abstract = {Recently two anomalous results in the literature have shown that certain feature learning architectures can yield useful features for object recognition tasks even with untrained, random weights. In this paper we pose the question: why do random weights sometimes do so well? Our answer is that certain convolutional pooling architectures can be inherently frequency selective and translation invariant, even with random weights. Based on this we demonstrate the viability of extremely fast architecture search by using random weights to evaluate candidate architectures, thereby sidestepping the time-consuming learning process. We then show that a surprising fraction of the performance of certain state-of-the-art methods can be attributed to the architecture alone.},
author = {Saxe, Andrew M and Koh, Pang Wei and Chen, Zhenghao and Bhand, Maneesh and Suresh, Bipin and Ng, Andrew Y},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Saxe et al. - 2011 - On Random Weights and Unsupervised Feature Learning.pdf:pdf},
isbn = {9781450306195},
journal = {Learning},
number = {2009},
pages = {1--9},
title = {{On Random Weights and Unsupervised Feature Learning}},
url = {http://ai.stanford.edu/{~}ang/papers/nipsdlufl10-RandomWeights.pdf},
year = {2011}
}
@article{Zufferey2005,
abstract = {  We aim at developing autonomous micro-flyers capable of navigating within houses or small built environments. The severe weight and energy constraints of indoor flying platforms led us to take inspiration from flying insects for the selection of sensors, signal processing, and behaviors. This paper presents the control strategies enabling obstacle avoidance and altitude control using only optic flow and gyroscopic information. For experimental convenience, the control strategies are first implemented and tested separately on a small wheeled robot featuring the same hardware as the targeted aircraft. The obstacle avoidance system is then transferred to a 30-gram aircraft, which demonstrates autonomous steering within a square textured arena. },
annote = {Vrij basic, daardoor goed geschikt als beginpunt. Behandelt obstacle detection met optical flow en saccades, en altitude hold met constante snelheid en downwards optical flow.

Optical flow alleen in bepaalde image regions waar deze de meeste informatie geeft.},
author = {Zufferey, Jean Christophe and Floreano, Dario},
doi = {10.1109/ROBOT.2005.1570504},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Zufferey, Floreano - 2005 - Toward 30-gram autonomous indoor aircraft Vision-based obstacle avoidance and altitude control.pdf:pdf},
isbn = {078038914X},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
keywords = {Altitude control,Collision avoidance,Indoor flying robot,Optic flow,Steering control,experiment,low complexity,monocular,nieuwe referenties,obstacle detection,optical flow,reactive obstacle avoidance,scenario: room,simulation,turn away from flow},
mendeley-tags = {experiment,low complexity,monocular,nieuwe referenties,obstacle detection,optical flow,reactive obstacle avoidance,scenario: room,simulation,turn away from flow},
number = {April},
pages = {2594--2599},
title = {{Toward 30-gram autonomous indoor aircraft: Vision-based obstacle avoidance and altitude control}},
volume = {2005},
year = {2005}
}
@book{DeBerg2008,
abstract = {This well-accepted introduction to computational geometry is a textbook for high-level undergraduate and low-level graduate courses. The focus is on algorithms and hence the book is well suited for students in computer science and engineering. Motivation is provided from the application areas: all solutions and techniques from computational geometry are related to particular applications in robotics, graphics, CAD/CAM, and geographic information systems. For students this motivation will be especially welcome. Modern insights in computational geometry are used to provide solutions that are both efficient and easy to understand and implement. All the basic techniques and topics from computational geometry, as well as several more advanced topics, are covered. The book is largely self-contained and can be used for self-study by anyone with a basic background in algorithms. In this third edition, besides revisions to the second edition, new sections discussing Voronoi diagrams of line segments, farthest-point Voronoi diagrams, and realistic input models have been added.},
author = {{De Berg}, M and Cheong, O and {Van Kreveld}, M and Overmars, M},
booktitle = {Computational Geometry},
doi = {10.2307/3620533},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/De Berg et al. - 2008 - Computational Geometry Algorithms and Applications.pdf:pdf},
isbn = {3540779736},
issn = {00255572},
pages = {99--105},
title = {{Computational Geometry: Algorithms and Applications}},
url = {http://www.google.com/books?id=tkyG8W2163YC},
volume = {17},
year = {2008}
}
@inproceedings{Matsumoto1999,
abstract = {Recently, view-based or appearance-based approaches have been attracting the interests of computer vision research. We have already proposed a visual view-based navigation method using a model of the route called the " View Sequence. " which contains a sequence of front views along a route memorized in the teach-ing run. In this paper, we apply an omnidirectional vision sensor to our view-based navigation and pro-pose an extended model of a route called " Omni-View Sequence. " The omnidirectional vision sensor is a de-sirable sensor for real-time view-based recognition of a mobile robot because the all information around the robot can be acquired simultaneously. The use of om-nidirectional images contribute to a better navigation method in three ways : 1) it enables the robot to come back along the route using the same View Sequence, 2) it improves the accuracy of the navigation and 3) it improves the robustness of the matching. These im-proved points are shown through experiments, and a indoor navigation, including getting on and off an ele-vator, is demonstrated.},
author = {Matsumoto, Yoshio and Ikeda, K. and Inaba, M. and Inoue, H.},
booktitle = {Proceedings 1999 IEEE/RSJ International Conference on Intelligent Robots and Systems. Human and Environment Friendly Robots with High Intelligence and Emotional Quotients (Cat. No.99CH36289)},
doi = {10.1109/IROS.1999.813023},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Matsumoto et al. - 1999 - Visual navigation using omnidirectional view sequence.pdf:pdf},
isbn = {0-7803-5184-3},
pages = {317--322},
publisher = {IEEE},
title = {{Visual navigation using omnidirectional view sequence}},
url = {http://ieeexplore.ieee.org/document/813023/},
volume = {1},
year = {1999}
}
@inproceedings{Doty1994,
author = {Doty, Keith L and Seed, Steven Louis},
booktitle = {Proc. MLCCOLT Workshop on Robot Learning},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Doty, Seed - 1994 - Autonomous Agent Map Construction in Unknown Enclosed Environments.pdf:pdf},
pages = {47--55},
publisher = {Citeseer},
title = {{Autonomous Agent Map Construction in Unknown Enclosed Environments}},
year = {1994}
}
@article{Lippiello2011,
abstract = {A new vision-based obstacle avoidance technique for indoor navigation of Micro Aerial Vehicles (MAVs) is presented in this paper. The vehicle trajectory is modified according to the obstacles detected through the Depth Map of the surrounding environment, which is computed online using the Optical Flow provided by a single onboard omnidirectional camera. An existing closed-form solution for the absolute-scale velocity estimation based on visual correspondences and inertial measurements is generalized and here employed for the Depth Map estimation. Moreover, a dynamic region-of-interest for image features extraction and a self-limitation control for the navigation velocity are proposed to improve safety in view of the estimated vehicle velocity. The proposed solutions are validated by means of simulations.},
author = {Lippiello, Vincenzo and Loianno, Giuseppe and Siciliano, Bruno},
doi = {10.1109/CDC.2011.6160577},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Lippiello, Loianno, Siciliano - 2011 - MAV indoor navigation based on a closed-form solution for absolute scale velocity estimation usin.pdf:pdf},
isbn = {9781612848006},
issn = {01912216},
journal = {Proceedings of the IEEE Conference on Decision and Control},
pages = {3566--3571},
title = {{MAV indoor navigation based on a closed-form solution for absolute scale velocity estimation using Optical Flow and inertial data}},
year = {2011}
}
@inproceedings{Filliat2007,
author = {Filliat, David},
booktitle = {Proceedings 2007 IEEE International Conference on Robotics and Automation},
doi = {10.1109/ROBOT.2007.364080},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Filliat - 2007 - A visual bag of words method for interactive qualitative localization and mapping.pdf:pdf},
isbn = {1-4244-0602-1},
issn = {1050-4729},
month = {apr},
number = {April},
pages = {3921--3926},
publisher = {IEEE},
title = {{A visual bag of words method for interactive qualitative localization and mapping}},
url = {http://ieeexplore.ieee.org/document/4209698/},
year = {2007}
}
@article{Zhang2009,
author = {Zhang, Alan M and Kleeman, Lindsay},
doi = {10.1177/0278364908098412},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Zhang, Kleeman - 2009 - Robust Appearance Based Visual Route Following for Navigation in Large-scale Outdoor Environments.pdf:pdf},
journal = {The International Journal of Robotics Research},
keywords = {appearance-based,mobile robot,navigation,outdoor,panoramic vision,re-,route following,route repeat},
number = {3},
pages = {331--356},
title = {{Robust Appearance Based Visual Route Following for Navigation in Large-scale Outdoor Environments}},
volume = {28},
year = {2009}
}
@article{Horn1981,
abstract = {Optical flow cannot be computed locally, since only one independent measurement is available from the image sequence at a point, while the flow velocity has two components. A second constraint is needed. A method for finding the optical flow pattern is presented which assumes that the apparent velocity of the brightness pattern varies smoothly almost everywhere in the image. An iterative implementation is shown which successfully computes the optical flow for a number of synthetic image sequences. The algorithm is robust in that it can handle image sequences that are quantized rather coarsely in space and time. It is also insensitive to quantization of brightness levels and additive noise. Examples are included where the assumption of smoothness is violated at singular points or along lines in the image. ?? 1981.},
author = {Horn, Berthold K P and Schunck, Brian G.},
doi = {10.1016/0004-3702(81)90024-2},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Horn, Schunck - 1981 - Determining optical flow.pdf:pdf},
isbn = {0867204524},
issn = {00043702},
journal = {Artificial Intelligence},
number = {1-3},
pages = {185--203},
pmid = {14765965},
title = {{Determining optical flow}},
volume = {17},
year = {1981}
}
@article{Andreasson2007,
abstract = {This paper presents a vision-based approach to SLAM in large-scale environments with minimal sensing and computational requirements. The approach is based on a graphical representation of robot poses and links between the poses. Links between the robot poses are established based on odomety and image similarity, then a relaxation algorithm is used to generate a globally consistent map. To estimate the covariance matrix for links obtained from the vision sensor, a novel method is introduced based on the relative similarity of neighbouring images, without requiring distances to image features or multiple view geometry. Indoor and outdoor experiments demonstrate that the approach scales well to large-scale environments, producing topologically correct and geometrically accurate maps at minimal computational cost. Mini-SLAM was found to produce consistent maps in an unstructured, large-scale environment (the total path length was 1.4 km) containing indoor and outdoor passages.},
author = {Andreasson, Henrik and Duckett, Tom and Lilienthal, Achim},
doi = {10.1109/ROBOT.2007.364108},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Andreasson, Duckett, Lilienthal - 2007 - Mini-SLAM Minimalistic visual SLAM in large-scale environments based on a new interpretation of.pdf:pdf},
isbn = {1424406021},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
keywords = {highlight},
mendeley-tags = {highlight},
number = {April},
pages = {4096--4101},
title = {{Mini-SLAM: Minimalistic visual SLAM in large-scale environments based on a new interpretation of image similarity}},
year = {2007}
}
@inproceedings{Liu2017,
author = {Liu, Peidong and Heng, Lionel and Sattler, Torsten and Geiger, Andreas and Pollefeys, Marc},
booktitle = {International Conference on Intelligent Robots and Systems (IROS)},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Liu et al. - 2017 - Direct Visual Odometry for a Fisheye-Stereo Camera.pdf:pdf},
title = {{Direct Visual Odometry for a Fisheye-Stereo Camera}},
year = {2017}
}
@article{Franz1998a,
abstract = {We present a purely vision-based scheme for learning a topological representation of an open environment. The system represents selected places by local views of the surrounding scene, and finds traversable paths between them. The set of recorded views and their connections are combined into a graph model of the environment. To navigate between views connected in the graph, we employ a homing strategy inspired by findings of insect ethology. In robot experiments, we demonstrate that complex visual exploration and navigation tasks can thus be performed without using metric information.},
author = {Franz, M O and Sch{\"{o}}lkopf, B and Mallot, H A and B{\"{u}}lthoff, H H},
doi = {10.1145/267658.267687},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Franz et al. - 1998 - Learning view graphs for robot navigation.pdf:pdf},
isbn = {0897918770},
issn = {09295593},
journal = {Autonomous Robots},
keywords = {cognitive maps,environment modeling,exploration,mobile,omnidirectional sensor,robots,topological maps,visual navigation},
number = {1},
pages = {111--125},
title = {{Learning view graphs for robot navigation}},
url = {http://portal.acm.org/citation.cfm?doid=267658.267687},
volume = {5},
year = {1998}
}
@article{Weiss2015,
abstract = {In this paper, we describe a novel method using only optical flow from a single camera and inertial information to quickly initialize, deploy, and autonomously stabilize an inherently unstable aerial vehicle. Our approach requires a minimal number of tracked features in only two consecutive frames and inertial readings eliminating the need of long feature tracks or local maps and rendering it inherently failsafe. We show theoretically, in simulation, and in real experiments that we can reliably estimate and control the vehicle velocity, full attitude, and metric distance to the scene while self-calibrating inertial intrinsics and sensor extrinsics. In fact, the fast initialization, self-calibration, and inherent fail-safe property leads to the first visual-inertial throw-and-go capable system.},
author = {Weiss, Stephan and Brockers, Roland and Albrektsen, Sigurd and Matthies, Larry},
doi = {10.1109/WACV.2015.42},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Weiss et al. - 2015 - Inertial optical flow for throw-and-go micro air vehicles.pdf:pdf},
isbn = {9781479966820},
journal = {Proceedings - 2015 IEEE Winter Conference on Applications of Computer Vision, WACV 2015},
pages = {262--269},
title = {{Inertial optical flow for throw-and-go micro air vehicles}},
year = {2015}
}
@article{Mur-Artal2015,
abstract = {The gold standard method for tridimensional reconstruction and camera localization from a set of images is well known to be Bundle Adjustment (BA). Although BA was regarded for years as a costly method restricted to the offline domain, several real time algorithms based on BA flourished in the last decade. However those algorithms were limited to perform SLAM in small scenes or only Visual Odometry. In this work we built on excellent algorithms of the last years to design from scratch a Monocular SLAM system that operates in real time, in small and large, indoor and outdoor environments, with the capability of wide baseline loop closing and relocalization, and including full automatic initialization. Our survival of the fittest approach to select the points and keyframes of the reconstruction generates a compact and trackable map that only grows if the scene content changes, enhancing lifelong operation. We present an exhaustive evaluation in 27 sequences from the most popular datasets achieving unprecedented performance with a typical localization accuracy from 0.2{\%} to 1{\%} of the trajectory dimension in scenes from a desk to several city blocks. We make public a ROS implementation.},
archivePrefix = {arXiv},
arxivId = {1502.00956},
author = {Mur-Artal, Raul and Montiel, JMM M M and Tardos, Juan D},
doi = {10.1109/TRO.2015.2463671},
eprint = {1502.00956},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Mur-Artal, Montiel, Tardos - 2015 - ORB-SLAM A Versatile and Accurate Monocular SLAM System.pdf:pdf},
isbn = {1552-3098},
issn = {1552-3098},
journal = {IEEE Transactions on Robotics},
month = {oct},
number = {5},
pages = {1147--1163},
title = {{ORB-SLAM: A Versatile and Accurate Monocular SLAM System}},
url = {http://ieeexplore.ieee.org/document/7219438/},
volume = {31},
year = {2015}
}
@article{Goldhoorn2007,
abstract = {Several methods can be used for a robot to return to a previously visited position. In our approach we use the average landmark vector method to calculate a homing vector which should point the robot to the destination. This approach was tested in a simulated environment, where panoramic projections of features were used. To evaluate the robustness of the method, several parameters of the simulation were changed such as the length of the walls and the number of features, and also several disturbance factors were added to the simulation such as noise and occlusion. The simulated robot performed really well. Randomly removing 50{\%} of the features resulted in a mean of 85{\%} successful runs. Even adding more than 100{\%} fake features did not have any significant result on the performance. {\textcopyright} 2007 The authors and IOS Press. All rights reserved.},
author = {Goldhoorn, Alex and Ramisa, Arnau and {De M{\'{a}}ntaras}, Ram{\'{o}}n L{\'{o}}pez and Toledo, Ricardo},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Goldhoorn et al. - 2007 - Using the average landmark vector method for robot homing.pdf:pdf},
isbn = {9781586037987},
issn = {09226389},
journal = {Frontiers in Artificial Intelligence and Applications},
keywords = {average landmark vector,invariant features,mobile robot homing},
pages = {331--338},
title = {{Using the average landmark vector method for robot homing}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-70549112531{\&}partnerID=tZOtx3y1{\%}5Cnhttp://www.scopus.com/inward/record.url?eid=2-s2.0-70549112531{\&}partnerID=40{\&}md5=edf27c23cf64d2c8163c2ec716d8ffec},
volume = {163},
year = {2007}
}
@article{DeSouza2002,
abstract = {This paper surveys the developments of the last 20 years in the area of vision for mobile robot navigation. Two major components of the paper deal with indoor navigation and outdoor navigation. For each component, we have further subdivided our treatment of the subject on the basis of structured and unstructured environments. For indoor robots in structured environments, we have dealt separately with the cases of geometrical and topological models of space. For unstructured environments, we have discussed the cases of navigation using optical flows, using methods from the appearance-based paradigm, and by recognition of specific objects in the environment.},
author = {DeSouza, Guilherme N. and Kak, Avinash C.},
doi = {10.1109/34.982903},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/DeSouza, Kak - 2002 - Vision for Mobile Robot Navigation A Survey.pdf:pdf},
isbn = {0162-8828},
issn = {0162-8828},
journal = {Pattern Analysis and Machine Intelligence},
keywords = {Cameras,Computer vision,Geometrical optics,Image motion analysis,Mobile robotics,Mobile robots,Navigation,Neural networks,Orbital robotics,Robot vision systems,Solid modeling,appearance-based paradigm,computer vision,geometrical models,image sequences,indoor navigation,indoor robots,mobile robot navigation,mobile robots,navigation,nieuwe referenties,object recognition,objects recognition,optical flows,outdoor navigation,path planning,slam,structured environments,survey,topological models,unstructured environments,voxel map},
mendeley-tags = {nieuwe referenties,slam,survey,voxel map},
number = {2},
pages = {237--267},
title = {{Vision for Mobile Robot Navigation : A Survey}},
volume = {24},
year = {2002}
}
@article{Steckel2013,
abstract = {We propose to combine a biomimetic navigation model which solves a simultaneous localization and mapping task with a biomimetic sonar mounted on a mobile robot to address two related questions. First, can robotic sonar sensing lead to intelligent interactions with complex environments? Second, can we model sonar based spatial orientation and the construction of spatial maps by bats? To address these questions we adapt the mapping module of RatSLAM, a previously published navigation system based on computational models of the rodent hippocampus. We analyze the performance of the proposed robotic implementation operating in the real world. We conclude that the biomimetic navigation model operating on the information from the biomimetic sonar allows an autonomous agent to map unmodified (office) environments efficiently and consistently. Furthermore, these results also show that successful navigation does not require the readings of the biomimetic sonar to be interpreted in terms of individual objects/landmarks in the environment. We argue that the system has applications in robotics as well as in the field of biology as a simple, first order, model for sonar based spatial orientation and map building.},
author = {Steckel, Jan and Peremans, Herbert},
doi = {10.1371/journal.pone.0054076},
editor = {Ouzounis, Christos A.},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Steckel, Peremans - 2013 - BatSLAM Simultaneous Localization and Mapping Using Biomimetic Sonar.pdf:pdf},
issn = {1932-6203},
journal = {PLoS ONE},
keywords = {biology,slam},
mendeley-tags = {biology,slam},
month = {jan},
number = {1},
pages = {e54076},
title = {{BatSLAM: Simultaneous Localization and Mapping Using Biomimetic Sonar}},
url = {http://dx.plos.org/10.1371/journal.pone.0054076},
volume = {8},
year = {2013}
}
@article{Griffiths2006,
abstract = {Despite the tremendous potential demonstrated by miniature aerial vehicles (MAV) in numerous applications, they are currently limited to operations in open air space, far away from obstacles and terrain. To broaden the range of applications for MAVs, methods to enable operation in environments of increased complexity must be developed. In this article, we presented two strategies for obstacle and terrain avoidance that provide a means for avoiding obstacles in the flight path and for staying centered in a winding corridor. Flight tests have validated the feasibility of these approaches and demonstrated promise for further refinement},
author = {Griffiths, Stephen and Saunders, Jeff and Curtis, Andrew and Barber, Blake and McLain, Tim and Bread, Randy},
doi = {10.1109/MRA.2006.1678137},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Griffiths et al. - 2006 - Maximizing miniature aerial vehicles.pdf:pdf},
isbn = {1070-9932},
issn = {10709932},
journal = {IEEE Robotics and Automation Magazine},
keywords = {Autonomous flight,Miniature aerial vehicle,Obstacle avoidance,Terrain navigation},
number = {3},
pages = {34--43},
title = {{Maximizing miniature aerial vehicles}},
volume = {13},
year = {2006}
}
@article{Morel2009,
abstract = {If a physical object has a smooth or piecewise smooth boundary, its images obtained by cameras in varying positions undergo smooth apparent deformations. These deformations are locally well approximated by aﬃne transforms of the image plane. In consequence the solid object recognition problem has often been led back to the computation of aﬃne invariant image local features. Such invariant features could be obtained by normalization methods, but no fully aﬃne normalization method exists for the time being. Even scale invariance is only dealt with rigorously by the {\{}SIFT{\}} method. By simulating zooms out and normalizing translation and rotation, {\{}SIFT{\}} is invariant to four out of the six parameters of an aﬃne transform. The method proposed in this paper, {\{}Aﬃne-SIFT{\}} ({\{}ASIFT{\}}), simulates all image views obtainable by varying the two camera axis orientation parameters, namely the latitude and the longitude angles, left over by the {\{}SIFT{\}} method. Then it covers the other four parameters by using the {\{}SIFT{\}} method itself. The resulting method will be mathematically proved to be fully aﬃne invariant. Against any prognosis, simulating all views depending on the two camera orientation parameters is feasible with no dramatic computational load. A two-resolution scheme further reduces the {\{}ASIFT{\}} complexity to about twice that of {\{}SIFT{\}}. A new notion, the transition tilt, measuring the amount of distortion from one view to another is introduced. While an absolute tilt from a frontal to a slanted view exceeding 6 is rare, much higher transition tilts are common when two slanted views of an object are compared (see Fig. 1.1). The attainable transition tilt is measured for each aﬃne image comparison method. The new method permits to reliably identify features that have undergone transition tilts of large magnitude, up to 36 and higher. This fact is substantiated by many experiments which show that {\{}ASIFT{\}} outperforms signiﬁcantly the state-of-the-art methods {\{}SIFT{\}}, {\{}MSER{\}}, {\{}Harris-Aﬃne{\}}, and {\{}Hessian-Aﬃne{\}}.},
author = {Morel, Jean-Michel and Yu, Guoshen},
doi = {10.1137/080732730},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Morel, Yu - 2009 - ASIFT A New Framework for Fully Affine Invariant Image Comparison.pdf:pdf},
isbn = {1936-4954},
issn = {1936-4954},
journal = {SIAM Journal on Imaging Sciences},
keywords = {080732730,10,1137,68t10,68t40,68t45,93c85,ASIFT,affine invariance,affine normalization,ams subject classifications,descriptors,doi,feature transform,image matching,scale invariance,scale-invariant,sift},
mendeley-tags = {ASIFT},
number = {2},
pages = {438--469},
title = {{ASIFT: A New Framework for Fully Affine Invariant Image Comparison}},
volume = {2},
year = {2009}
}
@article{Poggi2018,
archivePrefix = {arXiv},
arxivId = {arXiv:1806.11430v3},
author = {Poggi, Matteo and Aleotti, Filippo and Tosi, Fabio and Mattoccia, Stefano and Jul, C V},
eprint = {arXiv:1806.11430v3},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Poggi et al. - 2018 - Towards real-time unsupervised monocular depth estimation on CPU.pdf:pdf},
journal = {arXiv preprint arXiv:1806.11430},
title = {{Towards real-time unsupervised monocular depth estimation on CPU}},
year = {2018}
}
@article{Scaramuzza2011a,
author = {Scaramuzza, Davide and Fraundorfer, Friedrich},
doi = {10.1109/MRA.2011.943233},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Scaramuzza, Fraundorfer - 2011 - Visual Odometry Part I The First 30 Years and Fundamentals.pdf:pdf},
issn = {1070-9932},
journal = {IEEE Robotics {\&} Automation Magazine},
keywords = {state estimation,survey},
mendeley-tags = {state estimation,survey},
month = {dec},
number = {4},
pages = {80--92},
title = {{Visual Odometry Part I: The First 30 Years and Fundamentals}},
url = {http://ieeexplore.ieee.org/document/6096039/},
volume = {18},
year = {2011}
}
@article{Karpathy2015,
abstract = {Recurrent Neural Networks (RNNs), and specifically a variant with Long Short-Term Memory (LSTM), are enjoying renewed interest as a result of successful applications in a wide range of machine learning problems that involve sequential data. However, while LSTMs provide exceptional results in practice, the source of their performance and their limitations remain rather poorly understood. Using character-level language models as an interpretable testbed, we aim to bridge this gap by providing an analysis of their representations, predictions and error types. In particular, our experiments reveal the existence of interpretable cells that keep track of long-range dependencies such as line lengths, quotes and brackets. Moreover, our comparative analysis with finite horizon n-gram models traces the source of the LSTM improvements to long-range structural dependencies. Finally, we provide analysis of the remaining errors and suggests areas for further study.},
archivePrefix = {arXiv},
arxivId = {1506.02078},
author = {Karpathy, Andrej and Johnson, Justin and Fei-Fei, Li},
doi = {10.1007/978-3-319-10590-1_53},
eprint = {1506.02078},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Karpathy, Johnson, Fei-Fei - 2015 - Visualizing and Understanding Recurrent Networks.pdf:pdf},
isbn = {978-3-319-10589-5},
issn = {978-3-319-10589-5},
pages = {1--12},
pmid = {26353135},
title = {{Visualizing and Understanding Recurrent Networks}},
url = {http://arxiv.org/abs/1506.02078},
year = {2015}
}
@article{Palossi2018,
abstract = {Flying in dynamic, urban, highly-populated environments represents an open problem in robotics. State-of-the-art (SoA) autonomous Unmanned Aerial Vehicles (UAVs) employ advanced computer vision techniques based on computationally expensive algorithms, such as Simultaneous Localization and Mapping (SLAM) or Convolutional Neural Networks (CNNs) to navigate in such environments. In the Internet-of-Things (IoT) era, nano-size UAVs capable of autonomous navigation would be extremely desirable as self-aware mobile IoT nodes. However, autonomous flight is considered unaffordable in the context of nano-scale UAVs, where the ultra-constrained power envelopes of tiny rotor-crafts limit the on-board computational capabilities to low-power microcontrollers. In this work, we present the first vertically integrated system for fully autonomous deep neural network-based navigation on nano-size UAVs. Our system is based on GAP8, a novel parallel ultra-low-power computing platform, and deployed on a 27 g commercial, open-source CrazyFlie 2.0 nano-quadrotor. We discuss a methodology and software mapping tools that enable the SoA CNN presented in [1] to be fully executed on-board within a strict 12 fps real-time constraint with no compromise in terms of flight results, while all processing is done with only 94 mW on average - 1{\%} of the power envelope of the deployed nano-aircraft.},
archivePrefix = {arXiv},
arxivId = {1805.01831},
author = {Palossi, Daniele and Loquercio, Antonio and Conti, Francesco and Flamand, Eric and Scaramuzza, Davide and Benini, Luca},
eprint = {1805.01831},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Palossi et al. - 2018 - Ultra Low Power Deep-Learning-powered Autonomous Nano Drones.pdf:pdf},
pages = {1--12},
title = {{Ultra Low Power Deep-Learning-powered Autonomous Nano Drones}},
url = {http://arxiv.org/abs/1805.01831},
year = {2018}
}
@inproceedings{Garg2016,
address = {Cham},
author = {Garg, Ravi and Kumar, Vijay B.G. and Carneiro, Gustavo and Reid, Ian},
booktitle = {European Conference on Computer Vision},
doi = {10.1007/978-3-319-46484-8},
editor = {Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Garg et al. - 2016 - Unsupervised CNN for Single View Depth Estimation Geometry to the Rescue.pdf:pdf},
isbn = {9783319464848},
pages = {740--756},
publisher = {Springer International Publishing},
title = {{Unsupervised CNN for Single View Depth Estimation: Geometry to the Rescue}},
year = {2016}
}
@article{Clement2017,
abstract = {Visual Teach and Repeat (VT{\&}R) allows an autonomous vehicle to accurately repeat a previously traversed route using only vision sensors. Most VT{\&}R systems rely on natively three-dimensional (3D) sensors such as stereo cameras for mapping and localization, but many existing mobile robots are equipped with only 2D monocular vision, typically for teleoperation. In this paper, we extend VT{\&}R to the most basic sensor configuration—a single monocular camera. We show that kilometer-scale route repetition can be achieved with centimeter-level accuracy by approximating the local ground surface near the vehicle as a plane with some uncertainty. This allows our system to recover absolute scale from the known position and orientation of the camera relative to the vehicle, which simplifies threshold-based outlier rejection and the estimation and control of lateral path-tracking error—essential components of high-accuracy route repetition. We enhance the robustness of our monocular VT{\&}R system to common failure cases through the use of color-constant imagery, which provides it with a degree of resistance to lighting changes and moving shadows where keypoint matching on standard gray images tends to struggle. Through extensive testing on a combined 30 km of autonomous navigation data collected on multiple vehicles in a variety of highly nonplanar terrestrial and planetary-analogue environments, we demonstrate that our system is capable of achieving route-repetition accuracy on par with its stereo counterpart, with only a modest tradeoff in robustness.},
archivePrefix = {arXiv},
arxivId = {1707.08989},
author = {Clement, Lee and Kelly, Jonathan and Barfoot, Timothy D.},
doi = {10.1002/rob.21655},
eprint = {1707.08989},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Clement, Kelly, Barfoot - 2017 - Robust Monocular Visual Teach and Repeat Aided by Local Ground Planarity and Color-constant Imagery.pdf:pdf},
isbn = {9783319277004},
issn = {15564967},
journal = {Journal of Field Robotics},
number = {1},
pages = {74--97},
title = {{Robust Monocular Visual Teach and Repeat Aided by Local Ground Planarity and Color-constant Imagery}},
volume = {34},
year = {2017}
}
@phdthesis{Roben2009,
author = {R{\"{o}}ben, Frank},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/R{\"{o}}ben - 2009 - Biologically Inspired Visual Navigation in Indoor and Outdoor Environments.pdf:pdf},
number = {September},
school = {Universit{\"{a}}t Bielefeld},
title = {{Biologically Inspired Visual Navigation in Indoor and Outdoor Environments}},
year = {2009}
}
@article{Bay2006,
abstract = {In this paper, we present a novel scale-and rotation-invariant interest point detector and descriptor, coined SURF (Speeded Up Ro-bust Features). It approximates or even outperforms previously proposed schemes with respect to repeatability, distinctiveness, and robustness, yet can be computed and compared much faster. This is achieved by relying on integral images for image convolutions; by building on the strengths of the leading existing detectors and descrip-tors (in casu, using a Hessian matrix-based measure for the detector, and a distribution-based descriptor); and by simplifying these methods to the essential. This leads to a combination of novel detection, description, and matching steps. The paper presents experimental results on a standard evaluation set, as well as on imagery obtained in the context of a real-life object recognition application. Both show SURF's strong performance.},
author = {Bay, Herbert and Ess, Andreas and Tuytelaars, Tinne and {Van Gool}, Luc},
doi = {10.1016/j.cviu.2007.09.014},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Bay et al. - 2008 - Speeded-Up Robust Features (SURF).pdf:pdf},
issn = {10773142},
journal = {Computer Vision and Image Understanding},
keywords = {SURF,U-SURF},
mendeley-tags = {SURF,U-SURF},
month = {jun},
number = {3},
pages = {346--359},
title = {{Speeded-Up Robust Features (SURF)}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S1077314207001555},
volume = {110},
year = {2008}
}
@article{Epstein1966a,
author = {Epstein, William},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Epstein - 1966 - Perceived Depth as a Function of Relative Height under Three Background Conditions.pdf:pdf},
journal = {Journal of Experimental Psychology},
number = {3},
pages = {335--338},
title = {{Perceived Depth as a Function of Relative Height under Three Background Conditions}},
volume = {72},
year = {1966}
}
@article{Menegatti2004,
author = {Menegatti, Emanuele and Maeda, Takeshi and Ishiguro, Hiroshi},
doi = {10.1016/j.robot.2004.03.014},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Menegatti, Maeda, Ishiguro - 2004 - Image-based memory for robot navigation using properties of omnidirectional images.pdf:pdf},
issn = {09218890},
journal = {Robotics and Autonomous Systems},
keywords = {DFT,fourier transform,hierarchical localisation,image-based navigation,mobile robot,omnidirectional vision},
mendeley-tags = {DFT},
number = {4},
pages = {251--267},
title = {{Image-based memory for robot navigation using properties of omnidirectional images}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0921889004000582},
volume = {47},
year = {2004}
}
@article{Montemerlo2002,
abstract = {The ability to simultaneously localize a robot and accurately map its surroundings is considered by many to be a key prerequisite of truly autonomous robots. However, few approaches to this problem scale up to handle the very large number of landmarks present in real environments. Kalman filter-based algorithms, for example, require time quadratic in the number of landmarks to incorporate each sensor observation. This paper presents FastSLAM, an algorithm that recursively estimates the full posterior distribution over robot pose and landmark locations, yet scales logarithmically with the number of landmarks in the map. This algorithm is based on a factorization of the posterior into a product of conditional landmark distributions and a distribution over robot paths. The algorithm has been run successfully on as many as 50,000 landmarks, environments far beyond the reach of previous approaches. Experimental results demonstrate the advantages and limitations of the FastSLAM algorithm on both simulated and real-world data.},
author = {Montemerlo, Michael and Thrun, Sebastian and Koller, Daphne and Wegbreit, Ben},
doi = {10.1.1.16.2153},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Montemerlo et al. - 2002 - FastSLAM A factored solution to the simultaneous localization and mapping problem.pdf:pdf},
isbn = {0262511290},
journal = {Proc. of 8th National Conference on Artificial Intelligence/14th Conference on Innovative Applications of Artificial Intelligence},
keywords = {FastSLAM},
mendeley-tags = {FastSLAM},
number = {2},
pages = {593--598},
title = {{FastSLAM: A factored solution to the simultaneous localization and mapping problem}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:FastSLAM+:+A+Factored+Solution+to+the+Simultaneous+Localization+and+Mapping+Problem{\#}0},
volume = {68},
year = {2002}
}
@article{Fraundorfer2007,
abstract = {In this paper we present a highly scalable vision-based localization and mapping method using image collections. A topological world representation is created online during robot exploration by adding images to a database and maintaining a link graph. An efficient image matching scheme allows real-time mapping and global localization. The compact image representation allows us to create image collections containing millions of images, which enables mapping of very large environments. A path planning method using graph search is proposed and local geometric information is used to navigate in the topological map. Experiments show the good performance of the image matching for global localization and demonstrate path planning and navigation.},
author = {Fraundorfer, Friedrich and Engels, Christopher and Nister, David},
doi = {10.1109/IROS.2007.4399123},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Fraundorfer, Engels, Nister - 2007 - Topological mapping, localization and navigation using image collections.pdf:pdf},
isbn = {978-1-4244-0911-2},
journal = {2007 IEEE/RSJ International Conference on Intelligent Robots and Systems},
pages = {3872--3877},
pmid = {1507832357605702075},
title = {{Topological mapping, localization and navigation using image collections}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4399123},
year = {2007}
}
@incollection{Sabour2017,
author = {Sabour, Sara and Frosst, Nicholas and Hinton, Geoffrey E.},
booktitle = {Advances in Neural Information Processing Systems 30},
editor = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Viswanathan, S. and Garnett, R.},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Sabour, Frosst, Hinton - 2017 - Dynamic Routing Between Capsules.pdf:pdf},
number = {Nips},
pages = {3856----3866},
publisher = {Curran Associates, Inc.},
title = {{Dynamic Routing Between Capsules}},
url = {http://papers.nips.cc/paper/6975-dynamic-routing-between-capsules.pdf},
year = {2017}
}
@inproceedings{Ko2016,
author = {Ko, Hanseok and Park, Sangwook and Mun, Seongkyu},
booktitle = {22nd International Congress on Acoustics},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Ko, Park, Mun - 2016 - Robust acoustics and speech perception of aerial robot under ego noise for scene understanding.pdf:pdf},
keywords = {aerial robot,ego noise,scene understanding,speech recognition},
title = {{Robust acoustics and speech perception of aerial robot under ego noise for scene understanding}},
year = {2016}
}
@article{Liu2014a,
abstract = {In this paper, we present a novel omni-total variation (Omni-TV) algorithm for the restoration of defocus blur to obtain all-focused catadioptric image. Catadioptric omni-directional imaging systems usually consist of conventional cameras and curved mirrors for capturing 360°field of view. Mirror curvature in the catadioptric camera often leads to noticeable blurring artifacts in omni-directional imaging. The problem becomes more severe when high resolution sensor is introduced. In an omni-directional image, two points near each other may not be close to one another in the 3D scene. Traditional gradient computation cannot be directly applied to omni-directional image processing. Thus, an omni-gradient computing method combined with the characteristics of catadioptric imaging is proposed, in which an Omni-TV minimization is used as the constraint for deconvolution regularization. The proposed method is vital for improving catadioptric omni-directional imaging quality and promoting applications in related fields like omni-directional video and image processing. {\textcopyright} 2014 Elsevier GmbH. All rights reserved.},
author = {Liu, Yu and Li, Yongle and Lou, Jingtao and Wang, Wei and Zhang, Maojun},
doi = {10.1016/j.ijleo.2014.01.068},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Liu et al. - 2014 - Omni-total variation algorithm for the restoration of all-focused catadioptric image.pdf:pdf},
issn = {00304026},
journal = {Optik},
keywords = {Catadioptric optical system,Omni-total variation,Omnidirectional image defocus deblurring},
number = {14},
pages = {3685--3689},
publisher = {Elsevier GmbH.},
title = {{Omni-total variation algorithm for the restoration of all-focused catadioptric image}},
url = {http://dx.doi.org/10.1016/j.ijleo.2014.01.068},
volume = {125},
year = {2014}
}
@article{Mujumdar2011,
author = {Mujumdar, Anusha and Padhi, Radhakant},
doi = {10.2514/1.49985},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Mujumdar, Padhi - 2011 - Evolving Philosophies on Autonomous ObstacleCollision Avoidance of Unmanned Aerial Vehicles.pdf:pdf},
journal = {JOURNAL OF AEROSPACE COMPUTING, INFORMATION,AND COMMUNICATION},
number = {February},
title = {{Evolving Philosophies on Autonomous Obstacle/Collision Avoidance of Unmanned Aerial Vehicles}},
volume = {8},
year = {2011}
}
@article{Shah2017,
abstract = {Developing and testing algorithms for autonomous vehicles in real world is an expensive and time consuming process. Also, in order to utilize recent advances in machine intelligence and deep learning we need to collect a large amount of annotated training data in a variety of conditions and environments. We present a new simulator built on Unreal Engine that offers physically and visually realistic simulations for both of these goals. Our simulator includes a physics engine that can operate at a high frequency for real-time hardware-in-the-loop (HITL) simulations with support for popular protocols (e.g. MavLink). The simulator is designed from the ground up to be extensible to accommodate new types of vehicles, hardware platforms and software protocols. In addition, the modular design enables various components to be easily usable independently in other projects. We demonstrate the simulator by first implementing a quadrotor as an autonomous vehicle and then experimentally comparing the software components with real-world flights.},
archivePrefix = {arXiv},
arxivId = {1705.05065},
author = {Shah, Shital and Dey, Debadeepta and Lovett, Chris and Kapoor, Ashish},
doi = {10.1007/978-3-319-67361-5_40},
eprint = {1705.05065},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Shah et al. - 2017 - AirSim High-Fidelity Visual and Physical Simulation for Autonomous Vehicles.pdf:pdf},
pages = {1--14},
title = {{AirSim: High-Fidelity Visual and Physical Simulation for Autonomous Vehicles}},
url = {http://arxiv.org/abs/1705.05065},
year = {2017}
}
@incollection{Miksik2012,
abstract = {Local feature detectors and descriptors are widely used in many computer vision applications and various methods have been proposed during the past decade. There have been a number of evaluations focused on various aspects of local features, matching accuracy in particular, however there has been no comparisons considering the accuracy and speed trade-offs of recent extractors such as BRIEF, BRISK, ORB, MRRID, MROGH and LIOP. This paper provides a performance evaluation of recent feature detectors and compares their matching precision and speed in randomized kd-trees setup as well as an evaluation of binary descriptors with efficient computation of Hamming distance.},
author = {Miksik, Ondrej and Mikolajczyk, K.},
booktitle = {Pattern Recognition (ICPR), 2012 21st International Conference on},
doi = {978-1-4673-2216-4},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Miksik, Mikolajczyk - 2012 - Evaluation of Local Detectors and Descriptors for Fast Feature Matching.pdf:pdf},
isbn = {9784990644109},
issn = {1051-4651},
keywords = {Accuracy,Approximation methods,Artificial neural networks,BRIEF extractor,BRISK extractor,Databases,Detectors,Feature extraction,Hamming distance,LIOP extractor,MROGH extractor,MRRID extractor,ORB extractor,binary descriptor evaluation,computer vision,computer vision applications,feature detector performance evaluation,feature extraction,feature matching,image matching,local feature descriptor evaluation,local feature detector evaluation,matching accuracy,matching precision,matching speed,object detection,random processes,randomized kd-trees,salient features,tree data structures},
mendeley-tags = {salient features},
pages = {2681--2684},
publisher = {IEEE},
title = {{Evaluation of Local Detectors and Descriptors for Fast Feature Matching}},
year = {2012}
}
@article{Zhao2018,
abstract = {This paper introduces a fully deep learning approach to monocular SLAM, which can perform simultaneous localization using a neural network for learning visual odometry (L-VO) and dense 3D mapping. Dense 2D flow and a depth image are generated from monocular images by sub-networks, which are then used by a 3D flow associated layer in the L-VO network to generate dense 3D flow. Given this 3D flow, the dual-stream L-VO network can then predict the 6DOF relative pose and furthermore reconstruct the vehicle trajectory. In order to learn the correlation between motion directions, the Bivariate Gaussian modelling is employed in the loss function. The L-VO network achieves an overall performance of 2.68{\%} for average translational error and 0.0143 deg/m for average rotational error on the KITTI odometry benchmark. Moreover, the learned depth is fully leveraged to generate a dense 3D map. As a result, an entire visual SLAM system, that is, learning monocular odometry combined with dense 3D mapping, is achieved.},
archivePrefix = {arXiv},
arxivId = {1803.02286},
author = {Zhao, Cheng and Sun, Li and Purkait, Pulak and Duckett, Tom and Stolkin, Rustam},
eprint = {1803.02286},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Zhao et al. - 2018 - Learning monocular visual odometry with dense 3D mapping from dense 3D flow.pdf:pdf},
title = {{Learning monocular visual odometry with dense 3D mapping from dense 3D flow}},
url = {http://arxiv.org/abs/1803.02286},
year = {2018}
}
@article{Jaritz2018a,
abstract = {Convolutional neural networks are designed for dense data, but vision data is often sparse (stereo depth, point clouds, pen stroke, etc.). We present a method to handle sparse depth data with optional dense RGB, and accomplish depth completion and semantic segmentation changing only the last layer. Our proposal efficiently learns sparse features without the need of an additional validity mask. We show how to ensure network robustness to varying input sparsities. Our method even works with densities as low as 0.8{\%} (8 layer lidar), and outperforms all published state-of-the-art on the Kitti depth completion benchmark.},
archivePrefix = {arXiv},
arxivId = {1808.00769},
author = {Jaritz, Maximilian and de Charette, Raoul and Wirbel, Emilie and Perrotton, Xavier and Nashashibi, Fawzi},
eprint = {1808.00769},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Jaritz et al. - 2018 - Sparse and Dense Data with CNNs Depth Completion and Semantic Segmentation.pdf:pdf},
journal = {arXiv preprint arXiv:1808.00769},
month = {aug},
title = {{Sparse and Dense Data with CNNs: Depth Completion and Semantic Segmentation}},
year = {2018}
}
@article{Philbeck1997,
author = {Philbeck, John W and Loomis, Jack M},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Philbeck, Loomis - 1997 - Comparison of two indicators of percieved egocentric distance under full cue and reduced-cue conditions.pdf:pdf},
journal = {Journal of Experimental Psychology},
number = {1},
pages = {72--85},
title = {{Comparison of two indicators of percieved egocentric distance under full cue and reduced-cue conditions}},
url = {papers2://publication/uuid/4C90D8F8-61F2-4C5B-ADEB-69D679EA88FC},
volume = {23},
year = {1997}
}
@inproceedings{Lynen2013,
author = {Lynen, Simon and Achtelik, Markus W and Weiss, Stephan and Chli, Margarita and Siegwart, Roland},
booktitle = {Intelligent Robots and Systems (IROS), 2013 IEEE/RSJ International Conference on},
doi = {10.1109/IROS.2013.6696917},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Lynen et al. - 2013 - A Robust and Modular Multi-Sensor Fusion Approach Applied to MAV Navigation.pdf:pdf},
isbn = {9781467363587},
pages = {3923--3929},
publisher = {IEEE},
title = {{A Robust and Modular Multi-Sensor Fusion Approach Applied to MAV Navigation}},
year = {2013}
}
@article{Bohlin2000,
abstract = {Describes an approach to probabilistic roadmap planners (PRMs). The overall theme of the algorithm, called Lazy PRM, is to minimize the number of collision checks performed during planning and hence minimize the running time of the planner. Our algorithm builds a roadmap in the configuration space, whose nodes are the user-defined initial and goal configurations and a number of randomly generated nodes. Neighboring nodes are connected by edges representing paths between the nodes. In contrast with PRMs, our planner initially assumes that all nodes and edges in the roadmap are collision-free, and searches the roadmap at hand for a shortest path between the initial and the goal node. The nodes and edges along the path are then checked for collision. If a collision with the obstacles occurs, the corresponding nodes and edges are removed from the roadmap. Our planner either finds a new shortest path, or first updates the roadmap with new nodes and edges, and then searches for a shortest path. The above process is repeated until a collision-free path is returned. Lazy PRM is tailored to efficiently answer single planning queries, but can also be used for multiple queries. Experimental results presented in the paper show that our lazy method is very efficient in practice},
author = {Bohlin, R and Kavraki, L E},
doi = {10.1109/ROBOT.2000.844107},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Bohlin, Kavraki - 2000 - Path planning using lazy PRM.pdf:pdf},
isbn = {0780358864},
issn = {1050-4729},
journal = {Proceedings 2000 ICRA Millennium Conference IEEE International Conference on Robotics and Automation Symposia Proceedings Cat No00CH37065},
keywords = {motion planning},
mendeley-tags = {motion planning},
number = {April},
pages = {521--528},
title = {{Path planning using lazy PRM}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=844107},
volume = {1},
year = {2000}
}
@article{Menegatti2004a,
author = {Menegatti, Emanuele and Zoccarato, Mauro and Pagello, Enrico and Ishiguro, Hiroshi},
doi = {10.1016/j.robot.2004.05.003},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Menegatti et al. - 2004 - Image-based Monte Carlo localisation with omnidirectional images.pdf:pdf},
issn = {09218890},
journal = {Robotics and Autonomous Systems},
keywords = {fourier signature,fourier transform,image-based navigation,monte carlo localisation,omnidirectional vision},
month = {aug},
number = {1},
pages = {17--30},
title = {{Image-based Monte Carlo localisation with omnidirectional images}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0921889004000739},
volume = {48},
year = {2004}
}
@article{Fragoso2018,
author = {Fragoso, Anthony T},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Fragoso - 2018 - Egospace Motion Planning Representations for Micro Air Vehicles Thesis by.pdf:pdf},
number = {October 2017},
title = {{Egospace Motion Planning Representations for Micro Air Vehicles Thesis by}},
year = {2018}
}
@article{Burri2015,
abstract = {In this work, we present an MAV system that is able to relocalize itself, create consistent maps and plan paths in full 3D in previously unknown environments. This is solely based on vision and IMU measurements with all components running onboard and in real-time. We use visual-inertial odometry to keep the MAV airborne safely locally, as well as for exploration of the environment based on high-level input by an operator. A globally consistent map is constructed in the background, which is then used to correct for drift of the visual odometry algorithm. This map serves as an input to our proposed global planner, which finds dynamic 3D paths to any previously visited place in the map, without the use of teach and repeat algorithms. In contrast to previous work, all components are executed onboard and in real-time without any prior knowledge of the environment.},
author = {Burri, Michael and Oleynikova, Helen and Achtelik, Markus W. and Siegwart, Roland},
doi = {10.1109/IROS.2015.7353622},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Burri et al. - 2015 - Real-time visual-inertial mapping, re-localization and planning onboard MAVs in unknown environments.pdf:pdf},
isbn = {9781479999941},
issn = {21530866},
journal = {IEEE International Conference on Intelligent Robots and Systems},
keywords = {Buildings,Cameras,Helicopters,Inspection,Planning,Real-time systems,Visualization},
pages = {1872--1878},
title = {{Real-time visual-inertial mapping, re-localization and planning onboard MAVs in unknown environments}},
volume = {2015-Decem},
year = {2015}
}
@article{Burri2016,
abstract = {This paper presents visual-inertial datasets collected on-board a micro aerial vehicle. The datasets contain synchronized stereo images, IMU measurements and accurate ground truth. The first batch of datasets facilitates the design and evalua-tion of visual-inertial localization algorithms on real flight data. It was collected in an industrial environment and contains millimeter accurate position ground truth from a laser tracking system. The second batch of datasets is aimed at precise 3D environment reconstruction and was recorded in a room equipped with a motion capture system. The datasets contain 6D pose ground truth and a detailed 3D scan of the environment. Eleven datasets are provided in total, ranging from slow flights under good visual conditions to dynamic flights with motion blur and poor illumination, enabling researchers to thoroughly test and evaluate their algorithms. All datasets contain raw sensor measurements, spatio-temporally aligned sensor data and ground truth, extrinsic and intrinsic calibrations and datasets for custom calibrations.},
archivePrefix = {arXiv},
arxivId = {arXiv:1508.04886v1},
author = {Burri, Michael and Nikolic, Janosch and Gohl, Pascal and Schneider, Thomas and Rehder, Joern and Omari, Sammy and Achtelik, Markus W and Siegwart, Roland},
doi = {10.1177/0278364915620033},
eprint = {arXiv:1508.04886v1},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Burri et al. - 2016 - The EuRoC micro aerial vehicle datasets.pdf:pdf},
isbn = {0037549716666},
issn = {0278-3649},
journal = {The International Journal of Robotics Research},
keywords = {dataset,ground truth,mav,visual-inertial},
number = {10},
pages = {1157--1163},
title = {{The EuRoC micro aerial vehicle datasets}},
url = {http://journals.sagepub.com/doi/10.1177/0278364915620033},
volume = {35},
year = {2016}
}
@article{Matthies1989,
author = {Matthies, L and Kanade, T and Szeliski, R},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Matthies, Kanade, Szeliski - 1989 - Kalman Filtering algorithms for estimating depth from image sequences.pdf:pdf},
pages = {209--236},
title = {{Kalman Filtering algorithms for estimating depth from image sequences}},
volume = {3},
year = {1989}
}
@article{Lee2004,
abstract = {Recent advances in many multi-discipline technologies have allowed small, low-cost fixed wing unmanned air vehicles (UAV) or more complicated unmanned ground vehicles (UGV) to be a feasible solution in many scientific, civil and military applications. Cameras can be mounted on-board of the unmanned vehicles for the purpose of scientific data gathering, surveillance for law enforcement and homeland security, as well as to provide visual information to detect and avoid imminent collisions for autonomous navigation. However, most current computer vision algorithms are highly complex computationally and usually constitute the bottleneck of the guidance and control loop. In this paper, we present a novel computer vision algorithm for collision detection and time-to-impact calculation based on feature density distribution (FDD) analysis. It does not require accurate feature extraction, tracking, or estimation of focus of expansion (FOE). Under a few reasonable assumptions, by calculating the expansion rate of the FDD in space, time-to-impact can be accurately estimated. A sequence of monocular images is studied, and different features are used simultaneously in FDD analysis to show that our algorithm can achieve a fairly good accuracy in collision detection. In this paper we also discuss reactive path planning and trajectory generation techniques that can be accomplished without violating the velocity and heading rate constraints of the UAV.},
annote = {Obstacle detection met edge histograms over image columns en expansion rate (vergelijk met Kimberly's paper).

Obstacle avoidance met constant time-to-impact.},
author = {Lee, Dah-Jye D.-J. and Beard, Randal W. and Merrell, Paul C. and Zhan, Pengcheng},
doi = {10.1117/12.571550},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Lee et al. - 2004 - See and avoidance behaviors for autonomous navigation.pdf:pdf},
isbn = {0277-786X},
issn = {0277786X},
journal = {Proceedings of SPIE - The International Society for Optical Engineering},
keywords = {Feature Density Distribution,Feature Tracking,Reactive Path Planning,Unmanned Air Vehicle,Unmanned Ground Vehicle,expansion rate,experiment,low complexity,nieuwe referenties,obstacle avoidance,obstacle detection,optical flow,reactive obstacle avoidance},
mendeley-tags = {expansion rate,experiment,low complexity,nieuwe referenties,obstacle avoidance,obstacle detection,optical flow,reactive obstacle avoidance},
number = {1},
pages = {23--34},
title = {{See and avoidance behaviors for autonomous navigation}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-17644426016{\&}partnerID=40{\&}md5=fa0f1cc0fbf254be87056d4862618099},
volume = {5609},
year = {2004}
}
@article{Vardy2005,
abstract = {Insects are able to return to important places in their environment by storing an image of the surround- ings while at the goal, and later computing a home direction from a matching between this ‘snapshot' image and the currently perceived image. Very similar ideas are pursued for the visual navigation of mobile robots. A wide range of different solutions for the matching between the two images have been suggested. This paper explores the application of optical flow techniques for visual homing. The performance of five different flow techniques and a reference method is analysed based on image col- lections from three different indoor environments.We show that block matching, two simple variants of block matching and two even simpler differential techniques produce robust homing behaviour, despite the simplicity of the matched features. Our analysis reveals that visual homing can succeed even in the presence of many incorrect feature correspondences, and that low-frequency features are sufficient for homing. In particular, the successful application of differential methods opens newvistas on the visual homing problem, both as plausible and parsimonious models of visual insect navigation, and as a starting point for novel robot navigation methods.},
annote = {NULL},
author = {Vardy, Andrew and M{\"{o}}ller, Ralf},
doi = {10.1080/09540090500140958},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Vardy, M{\"{o}}ller - 2005 - Biologically plausible visual homing methods based on optical flow techniques.pdf:pdf},
isbn = {0954-0091$\backslash$r1360-0494},
issn = {0954-0091},
journal = {Connection Science},
number = {1-2},
pages = {47--89},
pmid = {1000291124},
title = {{Biologically plausible visual homing methods based on optical flow techniques}},
volume = {17},
year = {2005}
}
@article{Hafner2001,
author = {Hafner, V. V.},
doi = {10.1177/10597123010093002},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Hafner - 2001 - Adaptive Homing--Robotic Exploration Tours.pdf:pdf},
isbn = {0041163567},
issn = {1059-7123},
journal = {Adaptive Behavior},
keywords = {alv,hebbian learning,insect navigation,mobile robot,snapshot model,visual homing},
number = {3-4},
pages = {131--141},
title = {{Adaptive Homing--Robotic Exploration Tours}},
url = {http://adb.sagepub.com/cgi/doi/10.1177/10597123010093002},
volume = {9},
year = {2001}
}
@article{Harvey2018,
author = {Harvey, Brendan and O'Young, Siu},
doi = {10.3390/drones2010004},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Harvey, O'Young - 2018 - Acoustic Detection of a Fixed-Wing UAV.pdf:pdf},
issn = {2504-446X},
journal = {Drones},
keywords = {acoustic sensing,collision avoidance,constant false alarm rate,signal enhancement},
number = {1},
pages = {4},
title = {{Acoustic Detection of a Fixed-Wing UAV}},
url = {http://www.mdpi.com/2504-446X/2/1/4},
volume = {2},
year = {2018}
}
@article{Barrows2002,
abstract = {There is increased interest in new classes of mini- and micro-UAVs with sizes ranging from one meter to ten centimeters. Many envisioned applications of such UAVs require them to be able to fly close to the ground in complex environments. The difficulties associated with flying in such environments coupled with the reduced payload capacity of such airframes means that new methods of sensing and control need to be considered. Good models for such methods are found in the world of flying insects. This paper discusses several research efforts aimed at developing new sensing and control algorithms inspired by insect vision and flight behaviors. These efforts are part of DARPA's Controlled Biological and Biomimetic Systems (CBBS) program. In these (and related) efforts, many elegant control stratagems have been discovered which suggest that simple reflexive schemes combined with the measurement of optic flow may be sufficient to provide many aspects of autonomous navigation in complex environments. Furthermore, these efforts are implementing these behaviors in real flying UAV platforms by using novel hardware and software to measure optic flow, and inserting optic flow measurements into a control loop using a combination of “best engineering approaches” with inspiration taken from biology. This has resulted in fixed- and rotary-wing mini-UAVs that are able to hold an altitude and perform terrain following.},
author = {Barrows, Gl and Chahl, Js and Srinivasan, Mandyam},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Barrows, Chahl, Srinivasan - 2002 - Biomimetic visual sensing and flight control.pdf:pdf},
journal = {Proc. Bristol UAV Conf},
keywords = {flight control,mimetic visual sensing and},
pages = {15},
title = {{Biomimetic visual sensing and flight control}},
year = {2002}
}
@book{Cormen2001,
address = {Cambridge},
author = {Cormen, Thomas H. and Leiserson, Charles Eric and Rivest, Ronald L and Stein, Clifford},
publisher = {MIT press},
title = {{Introduction to Algorithms}},
year = {2001}
}
@article{Kumar2018,
abstract = {Near field depth estimation around a self driving car is an important function that can be achieved by four wide angle fisheye cameras having a field of view of over 180. CNN based depth estimation produce state of the art results, but progress is hindered because depth annotation cannot be obtained manually. Synthetic datasets are commonly used but they have limitations. For instance, they do not capture the extensive variability in the appearance of objects like vehicles present in real datasets. There is also a domain shift while performing inference on natural images illustrated by many attempts to handle the domain adaptation explicitly. In this work, we explore an alternate approach of training using sparse LIDAR data as ground truth for depth estimation for fisheye camera. We built our own dataset using our self$\backslash$hyp driving car setup which has a 64 beam Velodyne LIDAR and four wide angle fisheye cameras. LIDAR data projected onto the image plane is sparse and hence viewed as semi supervision for dense depth estimation. To handle the difference in view points of LIDAR and fisheye camera, an occlusion resolution mechanism was implemented. We started with Eigen's multiscale convolutional network architecture and improved by modifying activation function and optimizer. We obtained promising results on our dataset with RMSE errors better than the state of the art results obtained on KITTI because of vast amounts of training data. Our model runs at 20 fps on an embedded platform Nvidia TX2.},
archivePrefix = {arXiv},
arxivId = {1803.06192},
author = {Kumar, Varun Ravi and Milz, Stefan and Simon, Martin and Witt, Christian and Amende, Karl and Petzold, Johannes and Yogamani, Senthil},
eprint = {1803.06192},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Kumar et al. - 2018 - Monocular Fisheye Camera Depth Estimation Using Semi-supervised Sparse Velodyne Data.pdf:pdf},
keywords = {on-board},
mendeley-tags = {on-board},
pages = {1--15},
title = {{Monocular Fisheye Camera Depth Estimation Using Semi-supervised Sparse Velodyne Data}},
url = {http://arxiv.org/abs/1803.06192},
year = {2018}
}
@article{Moller2010a,
abstract = {Warping (Franz et al., Biological Cybernetics 79(3), 191–202, 1998b) and 2D-warping (M{\"{o}}ller, Robotics and Autonomous Systems 57(1), 87–101, 2009) are effective visual homing methods which can be applied for navigation in topological maps. This paper presents several improvements of 2D-warping and introduces two novel free warping methods in the same framework. The free warping methods partially lift the assumption of the original warping method that all landmarks have the same distance from the goal location. Experiments on image databases confirm the effect of the improvements of 2D-warping and show that the two free warping methods produce more precise home vectors and approximately the same proportion of erroneous home vectors. In addition, two novel and easier-to-interpret performance measures for the angular error are introduced.},
author = {M{\"{o}}ller, Ralf and Krzykawski, Martin and Gerstmayr, Lorenz},
doi = {10.1007/s10514-010-9195-y},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/M{\"{o}}ller, Krzykawski, Gerstmayr - 2010 - Three 2D-warping schemes for visual robot navigation.pdf:pdf},
isbn = {0929-5593},
issn = {09295593},
journal = {Autonomous Robots},
keywords = {Image warping,Navigation,Visual homing},
number = {3-4},
pages = {253--291},
pmid = {1000290999},
title = {{Three 2D-warping schemes for visual robot navigation}},
volume = {29},
year = {2010}
}
@article{Gardner2010,
abstract = {The thermal diffusivity of an object is a parameter that controls the rate at which heat is extracted from the hand when it touches that object. It is an important feature for distinguishing materials by means of touch. In order to quantitatively describe the ability of human observers to discriminate between materials on the basis of heat extraction rate, we conducted an experiment in which this heat extraction was performed in a controlled way. In different conditions, subjects were repeatedly asked to select from two stimuli the one that cooled faster. The discrimination threshold was around 43{\%} of the extraction rate. A rate that was twice as slow also yielded twice the absolute threshold. When we halved the temperature difference between the beginning and end of the stimulus, the threshold did not change as much. In separate experiments, we investigated the different cues that were available in the stimulus: initial cooling rate and end temperature. Both cues were used for discrimination, but cooling rate seemed to be the most important.},
author = {Gardner, Jonathan S. and Austerweil, Joseph L. and Palmer, Stephen E.},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Gardner, Austerweil, Palmer - 2010 - Vertical position as a cue to pictorial depth Height in the picture plane versus distance to the ho.pdf:pdf},
journal = {Attention, Perception, {\&} Psychophysics},
number = {2},
pages = {445--453},
title = {{Vertical position as a cue to pictorial depth: Height in the picture plane versus distance to the horizon}},
volume = {72},
year = {2010}
}
@article{Harris1988,
abstract = {Consistency of image edge filtering is of prime importance for 3D interpretation of image sequences using feature tracking algorithms. To cater for image regions containing texture and isolated features, a combined corner and edge detector based on the local auto-correlation function is utilised, and it is shown to perform with good consistency on natural imagery.},
author = {Harris, Chris and Stephens, Mike},
doi = {10.5244/C.2.23},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Harris, Stephens - 1988 - A Combined Corner and Edge Detector.pdf:pdf},
issn = {09639292},
journal = {Procedings of the Alvey Vision Conference 1988},
pages = {147--151},
pmid = {20130988},
title = {{A Combined Corner and Edge Detector}},
url = {http://www.bmva.org/bmvc/1988/avc-88-023.html},
year = {1988}
}
@inbook{Wedel2009,
abstract = {A look at the Middlebury optical flow benchmark [5] reveals that nowadays variational methods yield the most accurate optical flow fields between two image frames. In this work we propose an improvement variant of the original duality based TV-L 1 optical flow algorithm in [31] and provide implementation details. This formulation can preserve discontinuities in the flow field by employing total variation (TV) regularization. Furthermore, it offers robustness against outliers by applying the robust L 1 norm in the data fidelity term.},
address = {Berlin, Heidelberg},
author = {Wedel, Andreas and Pock, Thomas and Zach, Christopher and Bischof, Horst and Cremers, Daniel},
booktitle = {Statistical and Geometrical Approaches to Visual Motion Analysis: International Dagstuhl Seminar, Dagstuhl Castle, Germany, July 13-18, 2008. Revised Papers},
doi = {10.1007/978-3-642-03061-1_2},
editor = {Cremers, Daniel and Rosenhahn, Bodo and Yuille, Alan L and Schmidt, Frank R},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Wedel et al. - 2009 - An Improved Algorithm for TV-L 1 Optical Flow.pdf:pdf},
isbn = {978-3-642-03061-1},
pages = {23--45},
publisher = {Springer Berlin Heidelberg},
title = {{An Improved Algorithm for TV-L 1 Optical Flow}},
url = {https://doi.org/10.1007/978-3-642-03061-1{\_}2},
year = {2009}
}
@inproceedings{Scharstein2001,
author = {Scharstein, D. and Szeliski, R. and Zabih, R.},
booktitle = {Proceedings IEEE Workshop on Stereo and Multi-Baseline Vision (SMBV 2001)},
doi = {10.1109/SMBV.2001.988771},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Scharstein, Szeliski, Zabih - 2001 - A taxonomy and evaluation of dense two-frame stereo correspondence algorithms.pdf:pdf},
isbn = {0-7695-1327-1},
pages = {131--140},
publisher = {IEEE Comput. Soc},
title = {{A taxonomy and evaluation of dense two-frame stereo correspondence algorithms}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=988771},
year = {2001}
}
@article{Schlaile2009,
abstract = {The use of natural features for vision based navigation of an indoor Vertical-Take-Off-and-Landing (VTOL) Micro Aerial Vehicle (MAV) named Air-Quad is presented. Air-Quad is a small four-rotor helicopter developed at the ITE. Such a helicopter needs reliable attitude information. The measurements of the used MEMS gyroscopes and accelerometers are corrupted by strong noise. To be useful, the MEMS sensors have to be part of an integrated navigation system with aiding through complementary sensors like GPS or the computer vision module presented here. In the computer vision module, feature points are detected and tracked through the image sequence. The relative rotation and translation of the camera are estimated using the two-dimensional motion of the feature points. The three-dimensional points in the scene are modeled with the image coordinates of their first sighting and their inverse depths. Only these inverse depths are estimated for the feature points. An efficient sparse bundle adjustment algorithm is used to improve the estimation of the scene structure and the navigation solution. It is shown that the use of the computer vision module greatly improves the navigation solution compared to a solution based only on MEMS sensors. ?? 2009 Elsevier Masson SAS. All rights reserved.},
author = {Schlaile, Christian and Meister, Oliver and Frietsch, Natalie and Ke??ler, Christoph and Wendel, Jan and Trommer, Gert F.},
doi = {10.1016/j.ast.2009.09.001},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Schlaile et al. - 2009 - Using natural features for vision based navigation of an indoor-VTOL MAV.pdf:pdf},
isbn = {1270-9638},
issn = {12709638},
journal = {Aerospace Science and Technology},
keywords = {Kalman filter,Micro aerial vehicle,Sparse bundle adjustment,Vision based navigation},
number = {7},
pages = {349--357},
publisher = {Elsevier Masson SAS},
title = {{Using natural features for vision based navigation of an indoor-VTOL MAV}},
url = {http://dx.doi.org/10.1016/j.ast.2009.09.001},
volume = {13},
year = {2009}
}
@inproceedings{Usenko2016,
author = {Usenko, Vladyslav and Engel, Jakob and St{\"{u}}ckler, J{\"{o}}rg and Cremers, Daniel},
booktitle = {Robotics and Automation (ICRA), 2016 IEEE International Conference on},
doi = {10.1109/ICRA.2016.7487335},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Usenko et al. - 2016 - Direct Visual-Inertial Odometry with Stereo Cameras.pdf:pdf},
isbn = {9781467380263},
pages = {1885--1892},
publisher = {IEEE},
title = {{Direct Visual-Inertial Odometry with Stereo Cameras}},
year = {2016}
}
@article{Afyouni2012,
abstract = {This paper surveys indoor spatial models developed for research fields ranging from mobile robot mapping, to indoor location-based services (LBS), and most recently to context-aware navigation services applied to indoor environments. Over the past few years, several studies have evaluated the potential of spatial models for robot navigation and ubiquitous computing. In this paper we take a slightly different perspective, consid-ering not only the underlying properties of those spatial models, but also to which degree the notion of context can be taken into account when delivering services in indoor environ-ments. Some preliminary recommendations for the development of indoor spatial models are introduced from a context-aware perspective. A taxonomy of models is then presented and assessed with the aim of providing a flexible spatial data model for navigation pur-poses, and by taking into account the context dimensions.},
author = {Afyouni, Imad and Ray, Cyril and Claramunt, Christophe},
doi = {10.5311/JOSIS.2012.4.73},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Afyouni, Ray, Claramunt - 2012 - Spatial models for context-aware indoor navigation systems A survey.pdf:pdf},
journal = {JOURNAL OF SPATIAL INFORMATION SCIENCE},
keywords = {survey},
mendeley-tags = {survey},
pages = {85--123},
title = {{Spatial models for context-aware indoor navigation systems: A survey}},
volume = {4},
year = {2012}
}
@article{Cichy2016,
abstract = {The complex multi-stage architecture of cortical visual pathways provides the neural basis for efficient visual object recognition in humans. However, the stage-wise computations therein remain poorly understood. Here, we compared temporal (magnetoencephalography) and spatial (functional MRI) visual brain representations with representations in an artificial deep neural network (DNN) tuned to the statistics of real-world visual recognition. We showed that the DNN captured the stages of human visual processing in both time and space from early visual areas towards the dorsal and ventral streams. Further investigation of crucial DNN parameters revealed that while model architecture was important, training on real-world categorization was necessary to enforce spatio-temporal hierarchical relationships with the brain. Together our results provide an algorithmically informed view on the spatio-temporal dynamics of visual object recognition in the human visual brain.},
archivePrefix = {arXiv},
arxivId = {1601.02970},
author = {Cichy, Radoslaw Martin and Khosla, Aditya and Pantazis, Dimitrios and Torralba, Antonio and Oliva, Aude},
doi = {10.1038/srep27755},
eprint = {1601.02970},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Cichy et al. - 2016 - Comparison of deep neural networks to spatio-temporal cortical dynamics of human visual object recognition reveals.pdf:pdf},
isbn = {2045-2322 (Electronic)
2045-2322 (Linking)},
issn = {20452322},
journal = {Scientific Reports},
number = {January},
pages = {1--13},
pmid = {27282108},
publisher = {Nature Publishing Group},
title = {{Comparison of deep neural networks to spatio-temporal cortical dynamics of human visual object recognition reveals hierarchical correspondence}},
url = {http://dx.doi.org/10.1038/srep27755},
volume = {6},
year = {2016}
}
@article{Zhan2018,
abstract = {Despite learning based methods showing promising results in single view depth estimation and visual odometry, most existing approaches treat the tasks in a supervised manner. Recent approaches to single view depth estimation explore the possibility of learning without full supervision via minimizing photometric error. In this paper, we explore the use of stereo sequences for learning depth and visual odometry. The use of stereo sequences enables the use of both spatial (between left-right pairs) and temporal (forward backward) photometric warp error, and constrains the scene depth and camera motion to be in a common, real-world scale. At test time our framework is able to estimate single view depth and two-view odometry from a monocular sequence. We also show how we can improve on a standard photometric warp loss by considering a warp of deep features. We show through extensive experiments that: (i) jointly training for single view depth and visual odometry improves depth prediction because of the additional constraint imposed on depths and achieves competitive results for visual odometry; (ii) deep feature-based warping loss improves upon simple photometric warp loss for both single view depth estimation and visual odometry. Our method outperforms existing learning based methods on the KITTI driving dataset in both tasks. The source code is available at https://github.com/Huangying-Zhan/Depth-VO-Feat},
archivePrefix = {arXiv},
arxivId = {1803.03893},
author = {Zhan, Huangying and Garg, Ravi and Weerasekera, Chamara Saroj and Li, Kejie and Agarwal, Harsh and Reid, Ian},
eprint = {1803.03893},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Zhan et al. - 2018 - Unsupervised Learning of Monocular Depth Estimation and Visual Odometry with Deep Feature Reconstruction.pdf:pdf},
title = {{Unsupervised Learning of Monocular Depth Estimation and Visual Odometry with Deep Feature Reconstruction}},
url = {http://arxiv.org/abs/1803.03893},
year = {2018}
}
@inproceedings{Sural2002,
author = {Sural, Shamik and {Gang Qian} and Pramanik, Sakti},
booktitle = {Proceedings. International Conference on Image Processing},
doi = {10.1109/ICIP.2002.1040019},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Sural, Gang Qian, Pramanik - 2002 - Segmentation and histogram generation using the HSV color space for image retrieval.pdf:pdf},
isbn = {0-7803-7622-6},
pages = {II--589--II--592},
publisher = {IEEE},
title = {{Segmentation and histogram generation using the HSV color space for image retrieval}},
url = {http://ieeexplore.ieee.org/document/1040019/},
volume = {2},
year = {2002}
}
@article{Todd1987,
abstract = {The research described in the present article was designed to investigate how patterns of optical texture provide information about the three-dimensional structure of objects in space. Four experiments were performed in which observers were asked to judge the perceived depth of simulated ellipsoid surfaces under a variety of experimental conditions. The results revealed that judged depth increases linearly with simulated depth although the slope of this relation varies significantly among different types of texture patterns. Random variations in the sizes and shapes of individual surface elements have no detectable effect on observers' judgments. The perception of three-dimensional form is quite strong for surfaces displayed under parallel projection, but the amount of apparent depth is slightly less than for identical surfaces displayed under polar projection. Finally, the perceived depth of a surface is eliminated if the optical elements in a display are not sufficiently elongated or if they are not approximately aligned with one another. A theoretical explanation of these findings is proposed based on the neural network analysis of Grossberg and Mingolla.},
author = {Todd, James T. and Akerstrom, Robin A.},
doi = {10.1037/0096-1523.13.2.242},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Todd, Akerstrom - 1987 - Perception of Three-Dimensional Form From Patterns of Optical Texture.pdf:pdf},
isbn = {9781622763399},
issn = {00961523},
journal = {Journal of Experimental Psychology: Human Perception and Performance},
number = {2},
pages = {242--255},
pmid = {2953854},
title = {{Perception of Three-Dimensional Form From Patterns of Optical Texture}},
volume = {13},
year = {1987}
}
@article{Zia2016,
abstract = {SLAM has matured significantly over the past few years, and is beginning to appear in serious commercial products. While new SLAM systems are being proposed at every conference, evaluation is often restricted to qualitative visualizations or accuracy estimation against a ground truth. This is due to the lack of benchmarking methodologies which can holistically and quantitatively evaluate these systems. Further investigation at the level of individual kernels and parameter spaces of SLAM pipelines is non-existent, which is absolutely essential for systems research and integration. We extend the recently introduced SLAMBench framework to allow comparing two state-of-the-art SLAM pipelines, namely KinectFusion and LSD-SLAM, along the metrics of accuracy, energy consumption, and processing frame rate on two different hardware platforms, namely a desktop and an embedded device. We also analyze the pipelines at the level of individual kernels and explore their algorithmic and hardware design spaces for the first time, yielding valuable insights.},
archivePrefix = {arXiv},
arxivId = {1509.04648},
author = {Zia, M. Zeeshan and Nardi, Luigi and Jack, Andrew and Vespa, Emanuele and Bodin, Bruno and Kelly, Paul H.J. and Davison, Andrew J.},
doi = {10.1109/ICRA.2016.7487261},
eprint = {1509.04648},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Zia et al. - 2016 - Comparative design space exploration of dense and semi-dense SLAM.pdf:pdf},
isbn = {9781467380263},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
pages = {1292--1299},
title = {{Comparative design space exploration of dense and semi-dense SLAM}},
volume = {2016-June},
year = {2016}
}
@misc{Hillen2013a,
author = {Hillen, Lorenz},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Hillen - Unknown - Hillen chapter 3.pdf:pdf},
title = {{Hillen chapter 3}}
}
@article{Brockers2014a,
annote = {zelfde als andere paper van Matthies?},
author = {Brockers, Roland and Kuwata, Yoshiaki and Weiss, Stephan and Matthies, Lawrence},
doi = {10.1117/12.2055288},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Brockers et al. - 2014 - Micro air vehicle autonomous obstacle avoidance from stereo-vision.pdf:pdf},
isbn = {9781628410211},
issn = {1996756X},
journal = {SPIE Defense + Security},
keywords = {experiment,image space,obstacle avoidance,obstacle detection,scenario: forest,stereo vision},
mendeley-tags = {experiment,image space,obstacle avoidance,obstacle detection,scenario: forest,stereo vision},
pages = {90840O},
title = {{Micro air vehicle autonomous obstacle avoidance from stereo-vision}},
url = {http://proceedings.spiedigitallibrary.org/proceeding.aspx?articleid=1879744},
volume = {9084},
year = {2014}
}
@article{Marzat2017,
author = {Marzat, Julien and Bertrand, Sylvain and Eudes, Alexandre},
doi = {10.1016/j.ifacol.2017.08.1910},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Marzat, Bertrand, Eudes - 2017 - Reactive MPC for Autonomous MAV Navigation in Indoor Cluttered Environments Flight Experiments.pdf:pdf},
issn = {24058963},
journal = {IFAC-PapersOnLine},
keywords = {flight experiments,localization and mapping,micro-air vehicles,model predictive control,vision-based},
number = {1},
pages = {15996--16002},
title = {{Reactive MPC for Autonomous MAV Navigation in Indoor Cluttered Environments: Flight Experiments}},
volume = {50},
year = {2017}
}
@article{Saxena2009,
abstract = {We consider the problem of estimating detailed 3D structure from a single still image of an unstructured environment. Our goal is to create 3D models which are both quantitatively accurate as well as visually pleasing. For each small homogeneous patch in the image, we use a Markov random field (MRF) to infer a set of "plane parameters" that capture both the 3D location and 3D orientation of the patch. The MRF, trained via supervised learning, models both image depth cues as well as the relationships between different parts of the image. Inference in our model is tractable, and requires only solving a convex optimization problem. Other than assuming that the environment is made up of a number of small planes, our model makes no explicit assumptions about the structure of the scene; this enables the algorithm to capture much more detailed 3D structure than does prior art (such as Saxena et ah, 2005, Delage et ah, 2005, and Hoiem et el, 2005), and also give a much richer experience in the 3D flythroughs created using image-based rendering, even for scenes with significant non-vertical structure. Using this approach, we have created qualitatively correct 3D models for 64.9{\%} of 588 images downloaded from the Internet, as compared to Hoiem et al.'s performance of 33.1{\%}. Further, our models are quantitatively more accurate than either Saxena et al. or Hoiem et al.},
author = {Saxena, Ashutosh and {Min Sun} and Ng, A.Y.},
doi = {10.1109/TPAMI.2008.132},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Saxena, Min Sun, Ng - 2009 - Make3D Learning 3D Scene Structure from a Single Still Image.pdf:pdf},
isbn = {978-1-4244-1631-8},
issn = {0162-8828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {scenelayout},
month = {may},
number = {5},
pages = {824--840},
pmid = {19299858},
title = {{Make3D: Learning 3D Scene Structure from a Single Still Image}},
url = {http://ieeexplore.ieee.org/document/4531745/},
volume = {31},
year = {2009}
}
@inproceedings{Wu2014,
abstract = {— An important problem in robot simultaneous localization and mapping (SLAM) is loop closure detection. Recent studies of the problem have led to successful development of methods that are based on images captured by the robot. These methods tackle the issue of efficiency through data structures such as indexing and hierarchical (tree) organization of the image data that represent the robot map. In this paper, we offer an alternative approach and present a novel method for visual loopclosure detection. Our approach uses an extremely simple image representation, namely, a downsampled binarized version of the original image, combined with a highly efficient image similarity measure mutual information. As a result, our method is able to perform loop closure detection in a map with 20 million key locations in about 2.38 seconds on a commodity computer. The excellent performance of our method in terms of its low complexity and accuracy in experiments establishes it as a promising solution to loop closure detection in largescale robot maps.},
author = {Wu, Junjun and Zhang, Hong and Guan, Yisheng},
booktitle = {2014 IEEE International Conference on Robotics and Automation (ICRA)},
doi = {10.1109/ICRA.2014.6906955},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Wu, Zhang, Guan - 2014 - An efficient visual loop closure detection method in a map of 20 million key locations.pdf:pdf},
isbn = {978-1-4799-3685-4},
month = {may},
pages = {861--866},
publisher = {IEEE},
title = {{An efficient visual loop closure detection method in a map of 20 million key locations}},
url = {http://ieeexplore.ieee.org/document/6906955/},
year = {2014}
}
@article{Kuipers1988,
abstract = {In a large-scale space, structure is at a significantly larger scale than the observations available at an instant. To learn the structure of a large-scale space from observations, the observer must build a cognitive map of the environment by integrating observations over an extended period of time, inferring spatial structure from perceptions and the effects of actions. The cognitive map representation of large-scale space must account for a mapping, or learning structure from observations, and navigation, or creating and executing a plan to travel from one place to another. Approaches to date tend to be fragile either because they don't build maps; or because they assume nonlocal observations, such as those available in preexisting maps or global coordinate systems, including active landmark beacons and geo-locating satellites. We propose that robust navigation and mapping systems for large-scale space can be developed by adhering to a natural, four-level semantic hierarchy of descriptions for representation, planning, and execution of plans in large-scale space. The four levels are sensorimotor interaction, procedural behaviors, topological mapping, and metric mapping. Effective systems represent the environment, relative to sensors, at all four levels and formulate robust system behavior by moving flexibly between representational levels at run time. We demonstrate our claims in three implemented models: Tour, the Qualnav system simulator, and the NX robot.},
author = {Kuipers, Benjamin J. and Levitt, Todd S.},
doi = {10.1609/aimag.v9i2.674},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Kuipers, Levitt - 1988 - Navigation and Mapping in Large Scale Space.pdf:pdf},
issn = {0738-4602},
journal = {AI Magazine},
number = {2},
pages = {25},
title = {{Navigation and Mapping in Large Scale Space}},
url = {http://www.aaai.org/ojs/index.php/aimagazine/article/view/674},
volume = {9},
year = {1988}
}
@article{Heng2014,
abstract = {It is anticipatedthat theMars ScienceLaboratory rover, namedCuriosity,will traverse 10–20 kmon the surface of Mars during its primary mission. In preparation for this traverse, Earth-based tests were performed using Mars weight vehicles. These vehicles were driven over Mars analog bedrock, cohesive soil, and cohesionless sand at various slopes. Vehicle slip was characterized on each of these terrains versus slope for direct upslope driving. Results show that slopes up to 22 degrees are traversable on smooth bedrock and that slopes up to 28 degrees are traversable on some cohesive soils. In cohesionless sand, results show a sharp transition between moderate slip on 10 degree slopes and vehicle embedding at 17 degrees. For cohesionless sand, data are also presented showing the relationship between vehicle slip and wheel sinkage. Side by side testing of the Mars Exploration Rover test vehicle and the Mars Science Laboratory test vehicle show how increased wheel diameter leads to better slope climbing ability in sand for vehicles with nearly identical ground pressure. Lastly, preliminary data from Curiosity's initial driving on Mars are presented and compared to the Earth-based testing, showing good agreement for the driving done during the first 250 Martian days.},
archivePrefix = {arXiv},
arxivId = {10.1.1.91.5767},
author = {Heng, Lionel and Honegger, Dominik and Lee, Gim Hee and Meier, Lorenz and Tanskanen, Petri and Fraundorfer, Friedrich and Pollefeys, Marc},
doi = {10.1002/rob.21520},
eprint = {10.1.1.91.5767},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Heng et al. - 2014 - Autonomous Visual Mapping and Exploration With a Micro Aerial Vehicle.pdf:pdf},
isbn = {9783902661623},
issn = {15564959},
journal = {Journal of Field Robotics},
keywords = {Backstepping,Non-linear control,Path-following},
month = {jul},
number = {4},
pages = {654--675},
pmid = {22164016},
title = {{Autonomous Visual Mapping and Exploration With a Micro Aerial Vehicle}},
url = {http://doi.wiley.com/10.1002/rob.21520},
volume = {31},
year = {2014}
}
@article{Tippetts2016,
abstract = {A significant amount of research in the field of stereo vision has been published in the past decade. Considerable progress has been made in improving accuracy of results as well as achieving real-time performance in obtaining those results. This work provides a comprehensive review of stereo vision algorithms with specific emphasis on real-time performance to identify those suitable for resource-limited systems. An attempt has been made to compile and present accuracy and runtime performance data for all stereo vision algorithms developed in the past decade. Algorithms are grouped into three categories: (1) those that have published results of real-time or near real-time performance on standard processors, (2) those that have real-time performance on specialized hardware (i.e. GPU, FPGA, DSP, ASIC), and (3) those that have not been shown to obtain near real-time performance. This review is intended to aid those seeking algorithms suitable for real-time implementation on resource-limited systems, and to encourage further research and development of the same by providing a snapshot of the status quo.},
author = {Tippetts, Beau and Lee, Dah Jye and Lillywhite, Kirt and Archibald, James},
doi = {10.1007/s11554-012-0313-2},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Tippetts et al. - 2016 - Review of stereo vision algorithms and their suitability for resource-limited systems.pdf:pdf},
isbn = {1861-8200},
issn = {18618200},
journal = {Journal of Real-Time Image Processing},
keywords = {low complexity,stereo vision,survey},
mendeley-tags = {low complexity,stereo vision,survey},
number = {1},
pages = {5--25},
publisher = {Springer-Verlag},
title = {{Review of stereo vision algorithms and their suitability for resource-limited systems}},
url = {http://dx.doi.org/10.1007/s11554-012-0313-2},
volume = {11},
year = {2016}
}
@article{Bonin-Font2008,
abstract = {Mobile robot vision-based navigation has been the source of countless research contribu-tions, from the domains of both vision and control. Vision is becoming more and more common in applications such as localization, automatic map construction, autonomous navigation, path following, inspection, monitoring or risky situation detection. This survey presents those pieces of work, from the nineties until nowadays, which constitute a wide progress in visual navigation techniques for land, aerial and autonomous underwater vehicles. The paper deals with two major approaches: map-based naviga-tion and mapless navigation. Map-based navigation has been in turn subdivided in metric map-based navigation and topological map-based navigation. Our outline to mapless navigation includes reactive techniques based on qualitative characteristics extraction, appearance-based localization, optical flow, features tracking, plane ground detection/tracking, etc... The recent concept of visual sonar has also been revised.},
annote = {Indeling map-based vs mapless.},
author = {Bonin-Font, Francisco and Ortiz, Alberto and Oliver, Gabriel},
doi = {10.1007/s10846-008-9235-4},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Bonin-Font, Ortiz, Oliver - 2008 - Visual Navigation for Mobile Robots A Survey.pdf:pdf},
issn = {0921-0296},
journal = {Journal of Intelligent and Robotic Systems},
keywords = {highlight,survey},
mendeley-tags = {highlight,survey},
month = {nov},
number = {3},
pages = {263--296},
title = {{Visual Navigation for Mobile Robots: A Survey}},
url = {http://link.springer.com/10.1007/s10846-008-9235-4},
volume = {53},
year = {2008}
}
@article{Choi2008,
abstract = {This paper describes a geometrically constrained Extended Kalman Filter (EKF) framework for a line feature based SLAM, which is applicable to a rectangular indoor environment. Its focus is on how to handle sparse and noisy sensor data, such as PSD infrared sensors with limited range and limited number, in order to develop a low-cost navigation system. It has been applied to a vacuum cleaning robot in our research. In order to meet the real-time objective with low computing power, we develop an efficient line feature extraction algorithm based upon an iterative end point fit (IEPF) technique assisted by our constrained version of the Hough transform. It uses a geometric constraint that every line is orthogonal or parallel to each other because in a general indoor setting, most furniture and walls satisfy this constraint. By adding this constraint to the measurement model of EKF, we build a geometrically constrained EKF framework which can estimate line feature positions more accurately as well as allow their covariance matrices to converge more rapidly when compared to the case of an unconstrained EKF. The experimental results demonstrate the accuracy and robustness to the presence of sensor noise and errors in an actual indoor environment.},
archivePrefix = {arXiv},
arxivId = {there is not},
author = {Choi, Young Ho and Lee, Tae Kyeong and Oh, Se Young},
doi = {10.1007/s10514-007-9050-y},
eprint = {there is not},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Choi, Lee, Oh - 2008 - A line feature based SLAM with low grade range sensors using geometric constraints and active exploration for mob.pdf:pdf},
isbn = {0929-5593},
issn = {09295593},
journal = {Autonomous Robots},
keywords = {EKF,Environmental modeling,Geometric constraint,Line feature,SLAM,low complexity,slam},
mendeley-tags = {low complexity,slam},
number = {1},
pages = {13--27},
pmid = {17431302},
title = {{A line feature based SLAM with low grade range sensors using geometric constraints and active exploration for mobile robot}},
volume = {24},
year = {2008}
}
@incollection{Cutting1995,
abstract = {Publisher Summary This chapter discusses three questions: Why are there so many sources of information about layout? How is it that one perceives layout with near-metric accuracy when none of these sources yields metric information about it? Can one not do better, theoretically, in understanding the perception of layout than simply make a list? The answer to the first question begins with Todd's answer. Perceiving layout is extremely important to human beings, so important that it must be redundantly specified so that the redundancy can guard against the failure of any given source or the failure of any of the assumptions on which a given source is based. However, information redundancy is only part of the answer. Different sources of information about layout metrically reinforce and contrast with each other, providing a powerful network of constraints. The answer to the second proceeds from this idea. Through the analysis of depth-threshold functions for nine different sources of information about layout, one can begin to understand how those sources of information sharing the same-shaped functions across distances can help ramify judgments of layout by serving to correct measurement errors in each. Third, on the basis of the analyses and the pattern of functions, it suggests that list making has misled about space and layout. Psychologists and other vision scientists have generally considered layout, space, and distance as a uniform commodity in which observers carry out their day-to-day activities.},
author = {Cutting, James E. and Vishton, Peter M.},
booktitle = {Perception of Space and Motion},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Cutting, Vishton - 1995 - Perceiving Layout and Knowing Distances The Integration, Relative Potency, and Contextual Use of Different Inf.pdf:pdf},
pages = {69--117},
publisher = {Elsevier},
title = {{Perceiving Layout and Knowing Distances: The Integration, Relative Potency, and Contextual Use of Different Information about Depth}},
url = {http://linkinghub.elsevier.com/retrieve/pii/B9780122405303500055},
year = {1995}
}
@article{Yu2013b,
abstract = {In this paper we present an observability-based local path planning and obstacle avoidance technique that utilizes an extended Kalman Filter (EKF) to estimate the time-to-collision (TTC) and bearing to obstacles using bearing-only measurements. To ensure that the error covariance matrix computed by an EKF is bounded, the system should be observable. We perform a nonlinear observability analysis to obtain the necessary conditions for complete observability of the system. These conditions are used to explicitly design a path planning algorithm that enhances observability while simultaneously avoiding collisions with obstacles. We analyze the behavior of the path planning algorithm and specially define the environments where the path planning algorithm will guarantee collision-free paths that lead to a goal configuration. Numerical results show the effectiveness of the planning algorithm in solving single and multiple obstacle avoidance problems while improving the estimation accuracy. ?? 2013 Elsevier B.V. All rights reserved.},
author = {Yu, Huili and Sharma, Rajnikant and Beard, Randal W. and Taylor, Clark N.},
doi = {10.1016/j.robot.2013.07.013},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Yu et al. - 2013 - Observability-based local path planning and obstacle avoidance using bearing-only measurements.pdf:pdf},
isbn = {8019001271},
issn = {09218890},
journal = {Robotics and Autonomous Systems},
keywords = {Collision avoidance,Miniature air vehicle,Observability,Path planning},
number = {12},
pages = {1392--1405},
publisher = {Elsevier B.V.},
title = {{Observability-based local path planning and obstacle avoidance using bearing-only measurements}},
url = {http://dx.doi.org/10.1016/j.robot.2013.07.013},
volume = {61},
year = {2013}
}
@inproceedings{Shahbazi2011,
abstract = {—In this work we present a new approach for detecting loop closures in a realtime online setting. The Loop Closure Detection problem is important in visual SLAM applications and different approaches exist to deal with this problem. Most of these approaches are based on the BagofWords approach, and assume a fixed visual vocabulary can work in different types of environments. However BOW is known to introduce perceptual aliasing. By using Locality Sensitive Hashing (LSH) we are able to compute image similarity and detect loop closures by using visual features directly without vector quantization as in BOW and also LSH does not require a prior visual vocabulary. We show the effectiveness of our approach empirically by comparing it to the Bag of Words (BOW) approach which is the dominant method of selecting candidate loop closing images. Our method is fast enough for realtime applications and its accuracy is significantly better than the BOW approach.},
author = {Shahbazi, Hossein and Zhang, Hong},
booktitle = {2011 IEEE/RSJ International Conference on Intelligent Robots and Systems},
doi = {10.1109/IROS.2011.6095099},
file = {:home/tom/Documents/phd/mendeley{\_}pdf/Shahbazi, Zhang - 2011 - Application of Locality Sensitive Hashing to realtime loop closure detection.pdf:pdf},
isbn = {978-1-61284-456-5},
month = {sep},
pages = {1228--1233},
publisher = {IEEE},
title = {{Application of Locality Sensitive Hashing to realtime loop closure detection}},
url = {http://ieeexplore.ieee.org/document/6095099/},
year = {2011}
}
